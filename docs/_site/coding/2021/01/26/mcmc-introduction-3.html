<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Mcmc Introduction 3 &middot; Zhu Philip's AI Journey
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- merge something else -->
  
  <!-- merge something else 
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" /> -->
  
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>  
  
  <script defer src="/assets/js/lbox.js"></script>
   

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script> 

</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <link rel="stylesheet" href="/assets/style-3.css">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <div align="center">
          <img src="/assets/profile-pixel.png" class="profilepic pt-3 pb-2">
        </div>
        <!-- <a href="/"> -->
          Zhu Philip's AI Journey
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/working">Working</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <!-- <a class="sidebar-nav-item" href="/projects">Projects</a> -->
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">publications</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/working/">Working</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/zphilip/zphilip.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:zphilip48@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/tianda-zhu-37a5b031" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/zphilip" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/zphilip" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Mcmc Introduction 3</h1>
  <span class="post-date">26 Jan 2021</span>
  <h2 id="pure-monte-carlo-mcmc-sampling">Pure Monte Carlo (MCMC) sampling</h2>
<p>Importance Sampling
—
Using sampling to approximate a distribution</p>

<p>\(E[f(x)] = \int f(x)p(x) dx \approx \frac{1}{n}\sum_{i} f(x_i)\)
where $ x \sim p(x)$</p>

\[E[f(x)] = \int f(x)p(x) dx = \int f(x)\frac{p(x)}{q(x)}q(x) dx \approx \frac{1}{n} \sum_{i} f(x_i)\frac{p(x_i)}{q(x_i)}\]

<p>where $ x \sim q(x)$</p>

<p>Idea of importance sampling: draw the sample from a proposal distribution and re-weight the integral using importance weights so that the correct distribution is targeted</p>

\[Var(X) = E[X^2] - E[X]^2\]

<h2 id="example-1-of-importance-sampling">Example 1 of importance sampling</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of importance sampling in Python
</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># Number of Monte Carlo samples
</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Initialization of random number generator for replicability
</span>
<span class="c1"># Standard Monte Carlo
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span> <span class="o">**</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">MC</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="n">std_MC</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">((</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">g</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Standard Monte-Carlo estimate of the expected value: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">MC</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Standard deviation of plain-vanilla Monte Carlo: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">std_MC</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span>

<span class="c1"># Importance sampling
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">g</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span>  <span class="o">-</span> <span class="mi">3</span><span class="p">)</span> <span class="o">**</span> <span class="mi">4</span><span class="p">);</span>
<span class="n">g_weighted</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">IS</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">g_weighted</span><span class="p">)</span>
<span class="n">std_IS</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">g_weighted</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Importance-sampling Monte-Carlo estimate of the expected value: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">IS</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Standard deviation of importance-sampling Monte Carlo: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">std_IS</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Standard Monte-Carlo estimate of the expected value: 0.08579415409780462
Standard deviation of plain-vanilla Monte Carlo: 0.007904811247115087
 
Importance-sampling Monte-Carlo estimate of the expected value: 0.09096069224808337
Standard deviation of importance-sampling Monte Carlo: 0.0011925073695279826
</code></pre></div></div>

<h2 id="example-2-of-importance-sampling">Example 2 of importance sampling</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">f_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">distribution</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># return probability given a value
</span>    <span class="n">distribution</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">distribution</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>  <span class="c1"># x ranges from 0 to 4
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">f_x</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"$f(x)$"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"y"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s">"size"</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7f4dd012b790&gt;
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_4_1.png" alt="png" /></p>

<h2 id="sampling">Sampling</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># pre-setting
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">mu_target</span> <span class="o">=</span> <span class="mf">3.5</span>
<span class="n">sigma_target</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mu_appro</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">sigma_appro</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#simpling/target distribution
</span><span class="n">p_x</span> <span class="o">=</span> <span class="n">distribution</span><span class="p">(</span><span class="n">mu_target</span><span class="p">,</span> <span class="n">sigma_target</span><span class="p">)</span>
<span class="c1">#proposal distribution
</span><span class="n">q_x</span> <span class="o">=</span> <span class="n">distribution</span><span class="p">(</span><span class="n">mu_appro</span><span class="p">,</span> <span class="n">sigma_appro</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_target</span><span class="p">,</span> <span class="n">sigma_target</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">)],</span> <span class="n">label</span><span class="o">=</span><span class="s">"distribution $p(x)$"</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_appro</span><span class="p">,</span> <span class="n">sigma_appro</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">)],</span> <span class="n">label</span><span class="o">=</span><span class="s">"distribution $q(x)$"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Distributions"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751






&lt;matplotlib.legend.Legend at 0x7f4dc3914dd0&gt;
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_6_2.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># value
</span><span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># draw a sample
</span>    <span class="n">x_i</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_target</span><span class="p">,</span> <span class="n">sigma_target</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"simulate value"</span><span class="p">,</span> <span class="n">s</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># calculate value sampling from a different distribution
</span>
<span class="n">value_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># sample from different distribution
</span>    <span class="n">x_i</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_appro</span><span class="p">,</span> <span class="n">sigma_appro</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p_x</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_x</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_i</span><span class="p">))</span>
    
    <span class="n">value_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"average {} variance {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">value_list</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">value_list</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>simulate value 0.9549975623967805
average 0.9487821822916341 variance 0.2950013242336597
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">value_list</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"distribution $q(x)$"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Distributions"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  





Text(0.5, 1.0, 'Distributions')
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_8_2.png" alt="png" /></p>

<h2 id="different-qx">Different $q(x)$</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pre-setting
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">mu_target</span> <span class="o">=</span> <span class="mf">3.5</span>
<span class="n">sigma_target</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mu_appro</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma_appro</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">p_x</span> <span class="o">=</span> <span class="n">distribution</span><span class="p">(</span><span class="n">mu_target</span><span class="p">,</span> <span class="n">sigma_target</span><span class="p">)</span>
<span class="n">q_x</span> <span class="o">=</span> <span class="n">distribution</span><span class="p">(</span><span class="n">mu_appro</span><span class="p">,</span> <span class="n">sigma_appro</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_target</span><span class="p">,</span> <span class="n">sigma_target</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">)],</span> <span class="n">label</span><span class="o">=</span><span class="s">"distribution $p(x)$"</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_appro</span><span class="p">,</span> <span class="n">sigma_appro</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">)],</span> <span class="n">label</span><span class="o">=</span><span class="s">"distribution $q(x)$"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Distributions"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  
/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  from ipykernel import kernelapp as app





&lt;matplotlib.legend.Legend at 0x7f4dc0d58890&gt;
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_10_2.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate value sampling from a different distribution
</span>
<span class="n">value_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># need larger steps
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># sample from different distribution
</span>    <span class="n">x_i</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_appro</span><span class="p">,</span> <span class="n">sigma_appro</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p_x</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_x</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_i</span><span class="p">))</span>
    
    <span class="n">value_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"average {} variance {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">value_list</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">value_list</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>average 1.03347241919433 variance 154.89575590297025
</code></pre></div></div>

<h1 id="markov-chain-monte-carlo-mcmc-sampling-part-1-the-basics">Markov chain Monte Carlo (MCMC) sampling, part 1: the basics</h1>

<p>https://www.tweag.io/blog/2019-10-25-mcmc-intro1/</p>

<p><a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo (MCMC)</a> is a powerful class of methods to sample from probability distributions known only up to an (unknown) normalization constant. But before we dive into MCMC, let’s consider why you might want to do sampling in the first place.</p>

<p>The answer to that is: whenever you’re either interested in the samples themselves (for example, inferring unknown parameters in Bayesian inference) or you need them to approximate expected values of functions w.r.t. to a probability distribution (for example, calculating thermodynamic quantities from the distribution of microstates in statistical physics). Sometimes, only the mode of a probability distribution is of primary interest. In this case, it’s obtained by numerical optimization so full sampling is not necessary.</p>

<p>It turns out that sampling from any but the most basic probability distributions is a difficult task. <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">Inverse transform sampling</a> is an elementary method to sample from probability distributions, but requires the cumulative distribution function, which in turn requires knowledge of the, generally unknown, normalization constant. Now in principle, you could just obtain the normalization constant by numerical integration, but this quickly gets infeasible with an increasing number of dimensions. <a href="https://en.wikipedia.org/wiki/Rejection_sampling">Rejection sampling</a> does not require a normalized distribution, but efficiently implementing it requires a good deal of knowledge about the distribution of interest, and it suffers strongly from the curse of dimension, meaning that its efficiency decreases rapidly with an increasing number of variables. That’s when you need a smart way to obtain representative samples from your distribution which doesn’t require knowledge of the normalization constant.</p>

<p>MCMC algorithms are a class of methods which do exactly that. These methods date back to a <a href="https://pdfs.semanticscholar.org/7b3d/c9438227f747e770a6fb6d7d7c01d98725d6.pdf">seminal paper by Metropolis et al.</a>, who developed the first MCMC algorithm, correspondingly called <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis algorithm</a>, to calculate the equation of state of a two-dimensional system of hard spheres. In reality, they were looking for a general method to calculate expected values occurring in statistical physics.</p>

<p>In this blog post, I introduce the basics of MCMC sampling; in subsequent posts I’ll cover several important, increasingly complex and powerful MCMC algorithms, which all address different difficulties one frequently faces when using the Metropolis-Hastings algorithm. Along the way, you will gain a solid understanding of these challenges and how to address them. Also, this serves as a reference for MCMC methods in the context of the <a href="https://www.tweag.io/posts/2019-09-20-monad-bayes-1.html">monad-bayes</a> series. Furthermore, I hope the provided notebooks will not only spark your interest in exploring the behavior of MCMC algorithms for various parameters/probability distributions, but also serve as a basis for implementing and understanding useful extensions of the basic versions of the algorithms I present.</p>

<h2 id="markov-chains">Markov chains</h2>

<p>Now that we know why we want to sample, let’s get to the heart of MCMC — Markov chains.
What is a Markov chain?
Without all the technical details, a Markov chain is a random sequence of states in some state space in which the probability of picking a certain state next depends only on the current state in the chain and not on the previous history: it is memory-less.
Under certain conditions, a Markov chain has a unique stationary distribution of states to which it will converge after a certain number of states.
From that number on, states in the Markov chain will be distributed according to the invariant distribution.
MCMC algorithms work by constructing a Markov chain with the probability distribution you want to sample from as the stationary distribution.
In order to sample from a distribution $\pi(x)$, a MCMC algorithm constructs and simulates a Markov chain whose stationary distribution is $\pi(x)$, meaning that, after an initial “burn-in” phase, the states of that Markov chain are distributed according to $\pi(x)$. We thus just have to store the states to obtain samples from $\pi(x)$.</p>

<p>For didactic purposes, let’s for now consider both a discrete state space and discrete “time”.
The key quantity characterizing a Markov chain is the transition operator $T(x_{i+1}|x_i)$ which gives you the probability of being in state $x_{i+1}$ at time $i+1$ given that the chain is in state $x_i$ at time $i$.</p>

<p>Now just for fun (and for illustration), let’s quickly whip up a Markov chain which has a unique stationary distribution. We’ll start with some imports and settings for the plots:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">notebook</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p>The Markov chain will hop around on a discrete state space which is made up from three weather states:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">state_space</span> <span class="o">=</span> <span class="p">(</span><span class="s">"sunny"</span><span class="p">,</span> <span class="s">"cloudy"</span><span class="p">,</span> <span class="s">"rainy"</span><span class="p">)</span>
</code></pre></div></div>

<p>In a discrete state space, the transition operator is just a matrix.
Columns and rows correspond, in our case, to sunny, cloudy, and rainy weather.
We pick more or less sensible values for all transition probabilities:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transition_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(((</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
                              <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">),</span>
                              <span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)))</span>
</code></pre></div></div>

<p>The rows indicate the states the chain might currently be in and the columns the states the chains might transition to.
If we take one “time” step of the Markov chain as one hour, then, if it’s sunny, there’s a 60% chance it stays sunny in the next hour, a 30% chance that in the next hour we will have cloudy weather and only a 10% chance of rain immediately after it had been sunny before.
This also means that each row has to sum up to one.</p>

<p>Let’s run our Markov chain for a while:np.random.choice</p>

<p><strong>zphilip48</strong>: using exactly the current state — states[-1] and the corresponding transition_matrix[states[-1]] apply to next state by the np.random.choice 
<strong>the result is recorded into states</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">states</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">[</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="n">states</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
</code></pre></div></div>

<p>We can monitor the convergence of our Markov chain to its stationary distribution by calculating the empirical probability for each of the states as a function of chain length:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">despine</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">spines</span><span class="o">=</span><span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">spines</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="n">spine</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">width</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="c1">#calcuate the data every offsets 
</span><span class="n">offsets</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">state_space</span><span class="p">):</span>
    <span class="c1">#here cacluate the sum of the frequence of the certain states and normalize it is the eactly \pi(x)
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">offsets</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">states</span><span class="p">[:</span><span class="n">offset</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="n">offset</span> 
            <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="n">offsets</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"number of steps"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"empirical probability"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_28_0.png" alt="png" /></p>

<h2 id="the-mother-of-all-mcmc-algorithms-metropolis-hastings">The mother of all MCMC algorithms: Metropolis-Hastings</h2>

<p>So that’s lots of fun, but back to sampling an arbitrary probability distribution $\pi$. 
It could either be discrete, in which case we would keep talking about a transition matrix $T$, or it continous, in which case $T$ would be a transition <em>kernel</em>.
From now on, we’re considering continuous distributions, but all concepts presented here transfer to the discrete case.<br />
If we could design the transition kernel in such a way that the next state is already drawn from $\pi$, we would be done, as our Markov Chain would… well…  immediately sample from $\pi$.
Unfortunately, to do this, we need to be able to sample from $\pi$, which we can’t.
Otherwise you wouldn’t be reading this, right?<br />
A way around this is to split the transition kernel $T(x_{i+1}|x_i)$ into two parts:
a proposal step and an acceptance/rejection step. The proposal step features a proposal distribution $q(x_{i+1}|x_i)$, from which we can sample possible next states of the chain. In addition to being able to sample from it, we can choose this distribution arbitrarily. But, one should strive to design it such that samples from it are both as little correlated with the current state as possible and have a good chance of being accepted in the acceptance step. Said acceptance/rejection step is the second part of the transition kernel and corrects for the error introduced by proposal states drawn from $q \neq \pi$. It involves calculating an acceptance probability $p_\mathrm{acc}(x_{i+1}|x_i)$ and accepting the proposal $x_{i+1}$ with that probability as the next state in the chain. Drawing the next state $x_{i+1}$ from $T(x_{i+1}|x_i)$ is then done as follows: first, a proposal state $x_{i+1}$ is drawn from $q(x_{i+1}|x_i)$. It is then accepted as the next state with probability 
$p_\mathrm{acc}(x_{i+1}|x_i)$ or rejected with probability $1 - p_\mathrm{acc}(x_{i+1}|x_i)$, in which case the current state is copied as the next state.</p>

<p>We thus have 
\(T(x_{i+1}|x_i)=q(x_{i+1} | x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) \ \mbox .\)
A sufficient condition for a Markov chain to have $\pi$ as its stationary distribution is the transition kernel obeying <em>detailed balance</em> or, in the physics literature, <em>microscopic reversibility</em>:
\(\pi(x_i) T(x_{i+1}|x_i) = \pi(x_{i+1}) T(x_i|x_{i+1})\)
This means that the probability of being in a state $x_i$ and transitioning to $x_{i+1}$ must be equal to the probability of the reverse process, namely, being in state $x_{i+1}$ and transitioning to $x_i$.
Transition kernels of most MCMC algorithms satisfy this condition. <br />
For the two-part transition kernel to obey detailed balance, we need to choose $p_\mathrm{acc}$ correctly, meaning that is has to correct for any asymmetries in probability flow from / to $x_{i+1}$ or $x_i$.
One possibility is the Metropolis acceptance criterion: 
\(p_\mathrm{acc}(x_{i+1}|x_i) = \mathrm{min} \left\{1, \frac{\pi(x_{i+1}) \times q(x_i|x_{i+1})}{\pi(x_i) \times q(x_{i+1}|x_i)} \right\} \ \mbox .\)
Now here’s where the magic happens: we know $\pi$ only up to a constant, but it doesn’t matter, because that unknown constant cancels out in the expression for $p_\mathrm{acc}$! It is this property of $p_\mathrm{acc}$ which makes algorithms based on Metropolis-Hastings work for unnormalized distributions. Often, symmetric proposal distributions with $q(x_i|x_{i+1})=q(x_{i+1}|x_i)$ are used, in which case the Metropolis-Hastings algorithm reduces to the original, but less general Metropolis algorithm developed in 1953 and for which
\(p_\mathrm{acc}(x_{i+1}|x_i) = \mathrm{min} \left\{1, \frac{\pi(x_{i+1})}{\pi(x_i)} \right\} \ \mbox .\)
We can then write the complete Metropolis-Hastings transition kernel as
\(T(x_{i+1}|x_i) = \begin{cases}
                   q(x_{i+1}|x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) &amp;: x_{i+1} \neq x_i \mbox ; \\
                   1 - \int \mathrm{d}x_{i+1} \ q(x_{i+1}|x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) &amp;: x_{i+1} = x_i\mbox .
                 \end{cases}\)</p>

<h2 id="implementing-the-metropolis-hastings-algorithm-in-python">Implementing the Metropolis-Hastings algorithm in Python</h2>

<p>All right, now that we know how Metropolis-Hastings works, let’s go ahead and implement it.
First, we set <strong>the log-probability of the distribution we want to sample from</strong> - <strong>without normalization constants</strong>, as we pretend we don’t know them.
in practise we might even don’t the equation but just generate the data from lab system.
Let’s work for now with a standard normal distribution:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
     <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, we choose a symmetric proposal distribution.
Generally, including information you have about the distribution you want to sample from in the proposal distribution will lead to better performance of the Metropolis-Hastings algorithm. 
A naive approach is to just take the current state $x$ and pick a proposal from $\mathcal{U}(x-\frac{\Delta}{2}, x+\frac{\Delta}{2})$, that is, we set some step size $\Delta$ and move left or right a maximum of $\frac{\Delta}{2}$ from our current state:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">proposal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">stepsize</span><span class="p">,</span> 
                             <span class="n">high</span><span class="o">=</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">stepsize</span><span class="p">,</span> 
                             <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, we calculate our acceptance probability:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">p_acc_MH</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">x_old</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">x_old</span><span class="p">)))</span>
</code></pre></div></div>

<p>Now we piece all this together into our really brief implementation of a Metropolis-Hastings sampling step:
the samples come from uniform distribution-proposal here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_MH</span><span class="p">(</span><span class="n">x_old</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">):</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">proposal</span><span class="p">(</span><span class="n">x_old</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">)</span>
    <span class="c1"># here we determine whether we accept the new state or not:
</span>    <span class="c1"># we draw a random number uniformly from [0,1] and compare
</span>    <span class="c1"># it with the acceptance probability
</span>    <span class="n">accept</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">p_acc_MH</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">x_old</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">accept</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">accept</span><span class="p">,</span> <span class="n">x_new</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">accept</span><span class="p">,</span> <span class="n">x_old</span>
</code></pre></div></div>

<p>Apart from the next state in the Markov chain, <code class="language-plaintext highlighter-rouge">x_new</code> or <code class="language-plaintext highlighter-rouge">x_old</code>, we also return whether the MCMC move was accepted or not.
This will allow us to keep track of the acceptance rate.
Let’s complete our implementation by writing a function that iteratively calls <code class="language-plaintext highlighter-rouge">sample_MH</code> and thus builds up the Markov chain:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_MH_chain</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">,</span> <span class="n">n_total</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">):</span>

    <span class="n">n_accepted</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_total</span><span class="p">):</span>
        <span class="n">accept</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">sample_MH</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">)</span>
        <span class="n">chain</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">n_accepted</span> <span class="o">+=</span> <span class="n">accept</span>
    
    <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">n_accepted</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">n_total</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">chain</span><span class="p">,</span> <span class="n">acceptance_rate</span>
</code></pre></div></div>

<h2 id="testing-our-metropolis-hastings-implementation-and-exploring-its-behavior">Testing our Metropolis-Hastings implementation and exploring its behavior</h2>

<p>Now you’re probably excited to see this in action.
Here we go, taking some informed guesses at the <code class="language-plaintext highlighter-rouge">stepsize</code> and <code class="language-plaintext highlighter-rouge">n_total</code> arguments:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chain</span><span class="p">,</span> <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">build_MH_chain</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]),</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
<span class="n">chain</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span> <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Acceptance rate: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">acceptance_rate</span><span class="p">))</span>
<span class="n">last_states</span> <span class="o">=</span> <span class="s">", "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">"{:.5f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> 
                        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Last ten states of chain: "</span> <span class="o">+</span> <span class="n">last_states</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: 0.715
Last ten states of chain: 0.74584, 0.99478, 0.20970, 0.20970, 0.20970, -0.89192, -2.02830, -1.28760, -1.61138, -1.61138
</code></pre></div></div>

<p>All right.
So did this work?
We achieved an acceptance rate of around 71% and we have a chain of states.
We should throw away the first few states during which the chain won’t have converged to its stationary distribution yet.
Let’s check whether the states we drew are actually normally distributed:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_samples</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s">'vertical'</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                 <span class="n">xlims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>
    
    <span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"MCMC samples"</span><span class="p">,</span>
           <span class="n">orientation</span><span class="o">=</span><span class="n">orientation</span><span class="p">)</span>
    <span class="c1"># we numerically calculate the normalization constant of our PDF
</span>    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">Z</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">xses</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">yses</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="n">Z</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xses</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">orientation</span> <span class="o">==</span> <span class="s">'horizontal'</span><span class="p">:</span>
        <span class="p">(</span><span class="n">yses</span><span class="p">,</span> <span class="n">xses</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">xses</span><span class="p">,</span> <span class="n">yses</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xses</span><span class="p">,</span> <span class="n">yses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"true distribution"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">legend</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">plot_samples</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="mi">500</span><span class="p">:],</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
<span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_46_0.png" alt="png" /></p>

<p>Looks great!</p>

<p>Now, what’s up with the parameters <code class="language-plaintext highlighter-rouge">stepsize</code> and <code class="language-plaintext highlighter-rouge">n_total</code>?
We’ll discuss the step size first: it determines how far away a proposal state can be from the current state of the chain. It is thus a parameter of the proposal distribution $q$ and controls how big the random steps are which the Markov chain takes. If the step size is too large, the proposal state will often be in the tails of the distribution, where probability is low.
The Metropolis-Hastings sampler rejects most of these moves, meaning that the acceptance rate decreases and convergence is much slower.
See for yourself:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_and_display</span><span class="p">(</span><span class="n">init_state</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">,</span> <span class="n">n_total</span><span class="p">,</span> <span class="n">n_burnin</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">):</span>
    <span class="n">chain</span><span class="p">,</span> <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">build_MH_chain</span><span class="p">(</span><span class="n">init_state</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">,</span> <span class="n">n_total</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Acceptance rate: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">acceptance_rate</span><span class="p">))</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">plot_samples</span><span class="p">([</span><span class="n">state</span> <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">[</span><span class="n">n_burnin</span><span class="p">:]],</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
    <span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">sample_and_display</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]),</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: 0.105
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_49_1.png" alt="png" /></p>

<p>Not as cool, right?
Now you could think the best thing to do is do choose a tiny step size.
Turns out that this is not too smart either because then the Markov chain will explore the probability distribution only very slowly and thus again won’t converge as rapidly as with a well-adjusted step size:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_and_display</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: 0.983
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_51_1.png" alt="png" /></p>

<p>No matter how you choose the step size parameter, the Markov chain will eventually converge to its stationary distribution.
But it may take a looooong time.
The time we simulate the Markov chain for is set by the <code class="language-plaintext highlighter-rouge">n_total</code> parameter - it simply determines how many states of the Markov chain (and thus samples) we’ll end up with.
If the chain converges slowly, we need to increase <code class="language-plaintext highlighter-rouge">n_total</code> in order to give the Markov chain enough time to forget it’s initial state.
So let’s keep the tiny step size and increase the number of samples by increasing <code class="language-plaintext highlighter-rouge">n_total</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_and_display</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">25000</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: 0.991
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_53_1.png" alt="png" /></p>

<p>Sloooowly getting there…</p>

<h2 id="conclusions">Conclusions</h2>

<p>With these considerations, I conclude the first blog post of this series.
I hope you now understand the intuition behind the Metropolis-Hastings algorithm, its parameters and why it is an extremely useful tool to sample from non-standard probability distributions you might encounter out there in the wild.</p>

<p>I highly encourage you to play around with the code in this notebook - this way, you can learn how the algorithm behaves in various circumstances and deepen your understanding of it.
Go ahead and try out a non-symmetric proposal distribution!
What happens if you don’t adjust the acceptance criterion accordingly?
What happens if you try to sample from a bimodal distribution?
Can you think of a way to automatically tune the stepsize?
What are pitfalls here?
Try out and discover yourself!</p>

<p>In my next post, I will discuss the Gibbs sampler - a special case of the Metropolis-Hastings algorithm that allows you to approximately sample from a multivariate distribution by sampling from the conditional distributions.</p>

<p>Thanks for reading—go forward and sample!</p>

<h1 id="introduction-to-mcmc-part-ii-gibbs-sampling">Introduction to MCMC, Part II: Gibbs sampling</h1>

<p>In the <a href="https://www.tweag.io/posts/2019-10-25-mcmc-intro1.html">first blog post</a> of this series, we discussed Markov chains and the most elementary <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a> method, the <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings algorithm</a>, and used it to sample from a univariate distribution.
In this episode, we discuss another famous MCMC sampling algorithm: the <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampler</a>.
It is very useful to sample from multivariate distributions:
it reduces the complex problem of sampling from a joint distribution to sampling from the full conditional (meaning, conditioned on all other variables) distribution of each variable. 
That means that to sample from, say, $p(x,y)$, it is sufficient to be able to sample from $p(x|y)$ and $p(y|x)$, which might be considerably easier.
The problem of sampling from multivariate distributions often arises in Bayesian statistics, where inference of likely values for a parameter often entails sampling not only that parameter, but also additional parameters required by the statistical model.</p>

<h2 id="motivation">Motivation</h2>
<p>Why would splitting up sampling in this way be preferable?
Well, it might turn the problem of <strong>sampling from one untractable joint distribution into sampling from several well-known</strong>, tractable distributions.
If the latter (now conditional) distributions are still not tractable, at least you now can <strong>use different and well-suited samplers for each of them instead of sampling all variables with a one-size-fits-all sampler</strong>.
Take, for example, a bivariate normal distribution with density $p(x,y)$ that has very different variances for each variable:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">notebook</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="c1"># The np.sum() is for compatibility with sample_MH
</span>    <span class="k">return</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span> \
           <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">BivariateNormal</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">n_variates</span> <span class="o">=</span> <span class="mi">2</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mu1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu2</span> <span class="o">=</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigma1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span> <span class="o">=</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span>
        
    <span class="k">def</span> <span class="nf">log_p_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">log_p_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>        
        <span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                               <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">]])</span>
        <span class="n">inv_cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu1</span><span class="p">)</span> <span class="o">@</span> <span class="n">inv_cov_matrix</span> <span class="o">@</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu2</span><span class="p">).</span><span class="n">T</span>
        <span class="n">normalization</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_variates</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">det</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)))</span>
        
        <span class="k">return</span> <span class="n">kernel</span> <span class="o">-</span> <span class="n">normalization</span>               

    
<span class="n">bivariate_normal</span> <span class="o">=</span> <span class="n">BivariateNormal</span><span class="p">(</span><span class="n">mu1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">sigma1</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">@</code> is a recent-ish addition to Python and denotes the matrix multiplication operator.
Let’s plot this density:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1</span> <span class="kn">import</span> <span class="n">make_axes_locatable</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">xses</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">yses</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">log_density_values</span> <span class="o">=</span> <span class="p">[[</span><span class="n">bivariate_normal</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xses</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">yses</span><span class="p">]</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">xses</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">xses</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">dy</span> <span class="o">=</span> <span class="p">(</span><span class="n">yses</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">yses</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">extent</span> <span class="o">=</span> <span class="p">[</span><span class="n">xses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">dx</span><span class="p">,</span> <span class="n">xses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="n">yses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">dy</span><span class="p">,</span> <span class="n">yses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">dy</span><span class="p">]</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_density_values</span><span class="p">),</span> <span class="n">extent</span><span class="o">=</span><span class="n">extent</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
<span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="p">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s">'right'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s">'5%'</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">cb</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">)</span>
<span class="n">cb</span><span class="p">.</span><span class="n">set_label</span><span class="p">(</span><span class="s">'probability density'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_62_0.png" alt="png" /></p>

<p>Now you can try to sample from this using the previously discussed Metropolis-Hastings algorithm with a uniform proposal distribution.
Remember that in Metropolis-Hastings, a Markov chain is built by jumping a certain distance (“step size”) away from the current state, and accepting or rejecting the new state according to an acceptance probability.
A small step size will explore the possible values for \(x\) very slowly, while a large step size will have very poor acceptance rates for \(y\).
The Gibbs sampler allows us to use separate Metropolis-Hastings samplers for $x$ and $y$ - each with an appropriate step size.
Note that we could also choose a bivariate proposal distribution in the Metropolis-Hastings algorithm such that its variance in $x$-direction is larger than its variance in the $y$-direction, but let’s stick to this example for didactic purposes.</p>

<h2 id="the-systematic-scan-gibbs-sampler">The systematic scan Gibbs sampler</h2>
<p>So how does Gibbs sampling work?
The basic idea is that given the joint distribution $p(x, y)$ and a state $(x_i, y_i)$ from that distribution, you obtain a new state as follows:
first, you sample a new value for one variable, say, $x_{i+1}$, from its distribution conditioned on $y_i$, that is, from $p(x|y_i)$. Then, you sample a new state for the second variable, $y_{i+1}$, from its distribution conditioned on the previously drawn state for $x$, that is, from $p(y|x_{i+1})$.
This two-step procedure can be summarized as follows: 
\(\begin{align} x_{i+1} \sim&amp; \ p(x|y_i) \\
              y_{i+1} \sim&amp; \ p(y|x_{i+1})
\end{align}\)
This is then iterated to build up the Markov chain.
For more than two variables, the procedure is analogous: you pick a fixed ordering and draw one variable after the other, each conditioned on, in general, a mix of old and new values for all other variables.[^1]
Fixing an ordering, like this, is called a <em>systematic scan</em>, an alternative is the <em>random scan</em> where we’d randomly pick a new ordering at each iteration.</p>

<p>Implementing this Gibbs sampler for the above example is extremely simple, because the two variables are independent ($p(x|y)=p(x)$ and $p(y|x)=p(y)$).
We sample each of them with a Metropolis-Hastings sampler, implemented in the <a href="https://www.tweag.io/posts/2019-10-25-mcmc-intro1.html">first blog post</a> as the <code class="language-plaintext highlighter-rouge">sample_MH</code> function.
As a reminder, that function takes as arguments, in that order,</p>
<ul>
  <li>the old state of a Markov chain (a one-dimensional <code class="language-plaintext highlighter-rouge">numpy</code> array),</li>
  <li>a function returning the logarithm of the probability density function (PDF) to sample from,</li>
  <li>a real number representing the step size for the uniform proposal distribution, from which a new state is proposed.</li>
</ul>

<p>We then use <code class="language-plaintext highlighter-rouge">sample_MH</code> in the following, short function which implements the <strong>systematic scan Gibbs sampler</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_gibbs</span><span class="p">(</span><span class="n">old_state</span><span class="p">,</span> <span class="n">bivariate_dist</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">):</span>
    <span class="s">"""Draws a single sample using the systematic Gibbs sampling
    transition kernel
    
    Arguments:
    - old_state: the old (two-dimensional) state of a Markov chain
                 (a list containing two floats)
    - bivariate_dist: an object representing a bivariate distribution
                      (in our case, an instance of BivariateNormal)
    - stepsizes: a list of step sizes
    
    """</span>
    <span class="n">x_old</span><span class="p">,</span> <span class="n">y_old</span> <span class="o">=</span> <span class="n">old_state</span>
    
    <span class="c1"># for compatibility with sample_MH, change floats to one-dimensional
</span>    <span class="c1"># numpy arrays of length one
</span>    <span class="n">x_old</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_old</span><span class="p">])</span>
    <span class="n">y_old</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">y_old</span><span class="p">])</span>
    
    <span class="c1"># draw new x conditioned on y
</span>    <span class="n">p_x_y</span> <span class="o">=</span> <span class="n">bivariate_dist</span><span class="p">.</span><span class="n">log_p_x</span>
    <span class="n">accept_x</span><span class="p">,</span> <span class="n">x_new</span> <span class="o">=</span> <span class="n">sample_MH</span><span class="p">(</span><span class="n">x_old</span><span class="p">,</span> <span class="n">p_x_y</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># draw new y conditioned on x
</span>    <span class="n">p_y_x</span> <span class="o">=</span> <span class="n">bivariate_dist</span><span class="p">.</span><span class="n">log_p_y</span>
    <span class="n">accept_y</span><span class="p">,</span> <span class="n">y_new</span> <span class="o">=</span> <span class="n">sample_MH</span><span class="p">(</span><span class="n">y_old</span><span class="p">,</span> <span class="n">p_y_x</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Don't forget to turn the one-dimensional numpy arrays x_new, y_new
</span>    <span class="c1"># of length one back into floats
</span>    
    <span class="k">return</span> <span class="p">(</span><span class="n">accept_x</span><span class="p">,</span> <span class="n">accept_y</span><span class="p">),</span> <span class="p">(</span><span class="n">x_new</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">sample_gibbs</code> function will yield one single sample from <code class="language-plaintext highlighter-rouge">bivariate_normal</code>.
As we did in the previous blog post for the Metropolis-Hastings algorithm, we now write a function that repeatedly runs <code class="language-plaintext highlighter-rouge">sample_gibbs</code> to build up a Markov chain and call it:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_gibbs_chain</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">,</span> <span class="n">n_total</span><span class="p">,</span> <span class="n">bivariate_dist</span><span class="p">):</span>
    <span class="s">"""Builds a Markov chain by performing repeated transitions using
    the systematic Gibbs sampling transition kernel
    
    Arguments:
    - init: an initial (two-dimensional) state for the Markov chain
            (a list containing two floats)
    - stepsizes: a list of step sizes of type float
    - n_total: the total length of the Markov chain
    - bivariate_dist: an object representing a bivariate distribution
                      (in our case, an instance of BivariateNormal)
    
    """</span>
    <span class="n">init_x</span><span class="p">,</span> <span class="n">init_k</span> <span class="o">=</span> <span class="n">init</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>
    <span class="n">acceptances</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_total</span><span class="p">):</span>
        <span class="n">accept</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="n">sample_gibbs</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">bivariate_dist</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">)</span>
        <span class="n">chain</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>        
        <span class="n">acceptances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">accept</span><span class="p">)</span>
    
    <span class="n">acceptance_rates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">acceptances</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Acceptance rates: x: {:.3f}, y: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">acceptance_rates</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                          <span class="n">acceptance_rates</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="c1">#print(acceptances)
</span>    <span class="k">return</span> <span class="n">chain</span> 

<span class="n">stepsizes</span> <span class="o">=</span> <span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">initial_state</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">build_gibbs_chain</span><span class="p">(</span><span class="n">initial_state</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">bivariate_normal</span><span class="p">)</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rates: x: 0.463, y: 0.455
</code></pre></div></div>

<font color="red">**why the acceptance rate is not 1 ... which**</font>
<p>\(\begin{aligned}
&amp; q\left(x^{\prime} \mid x_t\right)=\delta\left(x_{\backslash i}^{\prime}-x_{t, \backslash i}\right) p\left(x_i^{\prime} \mid x_{t, \backslash i}\right) \\
&amp; p\left(x^{\prime}\right)=p\left(x_i^{\prime} \mid x_{\backslash}^{\prime}\right) p\left(x_{\backslash i}^{\prime}\right)=p\left(x_i^{\prime} \mid x_{t, \backslash i}\right) p\left(x_{t, \backslash i}\right) \\
&amp; \text { acceptance rate: } \\
&amp; a=\frac{p\left(x^{\prime}\right)}{p\left(x_t\right)} \cdot \frac{q\left(x_t \mid x^{\prime}\right)}{q\left(x^{\prime} \mid x_t\right)} \quad=\frac{p\left(x_i^{\prime} \mid x_{t, \backslash i}\right) p\left(x_{t, \backslash i}\right)}{p\left(x_{t i} \mid x_{t, \backslash i}\right) p\left(x_{t, \backslash i}\right)} \cdot \frac{q\left(x_t \mid x^{\prime}\right)}{\delta\left(x_{\backslash i}^{\prime}-x_{t, \backslash i}\right) p\left(x_i^{\prime} \mid x_{t, \backslash i}\right)} \\
&amp; =\frac{q\left(x_t \mid x^{\prime}\right)}{p\left(x_{t i} \mid x_{t, \backslash i}\right) \delta\left(x_{\backslash i}^{\prime}-x_{t, \backslash i}\right)} \quad=1 \\
&amp;
\end{aligned}\)</p>

<p>Tada! 
We used two very different step sizes and achieved very similar acceptance rates with both.<br />
We now plot a 2D histogram of the samples (with the estimated probability density color-coded) and the marginal distributions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_samples_2D</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">path_length</span><span class="p">,</span> <span class="n">burnin</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xlims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">ylims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)):</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ylims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)]</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">hist2d</span><span class="p">(</span><span class="o">*</span><span class="n">chain</span><span class="p">[</span><span class="n">burnin</span><span class="p">:].</span><span class="n">T</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">chain</span><span class="p">[:</span><span class="n">path_length</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'w'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> 
            <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlims</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylims</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
<span class="k">def</span> <span class="nf">plot_bivariate_samples</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">burnin</span><span class="p">,</span> <span class="n">pdf</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
   
    <span class="n">ax_c</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">rowspan</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colspan</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plot_samples_2D</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">burnin</span><span class="p">,</span> <span class="n">ax_c</span><span class="p">)</span>
    
    <span class="n">ax_t</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">rowspan</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colspan</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="n">ax_c</span><span class="p">)</span>
    <span class="n">plot_samples</span><span class="p">(</span><span class="n">chain</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">pdf</span><span class="p">.</span><span class="n">log_p_x</span><span class="p">,</span> <span class="n">ax_t</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">setp</span><span class="p">(</span><span class="n">ax_t</span><span class="p">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">visible</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">ax_t</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
    <span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">):</span>
        <span class="n">ax_t</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="n">spine</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">ax_r</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">rowspan</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colspan</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="n">ax_c</span><span class="p">)</span>
    <span class="n">plot_samples</span><span class="p">(</span><span class="n">chain</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">pdf</span><span class="p">.</span><span class="n">log_p_y</span><span class="p">,</span> <span class="n">ax_r</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s">'horizontal'</span><span class="p">,</span>
                 <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">setp</span><span class="p">(</span><span class="n">ax_r</span><span class="p">.</span><span class="n">get_yticklabels</span><span class="p">(),</span> <span class="n">visible</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">ax_r</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'bottom'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">):</span>
        <span class="n">ax_r</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="n">spine</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">plot_bivariate_samples</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">burnin</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">pdf</span><span class="o">=</span><span class="n">bivariate_normal</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_70_0.png" alt="png" /></p>

<p>Looking at the path the Markov chain takes, we see several horizontal and vertical lines.
These are Gibbs sampling steps in which only one of the Metropolis-Hastings moves was accepted.</p>

<h2 id="a-more-complex-example">A more complex example</h2>
<p>The previous example was rather trivial in the sense that both variables were independent.
Let’s discuss a more interesting example, which features both a discrete and a continuous variable.
We consider a mixture of two normal densities $p_\mathcal{N}(x; \mu, \sigma)$ with relative weights $w_1$ and $w_2$.
The PDF we want to sample from is then
\(p(x) = w_1p_\mathcal{N}(x; \mu_1, \sigma_1) + w_2p_\mathcal{N}(x; \mu_2, \sigma_2) \ \mbox .\)
This probability density is just a weighted sum of normal densities.
Let’s consider a concrete example, choosing the following mixture parameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mix_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mu1</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">sigma1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">w1</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">w2</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div></div>

<p>How does it look like?
Well, it’s a superposition of two normal distributions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">xspace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># densities of both components
</span><span class="n">first_component</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mix_params</span><span class="p">[</span><span class="s">'mu1'</span><span class="p">],</span> <span class="n">mix_params</span><span class="p">[</span><span class="s">'sigma1'</span><span class="p">]))</span>
                   <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xspace</span><span class="p">]</span>
<span class="n">second_component</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mix_params</span><span class="p">[</span><span class="s">'mu2'</span><span class="p">],</span> <span class="n">mix_params</span><span class="p">[</span><span class="s">'sigma2'</span><span class="p">]))</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xspace</span><span class="p">]</span>

<span class="c1"># apply component weights
</span><span class="n">first_component</span> <span class="o">=</span> <span class="n">mix_params</span><span class="p">[</span><span class="s">'w1'</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">first_component</span><span class="p">)</span>
<span class="n">second_component</span> <span class="o">=</span> <span class="n">mix_params</span><span class="p">[</span><span class="s">'w2'</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">second_component</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">first_component</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">first_component</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"1st component"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">second_component</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">second_component</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"2nd component"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="n">spine</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_75_0.png" alt="png" /></p>

<p>Inspired by this figure, we can also make the mixture nature of that density more explicit by introducing an additional integer variable $ k \in {1,2} $ which enumerates the mixture components. – <strong>In GMM the K is unknown,and is a hidden/latent varible</strong></p>

<p>This will allow us to highlight several features and properties of the Gibbs sampler and to introduce an important term in probability theory along the way.
Having introduced a second variable means that we can consider several probability distributions:</p>

<ul>
  <li>$p(x,k)$: the joint distribution of $x$ and $k$ tells us how probable it is to find a value for $x$ and a value for $k$ “at the same time” and is given by
\(p(x,k) = w_kp_\mathcal{N}(x; \mu_k, \sigma_k) \ \mbox .\)</li>
  <li>$p(x|k)$: the conditional distribution of $x$ given $k$ tells us the probability of $x$ for a certain $k$. For example, if $k=1$, what is $p(x|k)$? Setting $k=1$ means we’re considering only the first mixture component, which is a normal distribution with mean $\mu_1$ and standard deviation $\sigma_1$ and thus $p(x|k=1)=p_\mathcal{N}(x; \mu_1, \sigma_1)$. In general we then have
\(p(x|k) = p_\mathcal{N}(x; \mu_k, \sigma_k) \ \mbox .\)</li>
  <li>$p(k|x)$: assuming a certain value $x$, this probability distribution tells us for each $k$ the probability with which you would draw $x$ from the mixture component with index $k$. This probability is non-trivial, as the mixture components overlap and each $x$ thus has a non-zero probability in each component. But <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a> saves us and yields
\(p(k|x) = \frac{p(x|k) p(k)}{p(x)} \ \mbox .\)</li>
  <li>**<font color="red">$p(k)$: this is the probability of choosing a mixture component $k$ irrespective of $x$ and is given by the mixture weights $w_k$.**</font></li>
</ul>

<p>The probability distributions $p(x)$ and $p(k)$ are related to the joint distribution $p(x,k)$ by a procedure called <a href="https://en.wikipedia.org/wiki/Marginal_distribution"><em>marginalization</em></a>.
We marginalize $p(x,k)$ over, say, $k$, when we are only interested in the probability of $x$, independent of a specific value for $k$.
That means that the probability of $x$ is the sum of the probability of $x$ when $k=1$ plus the probability of $x$ when $k=2$, or, formally,
\(p(x)=\sum_{ k \in \{1, 2\} } p(x,k) \ \mbox .\)</p>

<p>With these probability distributions, we have all the required ingredients for setting up a Gibbs sampler.
We can then sample from $p(x,k)$ and reconstruct $p(x)$ by marginalization.
As marginalization means “not looking a variable”, obtaining samples from $p(x)$ given samples from $p(x,k)$ just amounts to discarding the sampled values for $k$.</p>

<p>Let’s first implement a Gaussian mixture with these conditional distributions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mu1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu2</span> <span class="o">=</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigma1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span> <span class="o">=</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span>
        
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma1</span><span class="p">),</span>
                            <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">log_p_x_k</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="c1"># logarithm of p(x|k)
</span>        <span class="n">mu</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mu1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu2</span><span class="p">)[</span><span class="n">k</span><span class="p">]</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="p">)[</span><span class="n">k</span><span class="p">]</span>
    
        <span class="k">return</span> <span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">p_k_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># p(k|x) using Bayes' theorem
</span>        <span class="n">mu</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mu1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu2</span><span class="p">)[</span><span class="n">k</span><span class="p">]</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="p">)[</span><span class="n">k</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w2</span><span class="p">)[</span><span class="n">k</span><span class="p">]</span>
        <span class="n">log_normalization</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_normalization</span><span class="p">)</span>
</code></pre></div></div>

<p>The interesting point here (and, in fact, the reason I chose this example) is that $p(x|k)$ is a probability density for a <strong>continuous</strong> variable $x$, while $p(k|x)$ is a probability distribution for a <strong>discrete</strong> variable. This means we will have to choose two very different sampling methods.
While we could just use a built-in <code class="language-plaintext highlighter-rouge">numpy</code> function to draw from the normal distributions $p(x|k)$, we will use Metropolis-Hastings.
The freedom to do this really demonstrates the flexibility we have in choosing samplers for the conditional distributions.</p>

<p>So we need to reimplement <code class="language-plaintext highlighter-rouge">sample_gibbs</code> and <code class="language-plaintext highlighter-rouge">build_gibbs_chain</code>, whose arguments are very similar to the previous implementation, but with a slight difference: the states now consist of a float for the continuous variabe and an integer for the mixture component, and <strong>instead of a list of stepsizes we just need one single stepsize, as we have only one variable to be sampled with Metropolis-Hastings.</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_gibbs</span><span class="p">(</span><span class="n">old_state</span><span class="p">,</span> <span class="n">mixture</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">):</span>
    <span class="s">"""Draws a single sample using the systematic Gibbs sampling
    transition kernel
    
    Arguments:
    - old_state: the old (two-dimensional) state of a Markov chain
                 (a list containing a float and an integer representing 
                 the initial mixture component)
    - mixture: an object representing a mixture of densities
               (in our case, an instance of GaussianMixture)
    - stepsize: a step size of type float 
    
    """</span>
    <span class="n">x_old</span><span class="p">,</span> <span class="n">k_old</span> <span class="o">=</span> <span class="n">old_state</span>
    
    <span class="c1"># for compatibility with sample_MH, change floats to one-dimensional
</span>    <span class="c1"># numpy arrays of length one
</span>    <span class="n">x_old</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_old</span><span class="p">])</span>
    
    <span class="c1"># draw new x conditioned on k
</span>    <span class="n">x_pdf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">mixture</span><span class="p">.</span><span class="n">log_p_x_k</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k_old</span><span class="p">)</span>
    <span class="n">accept</span><span class="p">,</span> <span class="n">x_new</span> <span class="o">=</span> <span class="n">sample_MH</span><span class="p">(</span><span class="n">x_old</span><span class="p">,</span> <span class="n">x_pdf</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">)</span>
    
    <span class="c1"># ... turn the one-dimensional numpy arrays of length one back
</span>    <span class="c1"># into floats
</span>    <span class="n">x_new</span> <span class="o">=</span> <span class="n">x_new</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># draw new k conditioned on x , just draw based on k_probabilities
</span>    <span class="n">k_probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="n">mixture</span><span class="p">.</span><span class="n">p_k_x</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x_new</span><span class="p">),</span> <span class="n">mixture</span><span class="p">.</span><span class="n">p_k_x</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_new</span><span class="p">))</span>
    <span class="c1">#print(k_probabilities)
</span>    <span class="n">jump_probability</span> <span class="o">=</span> <span class="n">k_probabilities</span><span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_old</span><span class="p">]</span>
    <span class="c1">#print(jump_probability)
</span>    <span class="n">k_new</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">k_probabilities</span><span class="p">)</span>
    <span class="c1">#print(k_new)
</span>    <span class="k">return</span> <span class="n">accept</span><span class="p">,</span> <span class="n">jump_probability</span><span class="p">,</span> <span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">k_new</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">build_gibbs_chain</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">,</span> <span class="n">n_total</span><span class="p">,</span> <span class="n">mixture</span><span class="p">):</span>
    <span class="s">"""Builds a Markov chain by performing repeated transitions using
    the systematic Gibbs sampling transition kernel
    
    Arguments:
    - init: an initial (two-dimensional) state of a Markov chain
            (a list containing a one-dimensional numpy array
            of length one and an integer representing the initial
            mixture component)
    - stepsize: a step size of type float
    - n_total: the total length of the Markov chain
    - mixture: an object representing a mixture of densities
               (in our case, an instance of GaussianMixture)
    
    """</span>
    <span class="n">init_x</span><span class="p">,</span> <span class="n">init_k</span> <span class="o">=</span> <span class="n">init</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>
    <span class="n">acceptances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">jump_probabilities</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_total</span><span class="p">):</span>
        <span class="n">accept</span><span class="p">,</span> <span class="n">jump_probability</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="n">sample_gibbs</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">mixture</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">)</span>
        <span class="n">chain</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
        <span class="n">jump_probabilities</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">jump_probability</span><span class="p">)</span>
        <span class="n">acceptances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">accept</span><span class="p">)</span>
    
    <span class="n">acceptance_rates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">acceptances</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Acceptance rate: x: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">acceptance_rates</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Average probability to change mode: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jump_probabilities</span><span class="p">)))</span>
    
    <span class="k">return</span> <span class="n">chain</span>

<span class="n">mixture</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="o">**</span><span class="n">mix_params</span><span class="p">)</span>
<span class="n">stepsize</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">initial_state</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">build_gibbs_chain</span><span class="p">(</span><span class="n">initial_state</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">mixture</span><span class="p">)</span>
<span class="n">burnin</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">[</span><span class="n">burnin</span><span class="p">:]]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: x: 0.623
Average probability to change mode: 0.07858658930962827
</code></pre></div></div>

<p>Plotting a histogram of our samples shows that the Gibbs sampler correctly reproduces the desired Gaussian mixture:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">plot_samples</span><span class="p">(</span><span class="n">x_states</span><span class="p">,</span> <span class="n">mixture</span><span class="p">.</span><span class="n">log_prob</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">xlims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="n">spine</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_81_0.png" alt="png" /></p>

<p>(<a href="#zphilip48-1"><em>this words is unclear to me what exactly means, I guess author try to say how to jump from one componet to anther component etc, but if probability is low then jumping might stucked for the low probability etc</em></a>)You might wonder why we’re also printing the average probability for the chain to sample from the component it is currently <em>not</em> in.</p>

<p>If this probability is very low, the Markov chain will get stuck for some time in the current mode and thus will have difficulties exploring the distribution rapidly. 
The quantity of interest here is $p(k|x)$: 
it is the probability of a certain component $k$ given a certain value $x$ and can be very low if the components are more separated and $x$ is more likely to be in the component which is not $k$. 
Let’s explore this behavior by increasing the separation between the means of the mixture components:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mix_params</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'mu1': 1.0, 'mu2': 2.0, 'sigma1': 0.5, 'sigma2': 0.2, 'w1': 0.3, 'w2': 0.7}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mixture</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">mu1</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">sigma1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">w1</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">w2</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">stepsize</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">initial_state</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">build_gibbs_chain</span><span class="p">(</span><span class="n">initial_state</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">mixture</span><span class="p">)</span>
<span class="n">burnin</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">[</span><span class="n">burnin</span><span class="p">:]]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: x: 0.555
Average probability to change mode: 2.8224059844723072e-06
</code></pre></div></div>

<p>Let’s plot the samples and the true distribution and see how the Gibbs sampler performs in this case:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">plot_samples</span><span class="p">(</span><span class="n">x_states</span><span class="p">,</span> <span class="n">mixture</span><span class="p">.</span><span class="n">log_prob</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">xlims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="n">spine</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_86_0.png" alt="png" /></p>

<p>You should see the probability decrease significantly and perhaps one of the modes being strongly over- and the other undersampled. 
The lesson here is that the Gibbs sampler might produce highly correlated samples.
Again—in the limit of many, many samples—the Gibbs sampler will reproduce the correct distribution, but you might have to wait a long time.</p>

<h2 id="conclusions-1">Conclusions</h2>
<p>By now, I hope you have a basic understanding of why Gibbs sampling is an important MCMC technique, how it works and why it can produce highly correlated samples.
I encourage you again to download the <a href="https://github.com/tweag/blog-resources/blob/master/mcmc-intro/mcmc_introduction.ipynb">full notebook</a> and play around with the code:
you could try using the <code class="language-plaintext highlighter-rouge">normal</code> function from the <code class="language-plaintext highlighter-rouge">numpy.random</code> module instead of Metropolis-Hastings in both examples or implement a <em>random</em> scan, in which the order in which you sample from the conditional distributions is chosen randomly.</p>

<p>Or you could read about and implement the <a href="https://en.wikipedia.org/wiki/Gibbs_sampling#Collapsed_Gibbs_sampler">collapsed Gibbs sampler</a>, which allows you to perfectly sample the Gaussian mixture example by sampling from $p(k)$ instead of $p(k|x)$. 
Or you can just wait a little more for the next post in the series, which will be about Hybrid Monte Carlo (HMC), 
a fancy Metropolis-Hastings variant which takes into account the derivative of the log-probability of interest to propose better, less correlated, states!</p>

<h2 id="footnotes">Footnotes</h2>
<ol>
  <li>It’s important to note, though, that the transition kernel given py the above procedure does <em>not</em> define a detailed-balanced transition kernel for a Markov chain on the joint space of $x$ and $y$. One can show, though, that for each single variable, this procedure is a detailed-balanced transition kernel and the Gibbs sampler thus constitutes a composition of Metropolis-Hastings steps with acceptance probability 1. For details, see, for example, <a href="https://stats.stackexchange.com/a/118453">this stats.stackexchange.com answer</a>.</li>
</ol>

<h1 id="introduction-to-mcmc-part-iii-hamiltonian-monte-carlo-tbs">Introduction to MCMC, part III: Hamiltonian Monte Carlo (TBS)</h1>

<p>This is the third post of a series of blog posts about Markov Chain Monte Carlo (MCMC) techniques:</p>

<ul>
  <li><a href="https://www.tweag.io/posts/2019-10-25-mcmc-intro1.html">Part I: Metropolis-Hastings</a></li>
  <li><a href="https://www.tweag.io/posts/2020-01-09-mcmc-intro2.html">Part II: Gibbs sampling</a></li>
</ul>

<p>So far, we discussed two MCMC algorithms: the Metropolis-Hastings algorithm and the Gibbs sampler.
Both algorithms can produce highly correlated samples—Metropolis-Hastings has a pronounced random walk behaviour, while the Gibbs sampler can easily get trapped when variables are highly correlated.
In this blog post, we will take advantage of an additional piece of information about a distribution—its shape—and learn about Hamiltonian Monte Carlo, an algorithm <a href="https://doi.org/10.1016%2F0370-2693%2887%2991197-X">whose roots are in quantum physics</a>.</p>

<p>As always, you can <a href="https://github.com/tweag/blog-resources/blob/master/mcmc-intro/mcmc_introduction.ipynb">download the corresponding IPython notebook</a> and play around with the code to your heart’s desire!</p>

<h2 id="physical-intuition">Physical intuition</h2>
<p>As mentioned above, when doing Metropolis-Hastings sampling with a naive proposal distribution, we are essentially performing a random walk without taking into account any additional information we may have about the distribution we want to sample from. 
We can do better!<br />
If the density function we want to sample from is differentiable, we have access to its local shape through its <em>derivative</em>.
This derivative tells us, at each point $x$, how the value of the density $p(x)$ increases or decreases depending on how we change the $x$.
That means we should be able to use the derivative of $p(x)$ to propose states with high probabilities—which is exactly the key idea of Hamiltonian Monte Carlo (HMC).</p>

<p>The intuition behind HMC is that we can interpret a random walker as a particle moving under the effect of <em>forces</em> attracting it to higher-probability zones.
That is easier to see with an example: in the next image we show both a probability density $p(x)$ and its corresponding negative log-density $-\log p(x)$, which we will call the <em>potential energy</em> $E(x)$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">xspace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">unnormalized_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">xspace</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">energies</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">xspace</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">unnormalized_probs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">"$p(x)$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">energies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">"$E(x)=-\log\ p(x)$"</span><span class="p">)</span>
<span class="n">prop</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-|&gt;,head_width=0.4,head_length=0.8"</span><span class="p">,</span>
            <span class="n">shrinkA</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">shrinkB</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_index1</span> <span class="o">=</span> <span class="mi">75</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">((</span><span class="n">xspace</span><span class="p">[</span><span class="n">x_index1</span><span class="p">],),</span> <span class="p">(</span><span class="n">energies</span><span class="p">[</span><span class="n">x_index1</span><span class="p">],),</span> <span class="n">color</span><span class="o">=</span><span class="s">"k"</span><span class="p">)</span>
<span class="n">a_start1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="n">xspace</span><span class="p">[</span><span class="n">x_index1</span><span class="p">],</span> <span class="n">energies</span><span class="p">[</span><span class="n">x_index1</span><span class="p">]))</span>
<span class="n">a_end1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="n">xspace</span><span class="p">[</span><span class="n">x_index1</span><span class="p">]</span> <span class="o">-</span> <span class="n">xspace</span><span class="p">[</span><span class="n">x_index1</span><span class="p">],</span> <span class="n">energies</span><span class="p">[</span><span class="n">x_index1</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">""</span><span class="p">,</span><span class="n">a_end1</span><span class="p">,</span> <span class="n">a_start1</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">prop</span><span class="p">)</span>
<span class="n">text_pos1</span> <span class="o">=</span> <span class="p">(</span><span class="n">a_start1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">a_end1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">a_start1</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">a_end1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.075</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="o">*</span><span class="n">text_pos1</span><span class="p">,</span> <span class="sa">r</span><span class="s">"force"</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s">"center"</span><span class="p">)</span>

<span class="n">x_index2</span> <span class="o">=</span> <span class="mi">38</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">((</span><span class="n">xspace</span><span class="p">[</span><span class="n">x_index2</span><span class="p">],),</span> <span class="p">(</span><span class="n">energies</span><span class="p">[</span><span class="n">x_index2</span><span class="p">],),</span> <span class="n">color</span><span class="o">=</span><span class="s">"k"</span><span class="p">)</span>
<span class="n">a_start2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="n">xspace</span><span class="p">[</span><span class="n">x_index2</span><span class="p">],</span> <span class="n">energies</span><span class="p">[</span><span class="n">x_index2</span><span class="p">]))</span>
<span class="n">a_end2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="n">xspace</span><span class="p">[</span><span class="n">x_index2</span><span class="p">]</span> <span class="o">-</span> <span class="n">xspace</span><span class="p">[</span><span class="n">x_index2</span><span class="p">],</span> <span class="n">energies</span><span class="p">[</span><span class="n">x_index2</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">""</span><span class="p">,</span><span class="n">a_end2</span><span class="p">,</span> <span class="n">a_start2</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">prop</span><span class="p">)</span>
<span class="n">text_pos2</span> <span class="o">=</span> <span class="p">(</span><span class="n">a_start2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">a_end2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">a_start2</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">a_end2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.075</span><span class="p">,</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="o">*</span><span class="n">text_pos2</span><span class="p">,</span> <span class="sa">r</span><span class="s">"force"</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s">"center"</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"x"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
<span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="n">spine</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_93_0.png" alt="png" /></p>

<p>As you can see, the potential energy $E(x)$ looks like a mountainous landscape that attracts the particle (shown at two different positions) to its bottom—the steeper the landscape, the stronger will be the force pulling it towards the bottom.
What’s more, the bottom of this landscape coincides with the region with the largest probability of $p(x)$!
That means that if we are able to predict where the particle will move to given its position and velocity, we can use the result of that prediction as a proposal state for a fancy version of Metropolis-Hastings.</p>

<p>This kind of prediction problem is very common and well-studied in physics—think of simulating the movement of planets around the sun, which is determined by gravitational forces.
We can thus borrow the well-known theory and methods from classical (“Hamiltonian”) mechanics to implement our method!</p>

<p>While HMC is quite easy to implement, the theory behind it is actually far from trivial.
We will skip a lot of detail, but provide additional information on physics and numerics in detailed footnotes.</p>

<h2 id="the-hmc-algorithm">The HMC algorithm</h2>

<p>In physics, the force acting on a particle can be calculated as the derivative (or gradient) of a <em>potential energy</em> $E(x)$. As said above, the negative log-density will play the role of that potential energy for us:
\(E(x)=-\log p(x)\)
Our particle does not only have a potential energy, but also a <em>kinetic</em> energy, which is the energy stored in its movement.
The kinetic energy depends on the particle’s momentum—the product of its mass and velocity.
Since in HMC, the mass is often set to one, we will denote the momentum with the letter $v$ (for velocity), meaning that the kinetic energy is defined as$^1$</p>

\[K(v)=\frac{|v|^2}{2} \mathrm .\]

<p>Just as the position $x$, the momentum $v$ is generally a vector, whose length is denoted by $|v|$.
With these ingredients in place, we can then define the total energy of the particle as</p>

\[H(x,v)=K(v)+E(x) \mathrm .\]

<p>In classical physics, this quantity completely determines the movement of a particle.</p>

<p>HMC introduces the momentum of the particle as an auxiliary variable and samples the joint distribution of the particle’s position and momentum.
Under some assumptions$^2$, this joint distribution is given by</p>

\[\begin{aligned}
p(x,v) &amp;\propto \exp\{-H(x,v)\} \\ 
&amp;= \exp\{-K(v)\} \times \exp\{-E(x)\} \\
&amp;= \exp\left\{-\frac{|v|^2}{2}\right\} \times p(x)\mathrm .
\end{aligned}\]

<p>We can sample from $p(x,v)$ efficiently by means of an involved Metropolis-Hastings method:
starting from a current state $x$, we draw a random initial momentum $v$$^3$ (that “3” is a footnote) from a normal distribution and simulate the movement of the particle for some time to obtain a proposal state $x^*$.</p>

<p>When the simulation stops, the fictive particle has some position $x^<em>$ and some momentum $v^</em>$, which serve as our proposal states.
If we had a perfectly accurate simulation, the total energy $H(x,v)$ would remain the same and the final position would be a perfectly fine sample from the joint distribution.
Unfortunately, all numerical simulation is only approximative, and we have to account for errors.
As we have done before in the original Metropolis-Hastings algorithm, we can correct for the bias with a Metropolis criterion.$^4$
In our case, the acceptance probability is given by</p>

\[p_\mathrm{acc}^\mathrm{HMC} = \min\left\{1, \exp\{-[H(x^*, v^*) - H(x, v)]\} \right\} \mathrm .\]

<p>We are not interested in the momentum $v^<em>$: if the move is accepted, we only store the position $x^</em>$ as the next state of the Markov chain.</p>

<h3 id="implementation">Implementation</h3>
<p>In terms of implementation, the only part of HMC that is not immediately obvious is the simulation of the particle’s movement.
The simulation of the movement of the particle works by discretizing time into steps of a certain size.
Position and momentum of a particle are then updated in an alternating fashion.
In HMC, this is usually done as follows$^5$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">leapfrog</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">trajectory_length</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">timestep</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trajectory_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">timestep</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">v</span> <span class="o">-=</span> <span class="n">timestep</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="n">timestep</span> <span class="o">*</span> <span class="n">v</span>
    <span class="n">v</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">timestep</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">v</span>
</code></pre></div></div>

<p>Now we already have all we need to proceed to the implementation of HMC!
Here it is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_HMC</span><span class="p">(</span><span class="n">x_old</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">log_prob_gradient</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">trajectory_length</span><span class="p">):</span>
    <span class="c1"># switch to physics mode!
</span>    <span class="k">def</span> <span class="nf">E</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="n">log_prob_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">K</span><span class="p">(</span><span class="n">v</span><span class="p">):</span> <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">v</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">H</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span> <span class="k">return</span> <span class="n">K</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="n">E</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Metropolis acceptance probability, implemented in "logarithmic space"
</span>    <span class="c1"># for numerical stability:
</span>    <span class="k">def</span> <span class="nf">log_p_acc</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">v_new</span><span class="p">,</span> <span class="n">x_old</span><span class="p">,</span> <span class="n">v_old</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">H</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">v_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">H</span><span class="p">(</span><span class="n">x_old</span><span class="p">,</span> <span class="n">v_old</span><span class="p">)))</span>

    <span class="c1"># give a random kick to particle by drawing its momentum from p(v)
</span>    <span class="n">v_old</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_old</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># approximately calculate position x_new and momentum v_new after
</span>    <span class="c1"># time trajectory_length  * timestep
</span>    <span class="n">x_new</span><span class="p">,</span> <span class="n">v_new</span> <span class="o">=</span> <span class="n">leapfrog</span><span class="p">(</span><span class="n">x_old</span><span class="p">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">v_old</span><span class="p">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">gradient</span><span class="p">,</span> 
                            <span class="n">timestep</span><span class="p">,</span> <span class="n">trajectory_length</span><span class="p">)</span>

    <span class="c1"># accept / reject based on Metropolis criterion
</span>    <span class="n">accept</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">())</span> <span class="o">&lt;</span> <span class="n">log_p_acc</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">v_new</span><span class="p">,</span> <span class="n">x_old</span><span class="p">,</span> <span class="n">v_old</span><span class="p">)</span>

    <span class="c1"># we consider only the position x (meaning, we marginalize out v)
</span>    <span class="k">if</span> <span class="n">accept</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">accept</span><span class="p">,</span> <span class="n">x_new</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">accept</span><span class="p">,</span> <span class="n">x_old</span>
</code></pre></div></div>

<p>And, analogously to the other MCMC algorithms we discussed before, here’s a function to build up a Markov chain using HMC transitions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_HMC_chain</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">trajectory_length</span><span class="p">,</span> <span class="n">n_total</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
    <span class="n">n_accepted</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_total</span><span class="p">):</span>
        <span class="n">accept</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">sample_HMC</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">copy</span><span class="p">(),</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span>
                                   <span class="n">timestep</span><span class="p">,</span> <span class="n">trajectory_length</span><span class="p">)</span>
        <span class="n">chain</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">n_accepted</span> <span class="o">+=</span> <span class="n">accept</span>

    <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">n_accepted</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">n_total</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">chain</span><span class="p">,</span> <span class="n">acceptance_rate</span>
</code></pre></div></div>

<p>As an example, let’s consider a two-dimensional Gaussian distribution.
Here is its log-probability, neglecting the normalization constant:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we need to calculate the gradient of the log-probability. 
Its $i$-th component is given by $\frac{\partial}{\partial x^i} E(x)$, where $x^i$ is the $i$-th component of the variable vector $x$.
Doing the math, you’ll end up with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_prob_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="n">x</span>
</code></pre></div></div>

<p>Now we’re ready to test the HMC sampler:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chain</span><span class="p">,</span> <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">build_HMC_chain</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span>
                                         <span class="n">log_prob</span><span class="p">,</span> <span class="n">log_prob_gradient</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Acceptance rate: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">acceptance_rate</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: 0.622
</code></pre></div></div>

<p>To display the result, we plot a two-dimensional histogram of the sampled states and the first 200 states of the chain:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">plot_samples_2D</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xlims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">),</span> <span class="n">ylims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_110_0.png" alt="png" /></p>

<p>We see that the HMC states are indeed quite far away from each other—the Markov chain jumps relatively large distances.</p>

<p>How does Metropolis-Hastings do on the same distribution?
Let’s have a look and run a Metropolis-Hastings sampler with the same initial state and a stepsize chosen such that a similar acceptance rate is achieved.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chain</span><span class="p">,</span> <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">build_MH_chain</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Acceptance rate: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">acceptance_rate</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Acceptance rate: 0.623
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">plot_samples_2D</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xlims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">),</span> <span class="n">ylims</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_114_0.png" alt="png" /></p>

<p>What you see is that Metropolis-Hastings takes a much longer time to actually find the relevant region of high probability and then explore it.
This means that, with a similar acceptance rate, HMC produces much less correlated samples and thus will need fewer steps to achieve the same sampling quality.</p>

<p>While this advantage is even more pronounced in higher dimensions, it doesn’t come for free:
numerically approximating the equations of motions of our fictive particle takes up quite some computing power, especially if evaluating the log-probability gradient is expensive.</p>

<h3 id="choosing-the-parameters">Choosing the parameters</h3>
<p>Two parameters effectively determine the distance of the proposal state to the current state: the integration time step and the number of steps to perform.<br />
Given a constant number of steps, the larger the integration time step, the further away from the current state the proposal will be.
But at the same time, larger time steps implies less accurate results, which means lower acceptance probabilities.<br />
The number of steps has a different impact:
given a fixed step size, the more steps performed, the less correlated the proposal will be to the current state.
There is a risk of the trajectory doubling back and wasting precious computation time, but
<a href="http://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf">NUTS</a>, a neat variant of HMC which optimizes the number of integration steps, addresses exactly this problem.<br />
For a basic implementation of HMC, one can use a fixed number of integration steps and automatically adapt the timestep, targeting a reasonable acceptance rate (e.g. 50%) during a burn-in period.
The adapted timestep can then be used for the final sampling.</p>

<h3 id="drawbacks">Drawbacks</h3>
<p>Unfortunately, there’s no such thing as a free lunch.
While the big advantage of HMC, namely, less correlated samples with high acceptance rates, are even more pronounced in real-world applications than in our toy example, there are also some disadvantages.</p>

<p>First of all, you can only easily use HMC when you have continuous variables since the gradient of the log-probability for non-continuous variables is not defined.
You might still be able to use HMC if you have a mix of discrete and continuous variables by embedding HMC in a <a href="https://www.tweag.io/posts/2020-01-09-mcmc-intro2.html">Gibbs sampler</a>.
Another issue is that the gradient can be very tedious to calculate and its implementation error-prone.
Luckily, popular probabilistic programming packages (see below) which implement HMC as their main sampling workhorse, take advantage of <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a>, which relieves the user from worrying about gradients.</p>

<p>Finally, as we just discussed, there are quite a few free parameters, the suboptimal tuning of which can severely decrease performance.
However, since HMC is a very popular algorithm, many implementations come with all sorts of useful heuristics to choose parameters, which makes HMC easy to profit from in spite of these drawbacks.</p>

<h2 id="conclusion">Conclusion</h2>
<p>I hope that at this point, you have a general understanding of the idea behind HMC and both its advantages and disadvantages.
HMC is still subject to active research and if you like what you just read (and have the required math / stats / physics skills), there are many more interesting things to learn.$^6$<br />
To test your understanding of HMC, here’s a little brain teaser, which also invites you to play around with the code:
what would happen if you made a mistake in the gradient calculation or implementation?
The answer is: you still get a perfectly valid MCMC algorithm, but acceptance rate will decrease.
But why?</p>

<p>I would also like to point out that there are several great, accessible HMC resources out there. For example, a classic introduction is <a href="https://arxiv.org/pdf/1206.1901.pdf">MCMC using Hamiltionian dynamics</a> and <a href="https://chi-feng.github.io/mcmc-demo/app.html">these amazing visualizations</a> explain HMC and the influence of the parameters better than any words.</p>

<p>Whether you now decide to take a deep dive into HMC or you’re just content to have finally learned what kind of magic is working under the hood of probabilistic programming packages such as <a href="https://github.com/pymc-devs/pymc3">PyMC3</a>, <a href="https://mc-stan.org">Stan</a>, and <a href="http://edwardlib.org/">Edward</a>—stay tuned to not miss the last blog post in this series, which will be about Replica Exchange, a cool strategy to fight issues arising from multimodality!</p>

<h2 id="footnotes-1">Footnotes</h2>
<ol>
  <li>
    <p>Although hidden here, in general, the kinetic energy of a physical object involves the mass of the object. It turns out that HMC even permits a different “mass” (measure of inertia) for each dimension, which can even be position-dependent. This can be exploited to tune the algorithm.</p>
  </li>
  <li>
    <p>These assumptions are defined by the <a href="https://en.wikipedia.org/wiki/Canonical_ensemble">canonical ensemble</a>. In statistical mechanics, this defines a large, constant number of particles confined in a constant volume. The whole system is kept at a constant temperature.</p>
  </li>
  <li>
    <p>The temperature of a system is closely related to the average kinetic energy of its constituent particles. Drawing a new momentum for every HMC step can be seen as a kind of thermostat, keeping the average kinetic energy and thus the temperature constant. It should be noted that there are also <a href="https://arxiv.org/abs/1205.1939">HMC variants with only partial momentum refreshment</a>, meaning that the previous momentum is retained, but with some additional randomness on top.</p>
  </li>
  <li>
    <p>HMC combines two very different techniques, both very popular in computational physics: Metropolis-Hastings, which, in a sense, circumvents a real simulation of the physical system, and molecular dynamics, which explicitly solves equations of motion and thus simulates the system. Because of this, HMC is also known as (and was originally named) Hybrid Monte Carlo.</p>
  </li>
  <li>
    <p>The movement of a classical particle follows a set of differential equations known as <a href="https://en.wikipedia.org/wiki/Hamiltonian_mechanics">Hamilton’s equations of motion</a>. In the context of HMC, numerical methods to solve them have to maintain crucial properties of Hamiltonian mechanics: <em>reversibility</em> assures that a trajectory can be run “backwards” by flipping the direction of the final momentum and is important for detailed balance. <a href="https://en.wikipedia.org/wiki/Liouville%27s_theorem_(Hamiltonian)"><em>Volume conservation</em></a> essentially keeps the acceptance criterion simple. In addition, <a href="https://en.wikipedia.org/wiki/Symplectic_integrator"><em>symplecticity</em></a> is highly advantageous: it implies volume conservation, but also guarantees that the energy error stays small and thus leads to high acceptance rates. We use <a href="https://en.wikipedia.org/wiki/Leapfrog_integration">leapfrog integration</a>, which  checks all these boxes and is the standard method used in HMC.</p>
  </li>
  <li>
    <p>How about <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2010.00765.x">using differential geometry to continuously optimize the mass</a> or <a href="https://arxiv.org/abs/1409.5191">viewing HMC in terms of operators acting on a discrete state space ladder</a>?</p>
  </li>
</ol>

<h1 id="introduction-to-mcmc-part-iv--replica-exchange">Introduction to MCMC, Part IV:  Replica Exchange</h1>

<p>This is part 4 of a series of blog posts about MCMC techniques:</p>

<ul>
  <li><a href="https://www.tweag.io/blog/2019-10-25-mcmc-intro1/">The basics</a></li>
  <li><a href="https://www.tweag.io/blog/2020-01-09-mcmc-intro2/">Gibbs sampling</a></li>
  <li><a href="https://www.tweag.io/blog/2020-08-06-mcmc-intro3/">Hamiltonian Monte Carlo</a></li>
</ul>

<p>In the previous three posts, we covered both basic and more powerful Markov chain Monte Carlo (MCMC) techniques.
In case you are unfamiliar with MCMC:
it is a class of methods for sampling from probability distributions with unknown normalization constant and to make the most of this post, I would recommend to get acquainted with MCMC basics by, for example, reading the <a href="https://www.tweag.io/blog/2019-10-25-mcmc-intro1/">first post</a> of this series.<br />
The algorithms we discussed previously already get you quite far in everyday problems and are implemented in probabilistic programming packages such as <a href="https://github.com/pymc-devs/pymc3">PyMC3</a> or <a href="https://mc-stan.org/">Stan</a>.
In the (for now) final post of this series, we will leave behind the MCMC mainstream and discuss a technique to tackle highly multimodal sampling problems.</p>

<p>As we saw in the Gaussian mixture example in the <a href="https://www.tweag.io/blog/2020-01-09-mcmc-intro2/">Gibbs sampling blog post</a>, a Markov chain can have a hard time overcoming low-probability barriers in the limited time we are simulating it.
Such a stuck Markov chain thus samples a distribution incorrectly, because it oversamples some modes and undersamples others.<br />
A cool way to overcome this problem is to not simulate only from the distribution of interest, but also flatter versions (“replicas”) of it, in which low-probability regions can be crossed more easily.
If we then occasionally exchange states between these flatter Markov chains and the Markov chain sampling the distribution of interest, the latter chain will eventually get unstuck, as incoming states from flatter distributions are more likely to be located in a different mode.
This is the principal idea of Replica Exchange (RE), also known as “Parallel Tempering” (PT) or “Metropolis Coupled Markov Chain Monte Carlo”.</p>

<h2 id="replica">Replica…</h2>

<p>Just as several other MCMC algorithms, RE has its <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.57.2607">origins</a> in computational physics and just as before, we will help ourselves to some physics terminology.<br />
As discussed in <a href="https://www.tweag.io/blog/2020-08-06-mcmc-intro3/">the previous blog post about Hamiltonian Monte Carlo</a>, under <a href="https://en.wikipedia.org/wiki/Canonical_ensemble">certain conditions</a>, the probability for a physical system to be in a certain state $x$ is characterized by a potential energy $E(x)$ and given by $p(x)\propto\exp[-E(x)]$.
In fact, for simplicity, we omitted a little detail:
the system has a temperature $T$, which influences the probability of the state $x$ via a constant $\beta \propto \frac{1}{T}$.
The complete formula for the probability is then</p>

\[p_\beta(x) \propto \exp[-\beta E(x)] \ \mathrm .\]

<p>The temperature is important, as it essentially determines the “flatness” of the distribution $p_\beta(x)$.
In a high-temperature system (meaning $\beta \approx 0$), energy barriers can be easily crossed because $p_\beta(x)$ is almost uniform.
Physically speaking, because particles move very fast at high temperatures, they can overcome forces acting on them more easily.<br />
Therefore, parameterizing the distribution $p(x)$ by $\beta$ in the above way gives us exactly what we need for RE:
a family of distributions (replicas), which are increasingly flatter and thus easier to sample.
This family of distributions is given by $p_\beta(x)=p(x)^\beta$ with $1 \geq \beta \geq 0$ and $p_1(x)=p(x)$ being the distribution we’re actually interested in.
We call the distributions with $\beta &lt; 1$ “tempered”.
Let’s take up again the example of a Gaussian mixture and make some choice of inverse temperatures:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mix_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mu1</span><span class="o">=-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">sigma1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">w1</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">w2</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">mixture</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="o">**</span><span class="n">mix_params</span><span class="p">)</span>
<span class="n">temperatures</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
</code></pre></div></div>

<p>What do the tempered mixtures looks like? Let’s plot them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>

<span class="k">def</span> <span class="nf">plot_tempered_distributions</span><span class="p">(</span><span class="n">log_prob</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)):</span>
    <span class="n">xspace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">temperatures</span><span class="p">,</span> <span class="n">axes</span><span class="p">)):</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">temp</span> <span class="o">*</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="o">-</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">xspace</span><span class="p">)))</span> <span class="o">/</span> <span class="n">Z</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sa">r</span><span class="s">'$\beta={}$'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">temp</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="p">.</span><span class="n">transAxes</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s">'replica {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">)</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> 
                <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="p">.</span><span class="n">transAxes</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plot_tempered_distributions</span><span class="p">(</span><span class="n">mixture</span><span class="p">.</span><span class="n">log_prob</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_124_0.png" alt="png" /></p>

<p>I believe that you’ll agree that the upper distributions look somewhat easier to sample. Note, though, that this is not the only way to obtain a family of tempered distributions.</p>

<h2 id="-exchange">… Exchange</h2>

<p>Now that we know how to obtain flatter replicas of our target density, let’s work out the other key component of Replica Exchange — the exchanges between them.
Naively, you would probably do the following: if, after $k$ steps of sampling, the Markov chain $i$ is in state state $x^i_k$ and the Markov chain $j$ is in state $x^j_k$, then an exchange of states between the two chains leads to the next state of chain $i$ being in state $x^i_{k+1}=x^j_k$, while chain $j$ will assume $x^j_{k+1}=x^i_k$ as its next state.$^1$<br />
Of course you can’t just swap states like that, because the exchanged states will not be drawn from the right distribution.
So what do we do if we have a proposal state which is not from the correct distribution?
The same thing we always do:
we use a Metropolis criterion to conditionally accept / reject it, thus making sure that the Markov chain’s equilibrium distribution is maintained.
So the probability to accept the exchange is the probability to accept the new proposal state in chain $i$ ($p_{j\rightarrow i}$) times the probability to accept the new proposal state in chain $j$ ($p_{i\rightarrow j}$).
Here’s the full expression for the exchange acceptance probability:</p>

\[p^\mathrm{RE}_\mathrm{acc} (x^i_{k+1}=x^j_k, x^j_{k+1}=x^i_k | x^i_k, x^j_k) = \mathrm{min}\Bigg\{1, \underbrace{\frac{p_{\beta_i}(x^j_k)}{p_{\beta_i}(x^i_k)}}_{p_{j\rightarrow i}} \times \underbrace{\frac{p_{\beta_j}(x^i_k)}{p_{\beta_j}(x^j_k)}}_{p_{i\rightarrow j}}\Bigg\}\]

<p>Some thought has to be put into how to choose the replica pairs for which exchanges are attempted.
Imagine having three replicas.
In a single exchange attempt, only two replicas, say, $2$ and $3$, are involved, while the target distribution, sampled by replica $1$, performs a regular, local MCMC move instead.
After attempting an exchange between replicas $2$ and $3$, all replicas would continue to sample locally for a while until it’s time for the next exchange attempt.
If we chose again replicas $2$ and $3$ and kept on doing so, replica $1$ would not be “connected” to the other two replicas.
The following figure, in which each local MCMC sample is denoted by a dash and samples involved in an exchange are shown as solid dots shows how that would look like:
<img src="/assets/2023-11-01-mcmc-introduction-3_files/bad_swap_strategy.png" alt="bad_swap_strategy.png" />
As the lower (target) replica doesn’t participate in any exchanges, it would likely stay stuck in its current mode.
This strategy thus defeats the purpose of RE.
Only if we also attempt exchanges between replica $1$ and, for example, replica $2$, replica $1$ has access to states originally stemming from the high-temperature replica $3$.
That way, we make sure that all replicas are connected to each other.
We best swap only replicas adjacent in the temperature ladder, because acceptance rate will decrease if the distributions are too different.
That swapping strategy would look as follows:
<img src="/assets/2023-11-01-mcmc-introduction-3_files/good_swap_strategy.png" alt="good_swap_strategy.png" />
If a replica is not participating in a swap, we just draw a normal sample.
As it was the case for the tempered replicas, also the exchanges do not necessarily have to be performed in the way presented here:
there is a certain freedom in choosing the acceptance criterion as well as in the scheme to select swap partners.</p>

<h2 id="implementation-1">Implementation</h2>
<p>Let’s implement this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">handle_left_border</span><span class="p">(</span><span class="n">leftmost_old_state</span><span class="p">,</span> <span class="n">leftmost_temperature</span><span class="p">,</span> 
                       <span class="n">leftmost_stepsize</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">new_multistate</span><span class="p">):</span>
    <span class="n">accepted</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">sample_MH</span><span class="p">(</span><span class="n">leftmost_old_state</span><span class="p">,</span> 
                                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">leftmost_temperature</span> <span class="o">*</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> 
                                <span class="n">leftmost_stepsize</span><span class="p">)</span>
    <span class="n">new_multistate</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="n">new_multistate</span>
    <span class="k">return</span> <span class="n">new_multistate</span><span class="p">,</span> <span class="n">accepted</span>


<span class="k">def</span> <span class="nf">handle_right_border</span><span class="p">(</span><span class="n">rightmost_old_state</span><span class="p">,</span> <span class="n">rightmost_temperature</span><span class="p">,</span> 
                        <span class="n">rightmost_stepsize</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">new_multistate</span><span class="p">):</span>
    <span class="n">accepted</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">sample_MH</span><span class="p">(</span><span class="n">rightmost_old_state</span><span class="p">,</span> 
                                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">rightmost_temperature</span> <span class="o">*</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                                <span class="n">rightmost_stepsize</span><span class="p">)</span>
    <span class="n">new_multistate</span> <span class="o">=</span> <span class="n">new_multistate</span> <span class="o">+</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">new_multistate</span><span class="p">,</span> <span class="n">accepted</span>
    
    
<span class="k">def</span> <span class="nf">build_RE_chain</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">,</span> <span class="n">n_total</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="n">swap_interval</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">):</span>

    <span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">cycle</span>

    <span class="n">n_replicas</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">)</span>

    <span class="c1"># a bunch of arrays in which we will store how many
</span>    <span class="c1"># Metropolis-Hastings / swap moves were accepted
</span>    <span class="c1"># and how many there were performed in total
</span>
    <span class="n">accepted_MH_moves</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_replicas</span><span class="p">)</span>
    <span class="n">total_MH_moves</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_replicas</span><span class="p">)</span>
    <span class="n">accepted_swap_moves</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_replicas</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">total_swap_moves</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_replicas</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">cycler</span> <span class="o">=</span> <span class="n">cycle</span><span class="p">((</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">))</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_total</span><span class="p">):</span>
        <span class="n">new_multistate</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">%</span> <span class="n">swap_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># perform RE swap
</span>            <span class="c1"># First, determine the swap partners
</span>            <span class="k">if</span> <span class="nb">next</span><span class="p">(</span><span class="n">cycler</span><span class="p">):</span>
                <span class="c1"># swap (0,1), (2,3), ...
</span>                <span class="n">partners</span> <span class="o">=</span> <span class="p">[(</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_replicas</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># swap (1,2), (3,4), ...
</span>                <span class="n">partners</span> <span class="o">=</span> <span class="p">[(</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">),</span> <span class="mi">2</span><span class="p">)]</span>
            <span class="c1"># Now, for each pair of replicas, attempt an exchange
</span>            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="n">partners</span><span class="p">:</span>
                <span class="n">bi</span><span class="p">,</span> <span class="n">bj</span> <span class="o">=</span> <span class="n">temperatures</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">temperatures</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                <span class="n">lpi</span><span class="p">,</span> <span class="n">lpj</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]),</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
                <span class="n">log_p_acc</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bi</span> <span class="o">*</span> <span class="n">lpj</span> <span class="o">-</span> <span class="n">bi</span> <span class="o">*</span> <span class="n">lpi</span> <span class="o">+</span> <span class="n">bj</span> <span class="o">*</span> <span class="n">lpi</span> <span class="o">-</span> <span class="n">bj</span> <span class="o">*</span> <span class="n">lpj</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">())</span> <span class="o">&lt;</span> <span class="n">log_p_acc</span><span class="p">:</span>
                    <span class="n">new_multistate</span> <span class="o">+=</span> <span class="p">[</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span>
                    <span class="n">accepted_swap_moves</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_multistate</span> <span class="o">+=</span> <span class="p">[</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">]]</span>
                <span class="n">total_swap_moves</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># We might have border cases: if left- / rightmost replicas don't participate
</span>            <span class="c1"># in swaps, have them draw a sample
</span>            <span class="k">if</span> <span class="n">partners</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">new_multistate</span><span class="p">,</span> <span class="n">accepted</span> <span class="o">=</span> <span class="n">handle_left_border</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">temperatures</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                              <span class="n">stepsizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">log_prob</span><span class="p">,</span>
                                                              <span class="n">new_multistate</span><span class="p">)</span>
                <span class="n">accepted_MH_moves</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">accepted</span>
                <span class="n">total_MH_moves</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">partners</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">new_multistate</span><span class="p">,</span> <span class="n">accepted</span> <span class="o">=</span> <span class="n">handle_right_border</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">temperatures</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                                               <span class="n">stepsizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_prob</span><span class="p">,</span>
                                                               <span class="n">new_multistate</span><span class="p">)</span>
                <span class="n">accepted_MH_moves</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">accepted</span>
                <span class="n">total_MH_moves</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># perform sampling in single chains
</span>            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">temp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">temperatures</span><span class="p">):</span>
                <span class="n">accepted</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">sample_MH</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">temp</span> <span class="o">*</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">stepsizes</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
                <span class="n">accepted_MH_moves</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">accepted</span>
                <span class="n">total_MH_moves</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">new_multistate</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">chain</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_multistate</span><span class="p">)</span>

    <span class="c1"># calculate acceptance rates
</span>    <span class="n">MH_acceptance_rates</span> <span class="o">=</span> <span class="n">accepted_MH_moves</span> <span class="o">/</span> <span class="n">total_MH_moves</span>
    <span class="c1"># safe division in case of zero total swap moves
</span>    <span class="n">swap_acceptance_rates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">divide</span><span class="p">(</span><span class="n">accepted_swap_moves</span><span class="p">,</span> <span class="n">total_swap_moves</span><span class="p">,</span>
                                      <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_replicas</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">where</span><span class="o">=</span><span class="n">total_swap_moves</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">MH_acceptance_rates</span><span class="p">,</span> <span class="n">swap_acceptance_rates</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</code></pre></div></div>

<p>Before we can run this beast, we have to set stepsizes for all the single Metropolis-Hastings samplers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stepsizes</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.75</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.75</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">]</span>
</code></pre></div></div>

<p>Note that the step size decreases:
the more pronounced the modes are, the lower a step size you need to maintain a decent Metropolis-Hastings acceptance rate.<br />
Let’s first run the three Metropolis-Hastings samplers independently by setting the <code class="language-plaintext highlighter-rouge">swap_interval</code> argument of the above function to something bigger than <code class="language-plaintext highlighter-rouge">n_total</code>, meaning that no swap will be attempted:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_MH_acceptance_rates</span><span class="p">(</span><span class="n">mh_acceptance_rates</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"MH acceptance rates: "</span> <span class="o">+</span> <span class="s">""</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="s">"{}: {:.3f} "</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
                                             <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mh_acceptance_rates</span><span class="p">)]))</span>


<span class="n">mh_acc_rates</span><span class="p">,</span> <span class="n">swap_acc_rates</span><span class="p">,</span> <span class="n">chains</span> <span class="o">=</span> <span class="n">build_RE_chain</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                                                        <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">)),</span>
                                                      <span class="n">stepsizes</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="mi">500000000</span><span class="p">,</span>
                                                      <span class="n">mixture</span><span class="p">.</span><span class="n">log_prob</span><span class="p">)</span>
<span class="n">print_MH_acceptance_rates</span><span class="p">(</span><span class="n">mh_acc_rates</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MH acceptance rates: 0: 0.796 1: 0.542 2: 0.477 3: 0.700 4: 0.695 
</code></pre></div></div>

<p>Let’s visualize the samples:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_RE_samples</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">axes</span><span class="p">)):</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"MCMC samples"</span><span class="p">)</span>
        
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                         <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plot_RE_samples</span><span class="p">(</span><span class="n">chains</span><span class="p">[</span><span class="mi">100</span><span class="p">:].</span><span class="n">T</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
<span class="n">plot_tempered_distributions</span><span class="p">(</span><span class="n">mixture</span><span class="p">.</span><span class="n">log_prob</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_132_0.png" alt="png" /></p>

<p>We find, as expected, that the replicas with lower inverse temperature are sampled much more exhaustively, while the Metropolis-Hastings sampler struggles for $\beta=1$ and perhaps already $\beta = 0.8$.<br />
Now we couple the chains by replacing every fifth Metropolis-Hastings step by an exchange step:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">4</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">))</span>
<span class="n">mh_acc_rates</span><span class="p">,</span> <span class="n">swap_acc_rates</span><span class="p">,</span> <span class="n">chains</span> <span class="o">=</span> <span class="n">build_RE_chain</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">stepsizes</span><span class="p">,</span>
                                                      <span class="mi">10000</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span>
                                                      <span class="n">mixture</span><span class="p">.</span><span class="n">log_prob</span><span class="p">)</span>
<span class="n">print_MH_acceptance_rates</span><span class="p">(</span><span class="n">mh_acc_rates</span><span class="p">)</span>
<span class="n">swap_rate_string</span> <span class="o">=</span> <span class="s">""</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="s">"{}&lt;-&gt;{}: {:.3f}, "</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">swap_acc_rates</span><span class="p">)])[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Swap acceptance rates:"</span><span class="p">,</span> <span class="n">swap_rate_string</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MH acceptance rates: 0: 0.788 1: 0.553 2: 0.522 3: 0.503 4: 0.474 
Swap acceptance rates: 0&lt;-&gt;1: 0.563, 1&lt;-&gt;2: 0.828, 2&lt;-&gt;3: 0.838, 3&lt;-&gt;4: 0.887
</code></pre></div></div>

<p>It looks like the temperatures we chose are (in this case, even more than) close enough to allow for good swap acceptance rates—a necessary condition for a RE simulation to be useful.<br />
Let’s see whether the many accepted exchanges actually helped the Markov chain at $\beta=1$ to sample the target distribution exhaustively:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plot_RE_samples</span><span class="p">(</span><span class="n">chains</span><span class="p">[</span><span class="mi">100</span><span class="p">:].</span><span class="n">T</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
<span class="n">plot_tempered_distributions</span><span class="p">(</span><span class="n">mixture</span><span class="p">.</span><span class="n">log_prob</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_136_0.png" alt="png" /></p>

<p>Et voilà! Thanks to coupling the replicas, we manage to sample correctly even at $\beta=1$!</p>

<p>A nice way to think about what is happening in the course of a RE simulation is to look at what happens to the initial state of a replica.
The initial state will be evolved through some “local” sampling algorithms for a few steps until an exchange is attempted.
If the exchange is successful, the state moves up or down a step on the “temperature ladder” and is evolved on that step until at least the next attempted exchange.<br />
We can visualize that by first detecting, for each pair of replicas, at which simulation time points a successful swap occured and then reconstructing the movement of a state from the list of swaps.
This yields, for each initial state, a trajectory across the temperature ladder.
For clarity, we only plot the trajectories of the initial states of the target and the flattest replica:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Detect swaps. This method works only under the assumption that
# when performing local MCMC moves, starting from two different 
# initial states, you cannot end up with the same state
</span><span class="n">swaps</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># for each pair of chains...
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chains</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># shift one chain by one state to the left.
</span>    <span class="c1"># Where states from both chains match up, a successful exchange
</span>    <span class="c1"># was performed
</span>    <span class="n">matches</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">chains</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">chains</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">matches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">swaps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">matches</span>


<span class="c1"># Reconstruct trajectories of single states through the temperature
# ladder
</span><span class="k">def</span> <span class="nf">reconstruct_trajectory</span><span class="p">(</span><span class="n">start_index</span><span class="p">,</span> <span class="n">chains</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current_ens</span> <span class="o">=</span> <span class="n">start_index</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chains</span><span class="p">)):</span>
        <span class="n">res</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_ens</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">swaps</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">current_ens</span> <span class="ow">in</span> <span class="n">swaps</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">current_ens</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">current_ens</span> <span class="ow">in</span> <span class="n">swaps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">current_ens</span> <span class="o">-=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_state_trajectories</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">trajectory</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="o">-</span><span class="n">trajectory</span><span class="p">[:</span><span class="n">max_samples</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"# of MCMC samples"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"inverse temperature $\beta$"</span><span class="p">)</span>
    <span class="c1"># make order of temperatures appear as above - whatever it takes...
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">temperatures</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    

<span class="c1"># which states to follow
</span><span class="n">start_state_indices</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">trajectories</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">reconstruct_trajectory</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">chains</span><span class="p">)</span> 
                         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">start_state_indices</span><span class="p">])</span>
<span class="n">plot_state_trajectories</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-3_files/2023-11-01-mcmc-introduction-3_138_0.png" alt="png" /></p>

<p>By following the initial state of the $\beta=0.1$ chain and checking when it arrives at $\beta=1$, you can estimate how long it takes for a state to traverse the temperature ladder and potentially help the simulation at $\beta=1$ escape from its local minimum.</p>

<h2 id="drawbacks-1">Drawbacks</h2>

<p>If what you read so far leaves you amazed about the power of RE, you have all the reason to!
But unfortunately, nothing is free in life…<br />
In the above example, the price we pay for the improved sampling of the distribution at $\beta=1$ is given by the computing time expended to simulate the replicas at $\beta &lt; 1$.
The samples we drew in those replicas are not immediately useful to us.</p>

<p>Another issue is finding an appropriate sequence of temperatures.
If your distribution has many modes and is higher-dimensional, you will need much more than one interpolating replica.
Temperatures of neighboring replicas need to be similar enough as to ensure reasonable exchange rates, but not too similar in order not to have too many replicas.<br />
Furthermore, the more replicas you have, the longer it takes for a state to diffuse from a high-temperature replica to a low-temperature one—states essentially perform a random walk in temperature space.</p>

<p>Finally, RE is not a mainstream technique yet—its use has mostly been limited to computational physics and biomolecular simulation, where computing clusters are readily available to power this algorithm.
This means that there are, to the best of my knowledge, no RE implementations in major probabilistic programming packages.
The exception is <a href="https://www.tensorflow.org/probability">TensorFlow Probability</a>. Seeing that <a href="https://github.com/pymc-devs/pymc4">PyMC4</a>, the successor to the very popular PyMC3 probabilistic programming package, will have TensorFlow Probability as a backend, we can hope for a GPU-ready RE implementation in PyMC4.</p>

<h2 id="conclusion-1">Conclusion</h2>

<p>I hope I was able to convince you that Replica Exchange is a powerful and relatively easy-to-implement technique to improve sampling of multimodal distributions.
Now go ahead, <a href="https://github.com/tweag/blog-resources/blob/master/mcmc-intro/mcmc_introduction.ipynb">download the notebook</a> and play around with the code!
Things you might be curious to try out are varying the number of replicas, the temperature schedule or the number of swap attempts. How are those changes reflected in the state trajectories across the temperature ladder?<br />
If you want to take a deeper dive into RE, topics you might be interested in are <a href="https://doi.org/10.1002/jcc.540130812">histogram</a> <a href="https://doi.org/10.1103/PhysRevLett.109.100601">reweighting</a> <a href="http://proceedings.mlr.press/v22/habeck12.html">methods</a>, which allow you to recycle the formerly useless samples from the $\beta&lt;1$ replicas to calculate useful quantities such as the probability distribution’s normalization constant, or research on optimizing the temperature ladder (see <a href="https://doi.org/10.1101/228262">this paper</a> for a recent example).</p>

<p>This concludes my introductory MCMC blog post series.
I hope you now have a basic understanding of both basic and advanced MCMC algorithms and specific sampling problems they address.
The techniques you learned are becoming more and more popular, enabling wide-spread use of Bayesian data analysis methods by means of user-friendly probabilistic programming packages.
You are now well-equipped with the necessary background knowledge to use these packages and ready to tackle your own complex data analysis problems with powerful sampling techniques!</p>

<h2 id="footnotes-2">Footnotes</h2>
<ol>
  <li>Strictly speaking, talking about separate Markov chains in each replica is not correct: when performing an exchange, the next state not only depends on the previous state in a given replica, but also on the previous state of the exchange partner replica, which violates the Markov property. Instead, the correct way to think about RE is that all replicas together make up one single “multi-state” Markov chain whose transitions are properly detailed-balanced.</li>
</ol>

</div>

<span class="post-tags">
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/AI"><nobr>AI</nobr></code>&nbsp;</a>    
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/MCMC"><nobr>MCMC</nobr></code>&nbsp;</a>    
    
</span>

<div class="recent">
  <h2>Recent Posts</h2>
  <ul class="recent-posts">
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo2.html">
            A Diffusion Model from Scratch in Pytorch
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo1.html">
            Diffusion Models Tutorial
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/02/Inspect-BERT-Vocabulary.html">
            Inspect BERT Vocabulary
          </a>
          <small>[02 Dec 2023]</small>
        </h4>
      </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
