<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Mcmc Introduction 2 &middot; Zhu Philip's AI Journey
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- merge something else -->
  
  <!-- merge something else 
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" /> -->
  
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>  
  
  <script defer src="/assets/js/lbox.js"></script>
   

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script> 

</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <link rel="stylesheet" href="/assets/style-3.css">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <div align="center">
          <img src="/assets/profile-pixel.png" class="profilepic pt-3 pb-2">
        </div>
        <!-- <a href="/"> -->
          Zhu Philip's AI Journey
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/working">Working</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <a class="sidebar-nav-item" href="/cv">cv</a>
      <!-- <a class="sidebar-nav-item" href="/projects">Projects</a> -->
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/cv/">CV</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">publications</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/working/">Working</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/zphilip/zphilip.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:zphilip48@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/tianda-zhu-37a5b031" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/zphilip" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/zphilip" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Mcmc Introduction 2</h1>
  <span class="post-date">26 Jan 2021</span>
  <p><strong>original of this example : https://towardsdatascience.com/from-scratch-bayesian-inference-markov-chain-monte-carlo-and-metropolis-hastings-in-python-ef21a29e25a</strong></p>

<h1 id="from-scratch-bayesian-inference-markov-chain-monte-carlo-and-metropolis-hastings-inpython">From Scratch: Bayesian Inference, Markov Chain Monte Carlo and Metropolis Hastings, in python</h1>

<p>In one of the courses during my data science degree, I came across a technique called Markov Chain Monte Carlo, or as it’s more commonly referred to, MCMC. The description for this method stated something along the lines of: MCMC is a class of techniques for sampling from a probability distribution and can be used to estimate the distribution of parameters given a set of observations.</p>

<p>Back then, I did not think much of it. I thought, “oh it’s just another sampling technique”, and I decided I’d read on it when I’d practically need it. This need never emerged, or perhaps it did and I wrongly used something else.</p>

<h2 id="so-why-the-interest-now">So why the interest now?</h2>

<p>Recently, I have seen a few discussions about MCMC and some of its implementations, specifically the Metropolis-Hastings algorithm and the PyMC3 library. $Markov : Chain: Monte: Carlo: in: Python: - :A :Complete: Real$-$World :Implementation$ was the article that caught my attention the most. In it, William Koehrsen explains how he was able to learn the approach by applying it to a real world problem: to estimate the parameters of a logistic function that represents his sleeping patterns.</p>

<p>\begin{equation} P(sleep/t,\alpha,\beta)=\dfrac{1}{1+e^{\beta t + \alpha}}  \end{equation}</p>

<p>Mr. Koehrsen uses the PyMC3 implementation of the Metropolis-Hastings algorithm to estimate $\beta$ and $\alpha$, thus computing the entire parameter space, and deriving the most likely logistic model.</p>

<h2 id="so-why-am-i-talking-about-all-that">So why am I talking about all that?</h2>

<p>I this article, I propose to implement from scratch, my own version of the Metropolis-Hastings algorithm to find parameter distributions for a dummy data example and then of a real world problem.</p>

<p>I figured that if I get my hands dirty, I might finally be able to understand it. I will only use numpy to implement the algorithm, and matplotlib to draw pretty things. Alternatively, scipy can be used to compute the density functions, but I will also show how to implement them using numpy.</p>

<h2 id="flow-of-the-article">Flow of the article:</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* At first, I will introduce Bayesian inference, MCMC-MH and their mathematical components.
* Second, I will explain the algorithm using dummy data.
* Third, I will apply it to a real world problem.
</code></pre></div></div>

<h1 id="part-1-bayesian-inference-markov-chain-monte-carlo-and-metropolis-hastings">Part 1: Bayesian inference, Markov Chain Monte Carlo, and Metropolis-Hastings</h1>

<h2 id="a-birds-eye-view-on-the-philosophy-of-probabilities">A bird’s eye view on the philosophy of probabilities</h2>

<p>In order to talk about Bayesian inference and what follows it, I shall first explain what the Bayesian view of probability is, and situate it within its historical context</p>

<h3 id="frequentist-vs-bayesian-thinking">Frequentist vs Bayesian thinking</h3>

<p>There are two major interpretations to probabilities: Bayesian and Frequentist.</p>

<p>From a <strong>Frequentist’s</strong> perspective, probabilities represent long term frequencies with which events occur. A frequentist can say that the probability of having tails from a coin toss is equal to 0.5 <em>on the long run</em>. Each new experiment, can be considered as one of an infinite sequence of possible repetitions of the same experiment. The idea is that there is <em>no</em> belief in a frequentist’s view of probability. The probability of event $x$ happening out of n trials is equal to the following frequency: $P(x)=\dfrac{n_x}{n}$, and the true probability is reached when $n-&gt;\infty$. Frequentists will never say “I am 45% (0.45) sure that there is lasagna for lunch today”, since this does not happen on the long run. Commonly, a frequentist approach is referred to as the <em>objective</em> approach since there is no expression of belief and/or prior events in it.</p>

<p>On the other hand, in <strong>Bayesian</strong> thinking, probabilities are treated as an expression of belief. Therefore it is perfectly reasonable for a Bayesian to say “I am 50% (0.5) sure that there is lasagna for lunch today”. By combining <em>prior</em> beliefs and current events (the <em>evidence</em>) one can compute the <em>posterior</em>, the belief that there is lasagna today. The idea behind Bayesian thinking is to keep updating the beliefs as more evidence is provided. Since this approach deals with belief, it is usually referred to as the <em>subjective</em> view on probability.</p>

<h3 id="bayesian-inference">Bayesian inference</h3>

<p><img src="data/Thomas-Bayes.jpg" style="width: 400px;" /> $\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad $  <em>A supposed portrait of Thomas Bayes, an English statistician, philosopher, and theologian. Image Credit: <a href="https://fs.blog/2018/09/bayes-theorem/">Farnam Street</a>.</em></p>

<p>In the philosophy of decision making, Bayesian inference is closely related to Bayesian probability, in the sense that it manipulates priors, evidence, and likelihood to compute the posterior. Given some event B, what is the probability that event A occurs?. This is answered by Bayes’ famous formula: $P(A/B)=\dfrac{P(B/A)P(A)}{P(B)}$</p>

<ul>
  <li>
    <p>$P(A/B)$ is the <strong>posterior</strong>. What we wish to compute.</p>
  </li>
  <li>
    <p>$P(B/A)$ is the <strong>likelihood</strong>. Assuming A occured, how likely is B.</p>
  </li>
  <li>
    <p>$P(A)$ is the <strong>prior</strong>. How likely the event $A$ is regardless of evidence.</p>
  </li>
  <li>
    <p>$P(B)$ is the <strong>evidence</strong>. How likely the evidence $B$ is regardless of the event.</p>
  </li>
</ul>

<p>In our case, we are mostly interested in the specific formulation of Bayes’ formula:</p>

<p>$P(\theta/D)=\dfrac{P(D/\theta)P(\theta)}{P(D)}$ where, $P(\theta/D)$ is the <strong>posterior</strong>, $P(D/\theta)$ is the <strong>likelihood</strong>, $P(\theta)$ is the <strong>prior</strong> and $P(D)$ is the <strong>evidence</strong>.</p>

<p>We would like to find the most likely distribution of $\theta$, the parameters of the model explaining the data, D.</p>

<p>Computing some of these probabilities can be tedious, especially the evidence $P(D)$. Also, other problems can arise such as those of ensuring conjugacy, which I will not dive into in this article. Luckily, some techniques, namely MCMC, allow us to sample from the posterior, and a draw distributions over our parameters without having to worry about computing the evidence, nor about conjugacy.</p>

<h3 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h3>

<p>MCMC allows us to draw samples from any distribution that we can’t sample from directly. It can be used to sample from the posterior distribution over parameters.
It has seen much success in many applications, such as computing the distribution of parameters, given a set of observations and some prior belief, and also computing high dimensional integrals in physics and in digital communications.</p>

<p>Bottom line: <strong>It can be used to compute the distribution over the parameters, given a set of observations and a prior belief.</strong></p>

<h3 id="metropolis-hastings">Metropolis-Hastings</h3>

<p>MCMC is a class of methods. Metropolis-Hastings is a specific implementation of MCMC. It works well in high dimensional spaces as opposed to Gibbs sampling and rejection sampling.</p>

<p>This technique requires a simple distribution called the <strong>proposal distribution</strong> (Which I like to call <strong>transition model</strong>… [it might be due to $T\left(x_{i+1} \mid x_i\right)=q\left(x_{i+1} \mid x_i\right) \times p_{\text {acc }}\left(x_{i+1} \mid x_i\right)$ ] ) $Q(\theta^\prime/\theta)$ to help draw samples from an intractable posterior distribution $P(\Theta=\theta/D)$.</p>

<p><strong>Metropolis-Hastings uses $Q$ to randomly walk in the distribution space, accepting or rejecting jumps to new positions based on how likely the sample is. This “memoriless” random walk is the “Markov Chain” part of MCMC.</strong></p>

<p>The “likelihood” of each new sample is decided by a function $f$ . That’s why $f$ must be proportional to the posterior we want to sample from. f is commonly chosen to be a probability density function that expresses this proportionality.</p>

<p>To get a new position of the parameter, just take our current one $\theta$, and propose a new one $\theta^\prime$, that is a random sample drawn from $Q(\theta^\prime/\theta)$. Often this is a symmetric distribution. For instance, a normal distribution with mean $\theta$ and some standard deviation $\sigma$: $Q(\theta^\prime/\theta) = \mathcal{N}(\theta, \sigma)$</p>

<p>To decide if $\theta^\prime$ is to be accepted or rejected, the following ratio must be computed for each new value of $\theta^\prime$: $\dfrac{P(\theta^\prime/D)}{P(\theta/D)}$. Using Bayes’ formula this can be easily re-formulated as: $\dfrac{P(D/\theta^\prime)P(\theta^\prime)}{P(D/\theta)P(\theta)}$ (The evidence $P(D)$ is simply crossed out during the division).  $\dfrac{P(D/\theta^\prime)P(\theta^\prime)}{P(D/\theta)P(\theta)}$ is also equivalent to $\dfrac{\prod_i^nf(d_i/\Theta=\theta^\prime)P(\theta^\prime)}{\prod_i^nf(d_i/\Theta=\theta)P(\theta)}$</p>

\[\begin{equation} P(\text{accept}) = \begin{cases}\dfrac{\prod_i^nf(d_i/\Theta=\theta^\prime)P(\theta^\prime)}{\prod_i^nf(d_i/\Theta=\theta)P(\theta)}, &amp; \prod_i^nf(d_i/\Theta=\theta)P(\theta)&gt;\prod_i^nf(d_i/\Theta=\theta^\prime)P(\theta^\prime) \\  1, &amp; \prod_i^nf(d_i/\Theta=\theta)P(\theta)\leq \prod_i^nf(d_i/\Theta=\theta^\prime)P(\theta^\prime) \end{cases} \end{equation}\]

<p>This means that if a θ’ is more likely than the current θ, then we always accept θ’. If it is less likely than the current θ, then we might accept it or reject it randomly with decreasing probability, the less likely it is.</p>

<p><em>Note: The prior components are often crossed if there is no preference or restrictions on the parameters.</em></p>

<h4 id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm:</h4>
<ul>
  <li>given:
    <ul>
      <li>$f$, the PDF of the distribution to sample from</li>
      <li>$Q$, the transition model</li>
      <li>$\theta_0$, a first guess for $\theta$</li>
      <li>$\theta = \theta_0$</li>
    </ul>
  </li>
  <li>for $n$ iterations
    <ul>
      <li>$p =  f(D/\Theta=\theta)P(\theta)$</li>
      <li>$\theta^\prime = Q(\theta_i)$</li>
      <li>$p^\prime = f(D/\Theta=\theta^\prime)P(\theta^\prime)$</li>
      <li>$ratio = \dfrac{p^\prime}{p}$</li>
      <li>generate a uniform random number $r$ in [0,1]</li>
      <li>if $r&lt;ratio$:
        <ul>
          <li>set $\theta_i = \theta^\prime$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="part-2-dummy-data-example">Part 2: Dummy data example</h1>
<h2 id="step-1-data-generation">Step 1: Data generation</h2>

<p>We generate 30,000 samples from a normal distribution with $\mu$ = 10, and $\sigma$= 3, but we can only observe 1000 of them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="n">mpl</span>   
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mod1</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>

<span class="c1">#Form a population of 30,000 individual, with average=10 and scale=3
</span><span class="n">population</span> <span class="o">=</span> <span class="n">mod1</span><span class="p">(</span><span class="mi">30000</span><span class="p">)</span>
<span class="c1">#Assume we are only able to observe 1,000 of these individuals.
</span><span class="n">observation</span> <span class="o">=</span> <span class="n">population</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">observation</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="mi">35</span> <span class="p">,)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Value"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 1: Distribution of 1000 observations sampled from a population of 30,000 with $\mu$=10, $\sigma$=3"</span><span class="p">)</span>
<span class="n">mu_obs</span><span class="o">=</span><span class="n">observation</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">mu_obs</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.976226220606643
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_12_1.png" alt="png" /></p>

<h2 id="step-2-what-do-we-want">Step 2: What do we want?</h2>

<p>We would like to find a distribution for $\sigma_{obs}$ using the 1000 observed samples. Those of you who are adept at mathematics will say that there is a formula for computing $\sigma$ ( $\sigma=\sqrt{\dfrac{1}{n}\sum_i^n(d_i-\mu)^2}$)! Why do we want to sample and whatnot?? Well, this is just a dummy data example, the real problem is in part 3, where the parameters cannot be computed directly. Plus here, we are not trying to find <em>a</em> value for $\sigma$, but rather, <strong>we are trying to compute a distribution of the possible values of $\sigma$.</strong></p>

<h2 id="step-3-define-the-pdf-and-the-transition-model">Step 3: Define the PDF and the transition model.</h2>

<p>From Figure 1, we can see that the data is normally distributed. The mean can be easily computed by taking the average of the values of the 1000 samples. By doing that, we get for example $\mu_{obs}=9.8$.</p>

<h3 id="for-the-transition-model-proposal-distribution">For the transition model/ proposal distribution:</h3>
<p>I have no specific distribution in mind, so I will choose a simple one: the Normal distribution!</p>

<p>\begin{equation} Q(\sigma_{new} / \sigma_{current}) = N(\mu=\sigma_{current},\sigma’=1) \end{equation}</p>

<p>Note that $\sigma’$ is unrelated to $\sigma_{new}$ and $\sigma_{current}$. It simply specifies the standard deviation of the parameter space. It can be any value desired. It only affects the convergence time of the algorithm.</p>

<h3 id="for-the-pdf">For the PDF:</h3>
<p>Since f should be proportional to the posterior, we choose f to be the following Probability Density Function (PDF), for each data point $d_i$ in the data D:</p>

<p>\begin{equation} f(d_i/ \mu,\sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\dfrac{(d_i-\mu)^2}{2\sigma^2}} \end{equation}</p>

<p>In our case, $\theta$ is made up of two values: $[\mu,\sigma]$, and that $\mu$ is a constant, $\mu = \mu_{obs}$.</p>

<h2 id="step-4-define-when-we-accept-or-reject-sigma_new">Step 4: Define when we accept or reject $\sigma_{new}$:</h2>
<p>We accept $\sigma_{new}$ if:</p>

<p>$\dfrac{Likelihood(D/\mu_{obs},\sigma_{new})<em>prior(\mu_{obs},\sigma_{new})}{Likelihood(D/\mu_{obs},\sigma_{current})</em>prior(\mu_{obs},\sigma_{current})}&gt;1     \quad \quad \quad \quad \quad      (1)$</p>

<p>If this ratio is smaller or equal to 1, then we compare it to a uniformly generated random number in the closed set [0,1]. If the ratio is larger than the random number, we accept $\sigma_{new}$, otherwise we reject it.</p>

<font color="red">*Note: Since we will be computing this ratio to decide which parameters should be accepted, it is imperative to make sure that the adopted function $f$  is proportional to the posterior itself, $P(\sigma/ D,\mu)$, which in that case is verified. ($f$ is the PDF of P)*</font>
<p>[don’t under above setenece yet]</p>

<h2 id="step-5-define-the-prior-and-the-likelihood">Step 5: Define the prior and the likelihood:</h2>
<h3 id="for-the-prior-ptheta-which-we-can-alternatively-note-psigma-since-mu-is-constant">For the Prior $P(\theta)$ which we can alternatively note $P(\sigma)$ since $\mu$ is constant:</h3>
<p>We don’t have any preferences for the values that $\sigma_{new}$ and $\sigma_{current}$ can take. The only thing worth noting is that they should be positive. Why? Intuitively, the standard deviation measures dispersion. Dispersion is a distance, and distances cannot be negative. Mathematically, $\sigma=\sqrt{\dfrac{1}{n}\sum_i^n(d_i-\mu)^2}$, and the square root of a number cannot be negative. We strictly enforce this in the prior.</p>

<h3 id="for-the-likelihood-">For the likelihood :</h3>
<p>The total likelihood for a set of observation $D$ is: 
\(Likelihood(D/\mu_{obs},\sigma_{a}) = \prod_i^n f(d_i/\mu_{obs},\sigma_{a})\), where $a=new : or : current$.</p>

<p><strong>In our case, we will log both the prior and the likelihood function. Why log? Simply because it helps with numerical stability, i.e. multiplying thousands of small values (probabilities, likelihoods, etc..) can cause an underflow in the system’s memory, and the log is a perfect solution because it transforms multiplications to additions and small positive numbers into non-small negative numbers.</strong></p>

<p>Therefore our acceptance condition from equation $(1)$ becomes:</p>

<p>Accept $\sigma_{new}$ if:</p>

\[Log(Likelihood(D/\mu_{obs},\sigma_{new})) + Log(prior(\mu_{obs},\sigma_{new})) - (Log(Likelihood(D/\mu_{obs},\sigma_{current}))+ Log(prior(\mu_{obs},\sigma_{current})))&gt;0\]

<p>Equivalent to:</p>

\[\sum_i^nLog(f(d_i/\mu_{obs},\sigma_{new})) + Log(prior(\mu_{obs},\sigma_{new})) - \sum_i^nLog(f(d_i/\mu_{obs},\sigma_{current}))-Log(prior(\mu_{obs},\sigma_{current}))&gt;0\]

<p>Equivalent to:</p>

\[\sum_i^nLog(f(d_i/\mu_{obs},\sigma_{new})) + Log(prior(\mu_{obs},\sigma_{new})) &gt; \sum_i^nLog(f(d_i/\mu_{obs},\sigma_{current}))+Log(prior(\mu_{obs},\sigma_{current}))\]

<p>Equivalent to:</p>

\[\sum_i^n -nLog(\sigma_{new}\sqrt{2\pi})-\dfrac{(d_i-\mu_{obs})^2}{2\sigma_{new}^2} + Log(prior(\mu_{obs},\sigma_{new})) \quad &gt; 
  \sum_i^n -nLog(\sigma_{current}\sqrt{2\pi})-\dfrac{(d_i-\mu_{obs})^2}{2\sigma_{current}^2}+Log(prior(\mu_{obs},\sigma_{current}))  \tag{2}\]

<p>This form can be reduced even more by taking the square root and the multiplication out of the log.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#The tranistion model defines how to move from sigma_current to sigma_new
</span><span class="n">transition_model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mf">0.5</span><span class="p">,(</span><span class="mi">1</span><span class="p">,))[</span><span class="mi">0</span><span class="p">]]</span>

<span class="k">def</span> <span class="nf">prior</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1">#x[0] = mu, x[1]=sigma (new or current)
</span>    <span class="c1">#returns 1 for all valid values of sigma. Log(1) =0, so it does not affect the summation.
</span>    <span class="c1">#returns 0 for all invalid values of sigma (&lt;=0). Log(0)=-infinity, and Log(negative number) is undefined.
</span>    <span class="c1">#It makes the new sigma infinitely unlikely.
</span>    <span class="k">if</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="mi">1</span>

<span class="c1">#Computes the likelihood of the data given a sigma (new or current) according to equation (2)
</span><span class="k">def</span> <span class="nf">manual_log_like_normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">data</span><span class="p">):</span>
    <span class="c1">#x[0]=mu, x[1]=sigma (new or current)
</span>    <span class="c1">#data = the observation
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="p">)</span><span class="o">-</span><span class="p">((</span><span class="n">data</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="c1">#Same as manual_log_like_normal(x,data), but using scipy implementation. It's pretty slow.
</span><span class="k">def</span> <span class="nf">log_lik_normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">data</span><span class="p">):</span>
    <span class="c1">#x[0]=mu, x[1]=sigma (new or current)
</span>    <span class="c1">#data = the observation
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]).</span><span class="n">pdf</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>


<span class="c1">#Defines whether to accept or reject the new sample
</span><span class="k">def</span> <span class="nf">acceptance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_new</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x_new</span><span class="o">&gt;</span><span class="n">x</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">accept</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Since we did a log likelihood, we need to exponentiate in order to compare to the random number
</span>        <span class="c1"># less likely x_new are less likely to be accepted
</span>        <span class="k">return</span> <span class="p">(</span><span class="n">accept</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_new</span><span class="o">-</span><span class="n">x</span><span class="p">)))</span>


<span class="k">def</span> <span class="nf">metropolis_hastings</span><span class="p">(</span><span class="n">likelihood_computer</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span> <span class="n">transition_model</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span><span class="n">iterations</span><span class="p">,</span><span class="n">data</span><span class="p">,</span><span class="n">acceptance_rule</span><span class="p">):</span>
    <span class="c1"># likelihood_computer(x,data): returns the likelihood that these parameters generated the data
</span>    <span class="c1"># transition_model(x): a function that draws a sample from a symmetric distribution and returns it
</span>    <span class="c1"># param_init: a starting sample
</span>    <span class="c1"># iterations: number of accepted to generated
</span>    <span class="c1"># data: the data that we wish to model
</span>    <span class="c1"># acceptance_rule(x,x_new): decides whether to accept or reject the new sample
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">param_init</span>
    <span class="n">accepted</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rejected</span> <span class="o">=</span> <span class="p">[]</span>   
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">x_new</span> <span class="o">=</span>  <span class="n">transition_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    
        <span class="n">x_lik</span> <span class="o">=</span> <span class="n">likelihood_computer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">data</span><span class="p">)</span>
        <span class="n">x_new_lik</span> <span class="o">=</span> <span class="n">likelihood_computer</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span><span class="n">data</span><span class="p">)</span> 
        <span class="k">if</span> <span class="p">(</span><span class="n">acceptance_rule</span><span class="p">(</span><span class="n">x_lik</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prior</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span><span class="n">x_new_lik</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prior</span><span class="p">(</span><span class="n">x_new</span><span class="p">)))):</span>            
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_new</span>
            <span class="n">accepted</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rejected</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>            
                
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">accepted</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">rejected</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="step-6-run-the-algorithm-with-initial-parameters-and-collect-accepted-and-rejected-samples">Step 6: Run the algorithm with initial parameters and collect accepted and rejected samples</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">accepted</span><span class="p">,</span> <span class="n">rejected</span> <span class="o">=</span> <span class="n">metropolis_hastings</span><span class="p">(</span><span class="n">manual_log_like_normal</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span><span class="n">transition_model</span><span class="p">,[</span><span class="n">mu_obs</span><span class="p">,</span><span class="mf">0.1</span><span class="p">],</span> <span class="mi">50000</span><span class="p">,</span><span class="n">observation</span><span class="p">,</span><span class="n">acceptance</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in log
  app.launch_new_instance()
/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in log
</code></pre></div></div>

<h3 id="the-algorithm-accepted-8803-samples-which-might-be-different-on-each-new-run-the-last-10-samples-contain-the-following--values-for-sigma">The algorithm accepted 8803 samples (which might be different on each new run). The last 10 samples contain the following  values for $\sigma$:</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">accepted</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:,</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([3.12825546, 3.23574383, 3.23190749, 3.14136148, 3.10197045,
       3.181015  , 2.94143149, 2.95667786, 3.00426027, 3.0758097 ])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(8590, 2)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">rejected</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'rx'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Rejected'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">accepted</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'b.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Accepted'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Iteration"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$\sigma$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 2: MCMC sampling for $\sigma$ with Metropolis-Hastings. First 50 samples are shown."</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>



<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">to_show</span><span class="o">=-</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">rejected</span><span class="p">[</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'rx'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Rejected'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">accepted</span><span class="p">[</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'b.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Accepted'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Iteration"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$\sigma$"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 3: MCMC sampling for $\sigma$ with Metropolis-Hastings. All samples are shown."</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>



<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">accepted</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(8590, 2)
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_20_1.png" alt="png" /></p>

<p>So, starting from an initial σ of 0.1, the algorithm converged pretty quickly to the expected value of 3. That said, it’s only sampling in a 1D space…. so it’s not very surprising.</p>

<h3 id="we-consider-the-initial-25-of-the-values-of-sigma-to-be-burn-in-so-we-drop-them">We consider the initial 25% of the values of $\sigma$ to be “burn-in”, so we drop them.</h3>
<h3 id="lets-visualize-the-trace-of--sigma-and-the-histogram-of-the-trace">Let’s visualize the trace of  $\sigma$ and the histogram of the trace.</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">show</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="mf">0.75</span><span class="o">*</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">hist_show</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="mf">0.75</span><span class="o">*</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 4: Trace for $\sigma$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$\sigma$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Iteration"</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">hist_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency (normed)"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$\sigma$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 5: Histogram of $\sigma$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">ax</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_23_0.png" alt="png" /></p>

<h3 id="the-most-likely-value-for-sigma-is-around-31-this-is-a-bit-more-than-the-original-value-of-30-the-difference-is-due-to-us-observing-only-333-of-the-original-population-1000-out-of-30000">The most likely value for $\sigma$ is around 3.1. This is a bit more than the original value of 3.0. The difference is due to us observing only 3.33% of the original population (1,000 out of 30,000)</h3>

<h2 id="predictions">Predictions:</h2>
<p>First, we average the last 75% of accepted samples of σ, and we generate 30,000 random individuals from a normal distribution with μ=9.8 and σ=3.05 (the average of the last 75% of accepted samples) which is actually better than the most likely value of 3.1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span><span class="o">=</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">0</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
<span class="n">sigma</span><span class="o">=</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">1</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">observation_gen</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">population</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">observation_gen</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="mi">70</span> <span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">"Predicted distribution of 30,000 individuals"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">population</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="mi">70</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Original values of the 30,000 individuals"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Mean"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 6: Posterior distribution of predicitons"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.976226220606645 3.0824154119925873





&lt;matplotlib.legend.Legend at 0x7f3764230a50&gt;
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_25_2.png" alt="png" /></p>

<h1 id="part-3-a-real-world-example">Part 3: A real world example</h1>

<p>A sunspot is a region on the Sun’s surface (photosphere) that is marked by a lower temperature than its environment. These reduced temperatures are caused by concentrations of magnetic field flux that inhibit convection by an effect similar to <a href="https://en.wikipedia.org/wiki/Eddy_current">eddy current brakes</a>. Sunspots usually appear in pairs of opposite magnetic polarity. Their number varies according to the approximately 11-year solar cycle.</p>

<p>The data we will be working on is the “Monthly mean total sunspot number”, for each month from January 1749 to November 2018. This is data collected, curated and made publicly available by the <a href="http://www.sidc.be/silso/home">World Data Center for the production, preservation and dissemination of the international sunspot number</a>.</p>

<h2 id="lets-plot-the-data-over-the-years-to-see-what-the-distribution-might-be-like">Let’s plot the data over the years to see what the distribution might be like.</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sunspots</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s">"data/SN_m_tot_V2.0.csv"</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>

<span class="c1">#years is the third column
</span><span class="n">years</span> <span class="o">=</span> <span class="n">sunspots</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
<span class="c1">#activity is the 4th column
</span><span class="n">activity</span> <span class="o">=</span> <span class="n">sunspots</span><span class="p">[:,</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span>
<span class="k">print</span><span class="p">(</span><span class="n">years</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span><span class="n">activity</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">activity</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Sunspot count"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Years"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Monthly mean count of sunspots"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 7: Sunspots, 1749-2018"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">years</span><span class="p">[</span><span class="mi">432</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">years</span><span class="p">[:</span><span class="mi">432</span><span class="p">],</span> <span class="n">activity</span><span class="p">[:</span><span class="mi">432</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Sunspot count"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Years"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Monthly mean count of sunspots"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 8: Sunspots, 1749-1785"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(3238,) (3238,)
1785.042





&lt;matplotlib.legend.Legend at 0x7f37655eeed0&gt;
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_29_2.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">years</span><span class="p">[</span><span class="mi">432</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1785.042
</code></pre></div></div>

<h2 id="it-seems-like-we-could-model-this-phenomenon-with-a-gamma-distribution-with-a-new-cycle-resetting-every-12-years">It seems like we could model this phenomenon with a gamma distribution, with a new cycle resetting every 12 years.</h2>

<p>A gamma distribution Γ is a two-parameter family of continuous probability distributions, the parameters are the shape a and the scale b. A random variable X that is gamma-distributed is noted X~Γ(a, b), and in our case X is the count of sunspots. The two parameters a and b are the unknowns that we would like to calculate distributions for.
<img src="data/gamma.PNG" /><em>Credit: Wikipedia Commons.</em></p>

<p>For example, in the first cycle, the sunspot counts start from their highest at about 300 at the end of 1749, and fall to their lowest 6 years after, during 1755. Then the number rises up again to it’s maximum during 1761 and 1762 before falling again during 1766 and so on…</p>

<h2 id="lets-make-sure-by-plotting-a-histogram-of-sunspot-counts">Let’s make sure by plotting a histogram of sunspot counts:</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">activity</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Sunspot count"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 9: Histogram showing the frequency of sunspot counts over 270 years (1749-2018)"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, 'Figure 9: Histogram showing the frequency of sunspot counts over 270 years (1749-2018)')
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_32_1.png" alt="png" /></p>

<h2 id="indeed-it-does-seem-like-the-frequency-of-counts-follows-a-gamma-distribution">Indeed, it does seem like the frequency of counts follows a gamma distribution</h2>

<p>The gamma distribution, has for PDF, $f/f(x;a,b) =\dfrac{b^a x^{a-1}e^{-b x}}{\Gamma{(a)}}$ where $\Gamma$ is the gamma function (not to be confused with the gamma distribution which requires 2 parameters): $\Gamma{(a)}=(a-1)!$</p>

<p>Following the same procedure as in the dummy data example, we can write down the log likelihood from this pdf (see code below). Alternatively, one could use the $scipy.stats.gamma(a,b).pdf(x)$ function to compute it.</p>

<p>Since a and b must be positive, we enforce this in the prior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transition_model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">5</span><span class="p">],(</span><span class="mi">2</span><span class="p">,))</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="k">def</span> <span class="nf">prior</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="k">if</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">&lt;=</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    
<span class="k">def</span> <span class="nf">manual_log_lik_gamma</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">data</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
    
<span class="k">def</span> <span class="nf">log_lik_gamma</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">scale</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">pdf</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>    

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">accepted</span><span class="p">,</span> <span class="n">rejected</span> <span class="o">=</span> <span class="n">metropolis_hastings</span><span class="p">(</span><span class="n">manual_log_lik_gamma</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span><span class="n">transition_model</span><span class="p">,[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="mi">50000</span><span class="p">,</span><span class="n">activity</span><span class="p">,</span><span class="n">acceptance</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">accepted</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(8609, 2)





array([[ 1.02490778, 83.8511566 ],
       [ 1.00278779, 78.08327369],
       [ 0.98022741, 80.21196491],
       [ 0.96402155, 81.07479172],
       [ 0.95529461, 81.63607986],
       [ 0.99872851, 78.39366091],
       [ 0.97600451, 82.37299469],
       [ 0.961985  , 83.88958938],
       [ 0.94603285, 86.84908186],
       [ 0.9749906 , 84.71025594]])
</code></pre></div></div>

<p>Starting from a=4, and b =10, the algorithm accepted 8561 pairs of samples, the last value for a is 1.01307402 and the last value for b is 83.40995308, which are pretty far off the initial values.</p>

<h2 id="as-with-the-dummy-data-example-lets-see-how-the-algorithm-worked-its-way-to-these-values">As with the dummy data example, let’s see how the algorithm worked its way to these values:</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">accepted</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Path"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">accepted</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'b.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Accepted'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rejected</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">rejected</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'rx'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Rejected'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 10: MCMC sampling for $a$ and $b$ with Metropolis-Hastings. First 50 samples are shown."</span><span class="p">)</span>  


<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">accepted</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Path"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">accepted</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'b.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Accepted'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rejected</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">rejected</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'rx'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Rejected'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 11: MCMC sampling for $a$ and $b$ with Metropolis-Hastings. All samples are shown."</span><span class="p">)</span> 

<span class="n">to_show</span><span class="o">=</span><span class="mi">50</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="o">-</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">accepted</span><span class="p">[</span><span class="o">-</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Path"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="o">-</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">accepted</span><span class="p">[</span><span class="o">-</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'b.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Accepted'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rejected</span><span class="p">[</span><span class="o">-</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">rejected</span><span class="p">[</span><span class="o">-</span><span class="n">to_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'rx'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Rejected'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 12: MCMC sampling for $a$ and $b$ with Metropolis-Hastings. Last 50 samples are shown."</span><span class="p">)</span> 
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, 'Figure 12: MCMC sampling for $a$ and $b$ with Metropolis-Hastings. Last 50 samples are shown.')
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_38_1.png" alt="png" /></p>

<p>As we can see from figures 10, 11, and 12, the algorithm converges quickly to the [a=1,b=85] zone.</p>

<p>Tip: when the algorithm starts to heavily reject samples, that means that we have reached a zone of saturation of the likelihood. Commonly, this can be interpreted as having reached the optimal parameter space from which we can sample, i.e. there is very little reason for the algorithm to accept new values. This is marked in figures 11, and 12 where the algorithm no longer accepts any values outside of a small range.</p>

<h3 id="we-consider-the-initial-50-of-the-values-of-a-and-b-to-be-burn-in-so-we-drop-them">We consider the initial 50% of the values of a and b to be “burn-in”, so we drop them.</h3>
<h3 id="lets-visualize-the-traces-of--and-b-and-the-histogram-of-the-traces">Let’s visualize the traces of  and b and the histogram of the traces.</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">show</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">hist_show</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="mf">0.50</span><span class="o">*</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 13: Trace for $a$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Iteration"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">hist_show</span><span class="p">:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency (normed)"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 14: Histogram of $a$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 15: Trace for $b$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Iteration"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">hist_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency (normed)"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 16: Histogram of $b$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xbins</span><span class="p">,</span> <span class="n">ybins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">1.2</span><span class="p">,</span><span class="mi">30</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">75</span><span class="p">,</span><span class="mi">90</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>
<span class="n">counts</span><span class="p">,</span> <span class="n">xedges</span><span class="p">,</span> <span class="n">yedges</span><span class="p">,</span> <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">hist2d</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">hist_show</span><span class="p">:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">accepted</span><span class="p">[</span><span class="n">hist_show</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="n">xbins</span><span class="p">,</span> <span class="n">ybins</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"2D histogram showing the joint distribution of $a$ and $b$"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, '2D histogram showing the joint distribution of $a$ and $b$')
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_41_1.png" alt="png" /></p>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_41_2.png" alt="png" /></p>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_41_3.png" alt="png" /></p>

<h2 id="prediction-time">Prediction time</h2>

<p>First, we average the last 50% of accepted samples of a and b, and we generate random individuals from a Γ distribution. $a_{average}$=0.9866200759935773 and $b_{average}$=83.70749712447888.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">show</span><span class="o">=-</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">mu</span><span class="o">=</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">0</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
<span class="n">sigma</span><span class="o">=</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">1</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">t</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">activity</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">observation_gen</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>



<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">observation_gen</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span> <span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">"Predicted values"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">activity</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Original values"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Count"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 17: Posterior distribution of predicitons"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9859124903678548 83.78299695212466





&lt;matplotlib.legend.Legend at 0x7f37643b1e50&gt;
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_43_2.png" alt="png" /></p>

<h2 id="evaluation">Evaluation</h2>
<h3 id="evaluation-of-the-proposal-distribution">Evaluation of the proposal distribution</h3>
<p>How do we specify the parameters for the distribution Q? Should we move far from the current sample θ, or stay relatively close? These questions can be answered by measuring the auto-correlation between accepted samples: we do not want the accepted samples to be too correlated between one another. We don’t want distant samples to be too correlated as we are trying to implement a markov chain, i.e. a sample should only depend on its previous sample, and the auto-correlation plot should show a quick, exponential decrease between the correlation of sample i and i-1,i-2,…i-n</p>

<p>The auto-correlation is defined by computing the following function for each lag $k$:
\(r_k=\dfrac{\sum_{i=1}^{N-k}(Y_i-Y_{avg})(Y_{i+k}-Y_{avg})}{\sum_{i=1}^{N}(Y_i-Y_{avg})^2}\)</p>

<p>The lag $k$, is basically the <em>range</em> around a sample $Y_i$ in which we would like to measure the correlation.</p>

<p>The plots below show the auto-correlation for a, b for k going from 1 to 100. A lag of k=1 means that we are measuring the correlation of a sample with itself, so we expect it to be equal to 1. The higher k goes, the lower that correlation ought to be.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_acc_0</span><span class="o">=</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">0</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
<span class="n">mean_acc_1</span><span class="o">=</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,</span><span class="mi">1</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean_acc_0</span><span class="p">,</span><span class="n">mean_acc_1</span><span class="p">)</span>

<span class="n">lag</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">autocorr</span><span class="p">(</span><span class="n">accepted</span><span class="p">,</span><span class="n">lag</span><span class="p">):</span>
    <span class="n">num_0</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">denom_0</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">num_1</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">denom_1</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">accepted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">lag</span><span class="p">):</span>
        <span class="n">num_0</span><span class="o">+=</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">mean_acc_0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">lag</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">mean_acc_0</span><span class="p">)</span>
        <span class="n">num_1</span><span class="o">+=</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">mean_acc_1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">accepted</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">lag</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">mean_acc_1</span><span class="p">)</span>
        <span class="n">denom_0</span><span class="o">+=</span><span class="p">(</span><span class="n">mean_acc_0</span><span class="o">-</span><span class="n">accepted</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">denom_1</span><span class="o">+=</span><span class="p">(</span><span class="n">mean_acc_1</span><span class="o">-</span><span class="n">accepted</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">rk_0</span><span class="o">=</span><span class="n">num_0</span><span class="o">/</span><span class="n">denom_0</span>
    <span class="n">rk_1</span><span class="o">=</span><span class="n">num_1</span><span class="o">/</span><span class="n">denom_1</span>
    <span class="k">return</span> <span class="n">rk_0</span><span class="p">,</span> <span class="n">rk_1</span>


<span class="n">accepted_reversed</span><span class="o">=</span><span class="n">accepted</span><span class="p">[</span><span class="n">show</span><span class="p">:,:]</span>
<span class="n">result</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="n">lag</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1">#print(lag)
</span><span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lag</span><span class="p">:</span>
    <span class="n">result</span><span class="p">[:,</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="n">autocorr</span><span class="p">(</span><span class="n">accepted_reversed</span><span class="p">,</span><span class="n">l</span><span class="p">)</span>
    
    
<span class="c1">###Instead of writing an autocorrelation function, one could simply use thee autocorr function provided in pymc3    
#from pymc3.stats import autocorr
</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="c1">#ax.plot(lag, [autocorr(accepted[show:,1], l) for l in lags], label='auto b')
#ax.plot(lag, [autocorr(accepted[show:,0], l) for l in lags], label='auto a')
</span><span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lag</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Auto correlation for b'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lag</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Auto correlation for a'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Figure 18: Auto-correlation for a and b, for k=1..100"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'lag'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'autocorrelation'</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9867768431864482 83.74459363370107





[Text(0.5, 0, 'lag'), Text(0, 0.5, 'autocorrelation'), (-0.1, 1.0)]
</code></pre></div></div>

<p><img src="/assets/2023-11-01-mcmc-introduction-2_files/2023-11-01-mcmc-introduction-2_45_2.png" alt="png" /></p>

<p>In our case, we are lucky to have a low enough correlation. In general, we might want to setup the parameters of the proposal distribution, Q, automatically, <strong>one common method is to keep adjusting the proposal parameters so that more than 50% proposals are rejected.</strong> Alternatively, one could use <strong>an enhanced version of MCMC called Hamiltonian Monte Carlo</strong>, which reduces the correlation between successive sampled states and reaches the stationary distribution quicker.</p>

<h2 id="conclusion">Conclusion</h2>
<p>While the abstraction behind this algorithm may seem out of grasp at first, the implementation is actually pretty simple, and gives awesome results. In fact, the great thing about probabilistic programming, notably MCMC is that you only need to write down the model and then run it. There is no need to compute evidence, or ensure some constraining mathematical properties.</p>

<h2 id="resources">Resources</h2>
<p>Peter Driscoll, “A comparison of least-squares and Bayesian fitting techniques to radial velocity data sets”</p>

<p>Carson Chow, “MCMC and fitting models to data”</p>

<p>John H. Williamson, “Data Fundamentals - Probabilities”</p>

<p>Simon Rogers, “A first course in machine learning”</p>

</div>

<span class="post-tags">
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/AI"><nobr>AI</nobr></code>&nbsp;</a>    
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/MCMC"><nobr>MCMC</nobr></code>&nbsp;</a>    
    
</span>

<div class="recent">
  <h2>Recent Posts</h2>
  <ul class="recent-posts">
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo2.html">
            A Diffusion Model from Scratch in Pytorch
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo1.html">
            Diffusion Models Tutorial
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/02/transformer-implementation.html">
            transformer implementation
          </a>
          <small>[02 Dec 2023]</small>
        </h4>
      </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
