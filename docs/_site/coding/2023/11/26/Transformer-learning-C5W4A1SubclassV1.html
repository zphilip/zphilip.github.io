<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Transformer Network &middot; Zhu Philip's AI Journey
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- merge something else -->
  
  <!-- merge something else 
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" /> -->
  
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>  
  
  <script defer src="/assets/js/lbox.js"></script>
   

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script> 

</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <link rel="stylesheet" href="/assets/style-3.css">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <div align="center">
          <img src="/assets/profile-pixel.png" class="profilepic pt-3 pb-2">
        </div>
        <!-- <a href="/"> -->
          Zhu Philip's AI Journey
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/working">Working</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <!-- <a class="sidebar-nav-item" href="/projects">Projects</a> -->
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">publications</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/working/">Working</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/zphilip/zphilip.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:zphilip48@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/tianda-zhu-37a5b031" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/zphilip" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/zphilip" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Transformer Network</h1>
  <span class="post-date">26 Nov 2023</span>
  <p>I update this document also refere to</p>
<ol>
  <li>https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn</li>
  <li>https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding</li>
</ol>

<h1 id="transformer-network">Transformer Network</h1>

<p>Welcome to Week 4’s assignment, the last assignment of Course 5 of the Deep Learning Specialization! And congratulations on making it to the last assignment of the entire Deep Learning Specialization - you’re almost done!</p>

<p>Ealier in the course, you’ve implemented sequential neural networks such as RNNs, GRUs, and LSTMs. In this notebook you’ll explore the Transformer architecture, a neural network that takes advantage of parallel processing and allows you to substantially speed up the training process.</p>

<p><strong>After this assignment you’ll be able to</strong>:</p>

<ul>
  <li>Create positional encodings to capture sequential relationships in data</li>
  <li>Calculate scaled dot-product self-attention with word embeddings</li>
  <li>Implement masked multi-head attention</li>
  <li>Build and train a Transformer model</li>
</ul>

<p>For the last time, let’s get started!</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#0">Packages</a></li>
  <li><a href="#1">1 - Positional Encoding</a>
    <ul>
      <li><a href="#1-1">1.1 - Sine and Cosine Angles</a>
        <ul>
          <li><a href="#ex-1">Exercise 1 - get_angles</a></li>
        </ul>
      </li>
      <li><a href="#1-2">1.2 - Sine and Cosine Positional Encodings</a>
        <ul>
          <li><a href="#ex-2">Exercise 2 - positional_encoding</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#2">2 - Masking</a>
    <ul>
      <li><a href="#2-1">2.1 - Padding Mask</a></li>
      <li><a href="#2-2">2.2 - Look-ahead Mask</a></li>
    </ul>
  </li>
  <li><a href="#3">3 - Self-Attention</a>
    <ul>
      <li><a href="#ex-3">Exercise 3 - scaled_dot_product_attention</a></li>
    </ul>
  </li>
  <li><a href="#4">4 - Encoder</a>
    <ul>
      <li><a href="#4-1">4.1 Encoder Layer</a>
        <ul>
          <li><a href="#ex-4">Exercise 4 - EncoderLayer</a></li>
        </ul>
      </li>
      <li><a href="#4-2">4.2 - Full Encoder</a>
        <ul>
          <li><a href="#ex-5">Exercise 5 - Encoder</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#5">5 - Decoder</a>
    <ul>
      <li><a href="#5-1">5.1 - Decoder Layer</a>
        <ul>
          <li><a href="#ex-6">Exercise 6 - DecoderLayer</a></li>
        </ul>
      </li>
      <li><a href="#5-2">5.2 - Full Decoder</a>
        <ul>
          <li><a href="#ex-7">Exercise 7 - Decoder</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#6">6 - Transformer</a>
    <ul>
      <li><a href="#ex-8">Exercise 8 - Transformer</a></li>
    </ul>
  </li>
  <li><a href="#7">7 - References</a></li>
</ul>

<p><a name="0"></a></p>
<h2 id="packages">Packages</h2>

<p>Run the following cell to load the packages you’ll need.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">LayerNormalization</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizerFast</span> <span class="c1">#, TFDistilBertModel
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForTokenClassification</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span> <span class="k">as</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="n">seaborn</span><span class="p">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="s">"talk"</span><span class="p">)</span>
</code></pre></div></div>

<p><a name="1"></a></p>
<h2 id="1---positional-encoding">1 - Positional Encoding</h2>

<p>In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model.  However, when you train a Transformer network, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful - you can specifically encode the positions of your inputs and pass them into the network using these sine and cosine formulas:</p>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/fafd0dfa-961d-416d-8159-678f0d903421.png" alt="image.png" />
\(PE_{(pos, 2i)}= sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\tag{1}\)</p>

\[PE_{(pos, 2i+1)}= cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\tag{2}\]

<font size="5" color="red">**it is like FFT, I should check it later!!**</font>
<ul>
  <li>$d$ is the dimension of the word embedding and positional encoding</li>
  <li>$pos$ is the position of the word.</li>
  <li>$i$ refers to each of the different dimensions of the positional encoding.</li>
</ul>

<p>The values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted. The sum of the positional encoding and word embeding is ultimately what is fed into the model. Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data. Note that while in the lectures Andrew uses vertical vectors but in this assignment, all vectors are horizontal. All matrix multiplications should be adjusted accordingly.</p>

<p><a name="1-1"></a></p>
<h3 id="11---sine-and-cosine-angles">1.1 - Sine and Cosine Angles</h3>

<p>Get the possible angles used to compute the positional encodings by calculating the inner term of the sine and cosine equations:</p>

\[\frac{pos}{10000^{\frac{2i}{d}}} \tag{3}\]

<p><a name="ex-1"></a></p>
<h3 id="exercise-1---get_angles">Exercise 1 - get_angles</h3>

<p>Implement the function <code class="language-plaintext highlighter-rouge">get_angles()</code> to calculate the possible angles for the sine and cosine  positional encodings</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION get_angles
</span><span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="s">"""
    Get the angles for the positional encoding
    
    Arguments:
        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]
        i --   Row vector containing the dimension span [[0, 1, 2, ..., M-1]]
        d(integer) -- Encoding size
    
    Returns:
        angles -- (pos, d) numpy array 
    """</span>
    <span class="c1"># START CODE HERE
</span>    <span class="n">angles</span> <span class="o">=</span> <span class="n">pos</span><span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="n">d</span><span class="p">)))</span>
    <span class="c1"># END CODE HERE
</span>    
    <span class="k">return</span> <span class="n">angles</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="k">def</span> <span class="nf">get_angles_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    <span class="n">position</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">pos_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">pos_m</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s">"You must return a numpy ndarray"</span>
    <span class="k">assert</span> <span class="n">result</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected: (</span><span class="si">{</span><span class="n">position</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s">)"</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">position</span> <span class="o">*</span> <span class="p">(</span><span class="n">position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">even_cols</span> <span class="o">=</span>  <span class="n">result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">odd_cols</span> <span class="o">=</span> <span class="n">result</span><span class="p">[:,</span>  <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nb">all</span><span class="p">(</span><span class="n">even_cols</span> <span class="o">==</span> <span class="n">odd_cols</span><span class="p">),</span> <span class="s">"Submatrices of odd and even columns must be equal"</span>
    <span class="c1"># edge value of the angle (d_model =16)
</span>    <span class="n">limit</span> <span class="o">=</span> <span class="p">(</span><span class="n">position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span><span class="mf">14.0</span><span class="o">/</span><span class="mf">16.0</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">limit</span> <span class="p">),</span> <span class="sa">f</span><span class="s">"Last value must be </span><span class="si">{</span><span class="n">limit</span><span class="si">}</span><span class="s">"</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>

<span class="n">get_angles_test</span><span class="p">(</span><span class="n">get_angles</span><span class="p">)</span>

<span class="c1"># Example
</span><span class="n">position</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1"># add new dimension to column so it will be (4, 1)
</span><span class="n">pos_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="c1"># add new dimension to row so it will be (1, 4)
</span><span class="n">dims</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">angles</span><span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">pos_m</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[92mAll tests passed
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pos_m</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0],
       [1],
       [2],
       [3]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dims</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0, 1, 2, 3, 4, 5, 6, 7]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">angles</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00],
       [1.e+00, 1.e+00, 1.e-01, 1.e-01, 1.e-02, 1.e-02, 1.e-03, 1.e-03],
       [2.e+00, 2.e+00, 2.e-01, 2.e-01, 2.e-02, 2.e-02, 2.e-03, 2.e-03],
       [3.e+00, 3.e+00, 3.e-01, 3.e-01, 3.e-02, 3.e-02, 3.e-03, 3.e-03]])
</code></pre></div></div>

<p><a name="1-2"></a></p>
<h3 id="12---sine-and-cosine-positional-encodings">1.2 - Sine and Cosine Positional Encodings</h3>

<p>Now you can use the angles you computed to calculate the sine and cosine positional encodings.</p>

\[PE_{(pos, 2i)}= sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\]

\[PE_{(pos, 2i+1)}= cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\]

<p><a name="ex-2"></a></p>
<h3 id="exercise-2---positional_encoding">Exercise 2 - positional_encoding</h3>

<p>Implement the function <code class="language-plaintext highlighter-rouge">positional_encoding()</code> to calculate the sine and cosine  positional encodings</p>

<p><strong>Reminder:</strong> Use the sine equation when $i$ is an even number and the cosine equation when $i$ is an odd number.</p>

<h4 id="additional-hints">Additional Hints</h4>
<ul>
  <li>You may find 
<a href="https://numpy.org/doc/stable/reference/arrays.indexing.html">np.newaxis</a> useful depending on the implementation you choose.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION positional_encoding
</span><span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="s">"""
    Precomputes a matrix with all the positional encodings 
    
    Arguments:
        positions (int) -- Maximum number of positions to be encoded 
        d (int) -- Encoding size 
    
    Returns:
        pos_encoding -- (1, position, d_model) A matrix with the positional encodings
    """</span>
    <span class="c1"># START CODE HERE
</span>    <span class="c1"># initialize a matrix angle_rads of all the angles 
</span>    <span class="n">angle_rads</span> <span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">positions</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">],</span>
                            <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d</span><span class="p">)[</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,:],</span>
                            <span class="n">d</span><span class="p">)</span>
  
    <span class="c1"># -&gt; angle_rads has dim (positions,d)
</span>    <span class="c1"># apply sin to even indices in the array; 2i
</span>    <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
  
    <span class="c1"># apply cos to odd indices in the array; 2i+1
</span>    <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
    <span class="c1"># END CODE HERE
</span>    
    <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">angle_rads</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">...]</span>
    
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="k">def</span> <span class="nf">positional_encoding_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    <span class="n">position</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">16</span>

    <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">sin_part</span> <span class="o">=</span> <span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">cos_part</span> <span class="o">=</span> <span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>

    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">),</span> <span class="s">"Output is not a tensor"</span>
    <span class="k">assert</span> <span class="n">pos_encoding</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected: (1, </span><span class="si">{</span><span class="n">position</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s">)"</span>

    <span class="n">ones</span> <span class="o">=</span> <span class="n">sin_part</span> <span class="o">**</span> <span class="mi">2</span>  <span class="o">+</span>  <span class="n">cos_part</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))),</span> <span class="s">"Sum of square pairs must be 1 = sin(a)**2 + cos(a)**2"</span>
    
    <span class="n">angs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">sin_part</span> <span class="o">/</span> <span class="n">cos_part</span><span class="p">)</span>
    <span class="n">angs</span><span class="p">[</span><span class="n">angs</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>
    <span class="n">angs</span><span class="p">[</span><span class="n">sin_part</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>
    <span class="n">angs</span> <span class="o">=</span> <span class="n">angs</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
    
    <span class="n">pos_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">trueAngs</span> <span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">pos_m</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">angs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">trueAngs</span><span class="p">),</span> <span class="s">"Did you apply sin and cos to even and odd parts respectively?"</span>
 
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>
    
    
<span class="n">positional_encoding_test</span><span class="p">(</span><span class="n">positional_encoding</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[92mAll tests passed


2023-05-09 10:37:45.354620: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-09 10:37:45.355316: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
</code></pre></div></div>

<p>another example implementation in torch https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">math</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"Implement the PE function."</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Compute the positional encodings once in log space.
</span>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span>
                             <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'pe'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> 
                         <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>Nice work calculating the positional encodings! Now you can visualize them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>

<span class="k">print</span> <span class="p">(</span><span class="n">pos_encoding</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdBu'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'d'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Position'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1, 50, 512)
</code></pre></div></div>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/2023-12-03-Transformer-learning-C5W4A1SubclassV1_17_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#forward acctually just adding the pos_endcoding to x, and adding one dropout which is ingored here
#this is just for corss check the pos_endcoding 
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">pos_encoding</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">position</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">])),</span><span class="n">pos_encoding</span><span class="p">)</span>
<span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">]))</span>
<span class="c1">#you will see the different value added
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">4</span><span class="p">:</span><span class="mi">8</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">"dim %d"</span><span class="o">%</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]])</span>
<span class="bp">None</span>
</code></pre></div></div>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/2023-12-03-Transformer-learning-C5W4A1SubclassV1_18_0.png" alt="png" /></p>

<p>Each row represents a positional encoding - <font size="5" color="red">notice how none of the rows are identical! </font> You have created a unique positional encoding for each of the words.</p>

<p><a name="2"></a></p>
<h2 id="2---masking">2 - Masking</h2>

<p>There are two types of masks that are useful when building your Transformer network: the <em>padding mask</em> and the <em>look-ahead mask</em>. Both help the softmax computation give the appropriate weights to the words in your input sentence.</p>

<p><a name="2-1"></a></p>
<h3 id="21---padding-mask">2.1 - Padding Mask</h3>

<p>Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let’s say the maximum length of your model is five, it is fed the following sequences:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[["Do", "you", "know", "when", "Jane", "is", "going", "to", "visit", "Africa"], 
 ["Jane", "visits", "Africa", "in", "September" ],
 ["Exciting", "!"]
]
</code></pre></div></div>

<p>which might get vectorized as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],
 [ 56, 1285, 15, 181, 545],
 [ 87, 600]
]
</code></pre></div></div>

<p>When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 71, 121, 4, 56, 99],
 [ 2344, 345, 1284, 15, 0],
 [ 56, 1285, 15, 181, 545],
 [ 87, 600, 0, 0, 0],
]
</code></pre></div></div>

<font color="red">Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length.</font>
<p>Similarly, for sequences shorter than the maximum length, they zeros will also be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! By multiplying a padding mask by -1e9 and adding it to your sequence, you mask out the zeros by setting them to close to negative infinity. We’ll implement this for you so you can get to the fun of building the Transformer network! 😇 Just make sure you go through the code so you can correctly implement padding when building your model.</p>

<p>After masking, your input should go from <code class="language-plaintext highlighter-rouge">[87, 600, 0, 0, 0]</code> to <code class="language-plaintext highlighter-rouge">[87, 600, -1e9, -1e9, -1e9]</code>, so that when you take the softmax, the zeros don’t affect the score.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
    <span class="s">"""
    Creates a matrix mask for the padding cells
    
    Arguments:
        seq -- (n, m) matrix
    
    Returns:
        mask -- (n, 1, 1, m) binary tensor
    """</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  
    <span class="c1"># add extra dimensions to add the padding
</span>    <span class="c1"># to the attention logits.
</span>    <span class="k">return</span> <span class="n">seq</span><span class="p">[:,</span> <span class="n">tf</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span> 
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">create_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.Tensor(
[[[[0. 0. 1. 1. 0.]]]


 [[[0. 0. 0. 1. 1.]]]


 [[[1. 1. 1. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)
</code></pre></div></div>

<p>If we multiply this mask by -1e9 and add it to the sample input sequences, the zeros are essentially set to negative infinity. Notice the difference when taking the softmax of the original sequence and the masked sequence:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1.0e9</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.Tensor(
[[7.2876638e-01 2.6809818e-01 6.6454895e-04 6.6454895e-04 1.8064313e-03]
 [8.4437370e-02 2.2952457e-01 6.2391245e-01 3.1062772e-02 3.1062772e-02]
 [4.8541026e-03 4.8541026e-03 4.8541026e-03 2.6502505e-01 7.2041273e-01]], shape=(3, 5), dtype=float32)
tf.Tensor(
[[[[7.2973627e-01 2.6845497e-01 0.0000000e+00 0.0000000e+00
    1.8088354e-03]
   [2.4472848e-01 6.6524094e-01 0.0000000e+00 0.0000000e+00
    9.0030573e-02]
   [6.6483542e-03 6.6483542e-03 0.0000000e+00 0.0000000e+00
    9.8670328e-01]]]


 [[[7.3057157e-01 2.6876226e-01 6.6619506e-04 0.0000000e+00
    0.0000000e+00]
   [9.0030566e-02 2.4472845e-01 6.6524088e-01 0.0000000e+00
    0.0000000e+00]
   [3.3333334e-01 3.3333334e-01 3.3333334e-01 0.0000000e+00
    0.0000000e+00]]]


 [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01
    7.3105860e-01]
   [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.0000000e-01
    5.0000000e-01]
   [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01
    7.3105860e-01]]]], shape=(3, 1, 3, 5), dtype=float32)
</code></pre></div></div>

<p><a name="2-2"></a></p>
<h3 id="22---look-ahead-mask--it-looks-like-the-mask-m-in-importedimplementing-transformersipynb---no-it-is-not-same-the-m-is-used-in-softmax">2.2 - Look-ahead Mask – it looks like the mask M in “imported/implementing-transformers.ipynb”  – no, it is not same, the M is used in softmax</h3>

<p>The look-ahead mask follows similar intuition. In training, you will have access to the complete correct output of your training example. The look-ahead mask helps your model pretend that it correctly predicted a part of the output and see if, <em>without looking ahead</em>, it can correctly predict the next output.</p>

<p>For example, if the expected correct output is <code class="language-plaintext highlighter-rouge">[1, 2, 3]</code> and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence <code class="language-plaintext highlighter-rouge">[1, -1e9, -1e9]</code> and see if it could generate <code class="language-plaintext highlighter-rouge">[1, 2, -1e9]</code>.</p>

<p>Just because you’ve worked so hard, we’ll also implement this mask for you 😇😇. Again, take a close look at the code so you can effictively implement it later.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="s">"""
    Returns an upper triangular matrix filled with ones
    
    Arguments:
        size -- matrix size
    
    Returns:
        mask -- (size, size) tensor
    """</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mask</span> 
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">temp</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.Tensor([[0.8312781  0.24044645 0.17191601]], shape=(1, 3), dtype=float32)





&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [1., 1., 0.],
       [1., 1., 1.]], dtype=float32)&gt;
</code></pre></div></div>

<p><a name="3"></a></p>
<h2 id="3---self-attention">3 - Self-Attention</h2>

<p>As the authors of the Transformers paper state, “Attention is All You Need”.</p>

<center><img src="/assets//assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/self-attention.png" alt="Encoder" width="600" /></center>
<caption><center><font color="purple">Figure 1: Self-Attention calculation visualization</font></center></caption>

<p>The use of self-attention paired with traditional convolutional networks allows for the parallization which speeds up training. You will implement <strong>scaled dot product attention</strong> which takes in a query, key, value, and a mask as inputs to returns rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:
\(\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{4}\)</p>

<ul>
  <li>$Q$ is the matrix of queries</li>
  <li>$K$ is the matrix of keys</li>
  <li>$V$ is the matrix of values</li>
  <li>$M$ is the optional mask you choose to apply</li>
  <li>${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn’t explode</li>
</ul>

<p><a name="ex-3"></a></p>
<h3 id="exercise-3---scaled_dot_product_attention">Exercise 3 - scaled_dot_product_attention</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Implement the function `scaled_dot_product_attention()` to create attention-based representations **Reminder**: The boolean mask parameter can be passed in as `none` or as either padding or look-ahead. Multiply it by -1e9 before applying the softmax. 
</code></pre></div></div>

<p><strong>Additional Hints</strong></p>
<ul>
  <li>You may find <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul">tf.matmul</a> useful for matrix multiplication.</li>
</ul>

<p>Q，K和V是经过卷积后得到的特征，其形状为（batch_size，seq_length，num_features）。</p>

<p>将查询（Q）和键（K）相乘会得到（batch_size，seq_length，seq_length）特征，这大致告诉我们序列中每个元素的重要性，确定我们“注意”哪些元素。 注意数组使用softmax标准化，因此所有权重之和为1。 最后，注意力将通过矩阵乘法应用于值（V）数组。
请注意，MatMul操作在PyTorch中对应为torch.bmm。 这是因为Q，K和V（查询，键和值数组）都是矩阵，每个矩阵的形状均为（batch_size，sequence_length，num_features），矩阵乘法仅在最后两个维度上执行。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION scaled_dot_product_attention
</span><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="s">"""
    Calculate the attention weights.
      q, k, v must have matching leading dimensions.
      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.
      The mask has different shapes depending on its type(padding or look ahead) 
      but it must be broadcastable for addition.

    Arguments:
        q -- query shape == (..., seq_len_q, depth)
        k -- key shape == (..., seq_len_k, depth)
        v -- value shape == (..., seq_len_v, depth_v)
        mask: Float tensor with shape broadcastable 
              to (..., seq_len_q, seq_len_k). Defaults to None.

    Returns:
        output -- attention_weights
    """</span>
    <span class="c1"># START CODE HERE
</span>    
    <span class="c1"># Q*K'
</span>    <span class="n">matmul_qk</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># scale matmul_qk
</span>    <span class="n">dk</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">scaled_attention_logits</span> <span class="o">=</span> <span class="n">matmul_qk</span> <span class="o">/</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

    <span class="c1"># add the mask to the scaled tensor.
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scaled_attention_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

    <span class="c1"># softmax is normalized on the last axis (seq_len_k) so that the scores
</span>    <span class="c1"># add up to 1.
</span>    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_attention_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)
</span>    <span class="c1"># attention_weights * V
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>   <span class="c1"># (..., seq_len_q, depth_v)
</span>    
    <span class="c1"># END CODE HERE
</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="k">def</span> <span class="nf">scaled_dot_product_attention_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1">#accoridng to vector is horizontal, so here means q is 4 feature vector but 
</span>                                                                                 <span class="c1">#3 is the seq_len， so here 3x4 matrix
</span>    <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1">#k shape (4,4), still 4 is the seq_len but lenth is also 4
</span>                                                                                               <span class="c1">#q , v should have the same feature number , but it could be different 
</span>                                                                                               <span class="c1">#lenght?? -- it seems not reasonable ?
</span>    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>    <span class="c1">#q lenth is  should be 4, feature bumber is 2
</span>
    <span class="n">attention</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="s">"Weights must be a tensor"</span>
    <span class="c1">#so here the weight shape is 3x4, shape should be (q.shape[0], k.shape[0]) instead of  (q.shape[0], k.shape[1]), k is transpose
</span>    <span class="c1">#assert tuple(tf.shape(weights).numpy()) == (q.shape[0], k.shape[1]), f"Wrong shape. We expected ({q.shape[0]}, {k.shape[1]})"
</span>    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected (</span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">)"</span>
    
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="p">[[</span><span class="mf">0.2589478</span><span class="p">,</span>  <span class="mf">0.42693272</span><span class="p">,</span> <span class="mf">0.15705977</span><span class="p">,</span> <span class="mf">0.15705977</span><span class="p">],</span>
                                   <span class="p">[</span><span class="mf">0.2772748</span><span class="p">,</span>  <span class="mf">0.2772748</span><span class="p">,</span>  <span class="mf">0.2772748</span><span class="p">,</span>  <span class="mf">0.16817567</span><span class="p">],</span>
                                   <span class="p">[</span><span class="mf">0.33620113</span><span class="p">,</span> <span class="mf">0.33620113</span><span class="p">,</span> <span class="mf">0.12368149</span><span class="p">,</span> <span class="mf">0.2039163</span> <span class="p">]])</span>

    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">attention</span><span class="p">),</span> <span class="s">"Output must be a tensor"</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected (</span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">v</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">)"</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="p">[[</span><span class="mf">0.74105227</span><span class="p">,</span> <span class="mf">0.15705977</span><span class="p">],</span>
                                   <span class="p">[</span><span class="mf">0.7227253</span><span class="p">,</span>  <span class="mf">0.16817567</span><span class="p">],</span>
                                   <span class="p">[</span><span class="mf">0.6637989</span><span class="p">,</span>  <span class="mf">0.2039163</span> <span class="p">]])</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="n">attention</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="p">[[</span><span class="mf">0.30719590187072754</span><span class="p">,</span> <span class="mf">0.5064803957939148</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.18632373213768005</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mf">0.3836517333984375</span><span class="p">,</span> <span class="mf">0.3836517333984375</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2326965481042862</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mf">0.3836517333984375</span><span class="p">,</span> <span class="mf">0.3836517333984375</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2326965481042862</span><span class="p">]]),</span> <span class="s">"Wrong masked weights"</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="p">[[</span><span class="mf">0.6928040981292725</span><span class="p">,</span> <span class="mf">0.18632373213768005</span><span class="p">],</span>
                                   <span class="p">[</span><span class="mf">0.6163482666015625</span><span class="p">,</span> <span class="mf">0.2326965481042862</span><span class="p">],</span> 
                                   <span class="p">[</span><span class="mf">0.6163482666015625</span><span class="p">,</span> <span class="mf">0.2326965481042862</span><span class="p">]]),</span> <span class="s">"Wrong masked attention"</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention</span><span class="p">,</span> <span class="n">weights</span> 
    
<span class="n">attention</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention_test</span><span class="p">(</span><span class="n">scaled_dot_product_attention</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[92mAll tests passed
tf.Tensor(
[[0.6928041  0.18632373]
 [0.61634827 0.23269655]
 [0.61634827 0.23269655]], shape=(3, 2), dtype=float32)
tf.Tensor(
[[0.3071959  0.5064804  0.         0.18632373]
 [0.38365173 0.38365173 0.         0.23269655]
 [0.38365173 0.38365173 0.         0.23269655]], shape=(3, 4), dtype=float32)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">matmul_qk</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">matmul_qk</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(4, 4)
tf.Tensor(
[[2. 3. 1. 1.]
 [2. 2. 2. 1.]
 [2. 2. 0. 1.]], shape=(3, 4), dtype=float32)
</code></pre></div></div>

<p>another sample to Obtaining Query, Key and Value matrix</p>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/ce597e22-7e6c-4e60-86f9-1d7af9a71b6b.png" alt="image.png" />!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#generate the emdedding the word vector for "this is book", each vector have 5 features
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Shape is :- </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">).</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">X</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">X</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Shape is :- (3, 5)





array([[ 0.47858264, -1.74327212, -0.82029071, -0.21163698,  1.31543753],
       [-0.09012238, -0.48729982,  0.84112528,  1.85078928,  0.32633046],
       [-1.62962921,  0.17127885, -0.49912593,  0.09966805, -0.55323133]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weight_of_query</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">weight_of_query</span>
<span class="n">Query</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">weight_of_query</span><span class="p">)</span>
<span class="n">Query</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[-2.28215318, -0.37214572,  0.97116363],
       [-0.04802642,  1.48761478, -0.61758178],
       [-2.79664269, -3.07753598, -0.81546614]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weight_of_key</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">weight_of_key</span>
<span class="n">Key</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">weight_of_key</span><span class="p">)</span>
<span class="n">Key</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[-2.03322686,  1.9992987 ,  2.09889272],
       [ 0.1997882 ,  0.12002915, -3.34110812],
       [ 3.23284659, -0.30207486, -0.76657696]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weight_of_values</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">weight_of_values</span>
<span class="n">Values</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">weight_of_values</span><span class="p">)</span>
<span class="n">Values</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[-2.60174708, -1.75190164, -1.32735991],
       [-2.93463348,  0.28382245, -1.42966431],
       [ 1.57416533,  3.9202751 , -0.21375642]])
</code></pre></div></div>

<p><strong>so most cases the W_q, W_k, W_v are same shape, so the q, k, v also have the same shape</strong><br />
<img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/18755036-dc11-421e-917c-7d2aec49f7fb.png" alt="image.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dimension</span><span class="o">=</span><span class="mi">5</span>
<span class="n">Scores</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Query</span><span class="p">,</span><span class="n">Key</span><span class="p">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dimension</span><span class="p">)</span> <span class="c1">#score is the weights
</span><span class="n">Scores</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 2.653977  , -1.6749841 , -3.58213928],
       [ 0.79407112,  0.998346  , -0.0586785 ],
       [-0.97415669,  0.80338804, -3.34799884]])
</code></pre></div></div>

<p>Excellent work! You can now implement self-attention. With that, you can start building the encoder block!</p>

<p><a name="4"></a></p>
<h2 id="4---encoder">4 - Encoder</h2>

<p>The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you’ll build later in the assignment. In this section of the assignment, you will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a).</p>
<center><img src="/assets//assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/encoder_layer.png" alt="Encoder" width="250" /></center>
<caption><center><font color="purple"><b>Figure 2a: Transformer encoder layer</b></font></center></caption>

<ul>
  <li><code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> you can think of as computing the self-attention several times to detect different features.</li>
  <li>Feed forward neural network contains two Dense layers which we’ll implement as the function <code class="language-plaintext highlighter-rouge">FullyConnected</code></li>
</ul>

<p>Your input sentence first passes through a <em>multi-head attention layer</em>, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a <em>feed forward neural network</em>. The exact same feed forward network is independently applied to each position.</p>

<ul>
  <li>For the <code class="language-plaintext highlighter-rouge">MultiHeadAttention</code> layer, you will use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">Keras implementation</a>. If you’re curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, you can look through the implementation.</li>
  <li>You will also use the <a href="https://keras.io/api/models/sequential/">Sequential API</a> with two dense layers to built the feed forward neural network layers.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">FullyConnected</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">fully_connected_dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">fully_connected_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>  <span class="c1"># (batch_size, seq_len, dff)
</span>        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>    <span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_ffn</span> <span class="o">=</span> <span class="n">FullyConnected</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
<span class="n">sample_ffn</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">))).</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorShape([64, 50, 512])
</code></pre></div></div>

<p><a name="4-1"></a></p>
<h3 id="41-encoder-layer">4.1 Encoder Layer</h3>

<p>Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training (Figure 2a).</p>

<p><a name="ex-4"></a></p>
<h3 id="exercise-4---encoderlayer">Exercise 4 - EncoderLayer</h3>

<p>Implement <code class="language-plaintext highlighter-rouge">EncoderLayer()</code> using the <code class="language-plaintext highlighter-rouge">call()</code> method</p>

<p>In this exercise, you will implement one encoder block (Figure 2) using the <code class="language-plaintext highlighter-rouge">call()</code> method. The function should perform the following steps:</p>
<ol>
  <li>You will pass the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute <em>self</em>-attention Q, V and K should be the same.</li>
  <li>Next, you will pass the output of the multi-head attention layer to a dropout layer. Don’t forget to use the <code class="language-plaintext highlighter-rouge">training</code> parameter to set the mode of your model.</li>
  <li>Now add a skip connection by adding your original input <code class="language-plaintext highlighter-rouge">x</code> and the output of the dropout layer.</li>
  <li>After adding the skip connection, pass the output through the first layer normalization.</li>
  <li>Finally, repeat steps 1-4 but with the feed forward neural network instead of the multi-head attention layer.</li>
</ol>

<p><strong>Additional Hints</strong>:</p>
<ul>
  <li>The <code class="language-plaintext highlighter-rouge">__init__</code> method creates all the layers that will be accesed by the the <code class="language-plaintext highlighter-rouge">call</code> method. Wherever you want to use a layer defined inside  the <code class="language-plaintext highlighter-rouge">__init__</code>  method you will have to use the syntax <code class="language-plaintext highlighter-rouge">self.[insert layer name]</code>.</li>
  <li>You will find the documentation of <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">MultiHeadAttention</a> helpful. <em>Note that if query, key and value are the same, then this function performs self-attention.</em></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># I borrow the another implemenation of the MultiHeadAttention to cross check 
</span><span class="k">def</span> <span class="nf">multihead_attention</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">W_KQV</span><span class="p">,</span> <span class="n">W_out</span><span class="p">):</span>
    <span class="n">N</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">K</span><span class="p">,</span><span class="n">Q</span><span class="p">,</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="o">@</span><span class="n">W_KQV</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">K</span><span class="p">,</span><span class="n">Q</span><span class="p">,</span><span class="n">V</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">heads</span><span class="p">,</span><span class="n">d</span><span class="o">//</span><span class="n">heads</span><span class="p">).</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">Q</span><span class="p">,</span><span class="n">V</span><span class="p">)]</span>
    
    <span class="n">attn</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">K</span><span class="o">@</span><span class="n">Q</span><span class="p">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="o">//</span><span class="n">heads</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">attn</span><span class="o">@</span><span class="n">V</span><span class="p">).</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">d</span><span class="p">)</span> <span class="o">@</span> <span class="n">W_out</span><span class="p">,</span> <span class="n">attn</span>
</code></pre></div></div>

<p>多头注意力由四部分组成：</p>

<p>线性层并分拆成多头。
按比缩放的点积注意力。
多头及联。
最后一层线性层。
每个多头注意力块有三个输入：Q（请求）、K（主键）、V（数值）。这些输入经过线性（Dense）层，并分拆成多头。– <font size="4" color="red">先经过dense 层,然后再分拆层多头matrix 喂给多个head ，经过学习后这些dense网络的W_q, W_k, W_v, 其实和前面实现中的W_q, W_k, W_v是相同的作用</font></p>

<p>将上面定义的 scaled_dot_product_attention 函数应用于每个头（进行了广播（broadcasted）以提高效率）。注意力这步必须使用一个恰当的 mask。然后将每个头的注意力输出连接起来（用tf.transpose 和 tf.reshape），并放入最后的 Dense 层。</p>

<p>Q、K、和 V 被拆分到了多个头，而非单个的注意力头，因为多头允许模型共同注意来自不同表示空间的不同位置的信息。在分拆后，每个头部的维度减少，因此总的计算成本与有着全部维度的单个注意力头相同。</p>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/d777e331-2ae4-48d6-8725-1975e945b192.png" alt="image.png" /></p>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/23a76e89-6143-4f1b-86c1-9e5f8a52e641.png" alt="image.png" />
<img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/a407995b-56d9-4c42-a6d9-f1fc11c53c76.png" alt="image.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># borrow the clip implemenation of the MultiHeadAttention to cross check 
</span><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_ctx</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_ctx</span> <span class="o">=</span> <span class="n">n_ctx</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">width</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">QKVMultiheadAttention</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">n_ctx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">width</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gelu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gelu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">c_fc</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>


<span class="k">class</span> <span class="nc">QKVMultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_ctx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_ctx</span> <span class="o">=</span> <span class="n">n_ctx</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">qkv</span><span class="p">):</span>
        <span class="c1">#get bachsize , 
</span>        <span class="n">bs</span><span class="p">,</span> <span class="n">n_ctx</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">attn_ch</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">attn_ch</span><span class="p">))</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_ctx</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1">### 分拆最后q,k,v维度到 (num_heads, depth).
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="n">attn_ch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="s">"bthc,bshc-&gt;bhts"</span><span class="p">,</span> <span class="n">q</span> <span class="o">*</span> <span class="n">scale</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="p">)</span>  <span class="c1"># More stable with f16 than dividing afterwards
</span>        <span class="n">wdtype</span> <span class="o">=</span> <span class="n">weight</span><span class="p">.</span><span class="n">dtype</span>

        <span class="c1">### calucate the softmax attention
</span>        <span class="n">weight</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">weight</span><span class="p">.</span><span class="nb">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nb">type</span><span class="p">(</span><span class="n">wdtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">th</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bhts,bshc-&gt;bthc"</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">v</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_ctx</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># I borrow the another implemenation of the MultiHeadAttention to cross check 
</span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="c1">## numbers heads will collapse the feature depth into heads.. such as feed features parallel to different heads so that improve the efficiency
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> 
        
        <span class="bp">self</span><span class="p">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="s">"""分拆最后一个维度到 (num_heads, depth).
        转置结果使得形状为 (batch_size, num_heads, seq_len, depth)
        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">depth</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">q</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">wq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">wk</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">wv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_q, depth)
</span>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_k, depth)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_v, depth)
</span>
        <span class="c1"># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)
</span>        <span class="c1"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
</span>        <span class="n">scaled_attention</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        
        <span class="c1">#reverse the split head procedure 
</span>        <span class="n">scaled_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># (batch_size, seq_len_q, num_heads, depth)
</span>    
        <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> 
                                      <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">))</span>  <span class="c1"># (batch_size, seq_len_q, d_model)
</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len_q, d_model)
</span>        <span class="k">if</span> <span class="n">return_attention_scores</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">temp_mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>  <span class="c1"># (batch_size, encoder_sequence, d_model)
</span><span class="n">out</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">temp_mha</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">return_attention_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">out</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">temp_mha</span><span class="p">.</span><span class="n">num_heads</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>8
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## if for tf , give the number_heads &gt; key_dim , how the output could be?
</span><span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">output_tensor</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span>
                               <span class="n">return_attention_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(None, 8, 4)
(None, 8, 8, 4)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#check the matrix split heads
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">depth</span> <span class="o">=</span> <span class="mi">512</span> <span class="o">//</span> <span class="mi">8</span>  <span class="c1"># numbers heads will collapse the feature depth into heads.. such as feed features parallel to different heads so that improve the efficiency
</span><span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">wq</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">wq</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1">#check split heads
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">depth</span><span class="p">))</span> <span class="c1"># (batch_size, seq_len_q ,num_heads, depth)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="c1"># (batch_size, num_heads, seq_len_q, depth)
</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorShape([1, 8, 60, 64])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION EncoderLayer
</span><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="s">"""
    The encoder layer is composed by a multi-head self-attention mechanism,
    followed by a simple, positionwise fully connected feed-forward network. 
    This archirecture includes a residual connection around each of the two 
    sub-layers, followed by layer normalization.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">fully_connected_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">layernorm_eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ownMultiHead</span> <span class="o">=</span> <span class="n">ownMultiHead</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="n">ownMultiHead</span><span class="o">==</span><span class="bp">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                          <span class="n">key_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">num_heads</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FullyConnected</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                  <span class="n">fully_connected_dim</span><span class="o">=</span><span class="n">fully_connected_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="s">"""
        Forward pass for the Encoder Layer
        
        Arguments:
            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)
            training -- Boolean, set to true to activate
                        the training mode for dropout layers
            mask -- Boolean mask to ensure that the padding is not 
                    treated as part of the input
        Returns:
            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)
        """</span>
        <span class="c1"># START CODE HERE
</span>        <span class="c1"># calculate self-attention using mha(~1 line)
</span>        <span class="c1">#-&gt; To compute self-attention Q, V and K should be the same (x)
</span>        <span class="c1">#ztd, namely it should be different q, v and k, but it seems merged with  EmbeddingParamtersMatrix*(Embedding matrix *X)
</span>        <span class="n">self_attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="c1"># Self attention (batch_size, input_seq_len, embedding_dim)
</span>        
        <span class="c1"># apply dropout layer to the self-attention output (~1 line)
</span>        <span class="n">self_attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">self_attn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        
        <span class="c1"># apply layer normalization on sum of the input and the attention output to get the  
</span>        <span class="c1"># output of the multi-head attention layer (~1 line)
</span>        <span class="n">mult_attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self_attn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, embedding_dim)
</span>
        <span class="c1"># pass the output of the multi-head attention layer through a ffn (~1 line)
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">mult_attn_out</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, embedding_dim)
</span>        
        <span class="c1"># apply dropout layer to ffn output (~1 line)
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        
        <span class="c1"># apply layer normalization on sum of the output from multi-head attention and ffn output to get the
</span>        <span class="c1"># output of the encoder layer (~1 line)
</span>        <span class="n">encoder_layer_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">ffn_output</span> <span class="o">+</span> <span class="n">mult_attn_out</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, embedding_dim)
</span>        <span class="c1"># END CODE HERE
</span>        
        <span class="k">return</span> <span class="n">encoder_layer_out</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="n">ownMultiHead</span><span class="o">=</span><span class="bp">True</span>
<span class="k">def</span> <span class="nf">EncoderLayer_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">encoder_layer1</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder_layer1</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]))</span>
    
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="s">"Wrong type. Output must be a tensor"</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">encoded</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected ((1, </span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s">))"</span>
    <span class="k">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">encoded</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> 
                       <span class="p">[[</span><span class="o">-</span><span class="mf">0.5214877</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.001476</span>  <span class="p">,</span> <span class="o">-</span><span class="mf">0.12321664</span><span class="p">,</span>  <span class="mf">1.6461804</span> <span class="p">],</span>
                       <span class="p">[</span><span class="o">-</span><span class="mf">1.3114998</span> <span class="p">,</span>  <span class="mf">1.2167752</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.5830886</span> <span class="p">,</span>  <span class="mf">0.6778133</span> <span class="p">],</span>
                       <span class="p">[</span> <span class="mf">0.25485858</span><span class="p">,</span>  <span class="mf">0.3776546</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.6564771</span> <span class="p">,</span>  <span class="mf">1.023964</span>  <span class="p">]],),</span> <span class="s">"Wrong values"</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>
    

<span class="n">EncoderLayer_test</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[[-0.12410855 -1.4025799   0.1106668   1.4160216 ]
  [-1.4312509  -0.02727069  0.06325354  1.3952682 ]
  [-0.0983822  -0.7909938  -0.7737439   1.6631199 ]]]



---------------------------------------------------------------------------

AssertionError                            Traceback (most recent call last)

/var/tmp/ipykernel_27065/4238340319.py in &lt;module&gt;
     18 
     19 
---&gt; 20 EncoderLayer_test(EncoderLayer)


/var/tmp/ipykernel_27065/4238340319.py in EncoderLayer_test(target)
     13                        [[-0.5214877 , -1.001476  , -0.12321664,  1.6461804 ],
     14                        [-1.3114998 ,  1.2167752 , -0.5830886 ,  0.6778133 ],
---&gt; 15                        [ 0.25485858,  0.3776546 , -1.6564771 ,  1.023964  ]],), "Wrong values"
     16 
     17     print("\033[92mAll tests passed")


AssertionError: Wrong values
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_encoder_layer</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span> <span class="c1">#embedding_dim, num_heads, fully_connected_dim
</span><span class="n">sample_encoder_layer_output</span> <span class="o">=</span> <span class="n">sample_encoder_layer</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="c1">#(batch_size, input_seq_len, embedding_dim)
</span>
<span class="n">sample_encoder_layer_output</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, input_seq_len, d_model)
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorShape([64, 43, 512])
</code></pre></div></div>

<p>不太明白这个矩阵检查的根据是什么， ingore this error firstly</p>

<p>assert np.allclose(encoded.numpy(), 
                       [[-0.5214877 , -1.001476  , -0.12321664,  1.6461804 ],
                       [-1.3114998 ,  1.2167752 , -0.5830886 ,  0.6778133 ],
                       [ 0.25485858,  0.3776546 , -1.6564771 ,  1.023964  ]],), “Wrong values”</p>

<p><a name="4-2"></a></p>
<h3 id="42---full-encoder">4.2 - Full Encoder</h3>

<p>Awesome job! You have now successfully implemented positional encoding, self-attention, and an encoder layer - give yourself a pat on the back. Now you’re ready to build the full Transformer Encoder (Figure 2b), where you will embedd your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers.</p>

<center><img src="/assets//assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/encoder.png" alt="Encoder" width="330" /></center>
<caption><center><font color="purple">Figure 2b: Transformer Encoder</font></center></caption>

<p><a name="ex-5"></a></p>
<h3 id="exercise-5---encoder">Exercise 5 - Encoder</h3>

<p>Complete the <code class="language-plaintext highlighter-rouge">Encoder()</code> function using the <code class="language-plaintext highlighter-rouge">call()</code> method to embed your input, add positional encoding, and implement multiple encoder layers</p>

<p>In this exercise, you will initialize your Encoder with an Embedding layer, positional encoding, and multiple EncoderLayers. Your <code class="language-plaintext highlighter-rouge">call()</code> method will perform the following steps:</p>
<ol>
  <li>Pass your input through the Embedding layer.</li>
  <li>Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type <code class="language-plaintext highlighter-rouge">tf.float32</code> before computing the square root.</li>
</ol>

<p>** some dicussion about this scal square root of your embedding dimension **
    This is specified in the original Transformer paper, at the end of section 3.4:
    <img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/7612d8b6-3931-45a4-901e-5765642e5458.png" alt="image.png" /><br />
    <font color="red">Transcription：
    **3.4 Embeddings and Softmax**
    Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension 𝑑model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by $\sqrt{𝑑_model}$</font></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>This aspect is not justified by the authors, either on the paper or anywhere else. It was specifically asked as an issue in the original implementation by Google with no response.      
    
Other implementations of the Transformer have also wondered if this was actually needed (see this, this and this).   
Some hypothesithed arguments (source) are:   
    
    It is for the sharing weight between the decoder embedding and the decoder pre-softmax linear weights.  
    It is not actually needed.  
    It is to make the positional encoding relatively smaller. This means the original meaning in the embedding vector won’t be lost when we add them together.  
    For reference, there are other StackExchange questions discussing this (see this and this).
Thank-you!! I'd also missed that multiply in my (fairseq transformer) code study, and it helps clear up a mystery that I'd noted: the (sinusoidal, non-learned) **positional embeddings are initialized with a range of -1.0 to +1.0, but the word-embeddings are initialized with a mean of 0.0 and s.d. of embedding_dim ** -0.5 (0.044 for 512, 0.03125 for 1024).**

So, on the face of it, the positional embeddings would overwhelm any signal coming from the word embeddings.

But now I can see word embeddings are scaled by math.sqrt(embed_dim) (22.6 for 512, 32 for 1024), it makes sense again.
Following the links in the other answer, it seems it is done this way &lt;font color=red&gt;**because the same embeddings can be used in other parts of the transformer model**, and that has decided the initialization values.&lt;\font&gt;
</code></pre></div></div>

<ol>
  <li>Add the position encoding: self.pos_encoding <code class="language-plaintext highlighter-rouge">[:, :seq_len, :]</code> to your embedding.</li>
  <li>
    <p>Pass the encoded embedding through a dropout layer, remembering to use the <code class="language-plaintext highlighter-rouge">training</code> parameter to set the model training mode.</p>

    <p>https://discuss.pytorch.org/t/why-use-dropout-in-positional-encoding-layer/159923/6    <br />
Dropout is a type of regularization. The final embedding for each token that you use (for the transformer) is a sum of positional and standard embeddings and then they apply dropout to that sum. So dropout is applied to the sum of the standard embedding and the positional embedding, not just the (constant) positional embedding. This sum is then an embedding, a bunch of parameters, and dropout is used to regularize as is usual. 
usually the “embedding” of a word is the embedding that’s used for that token. In this case, <font color="red">**the embedding is the parametric embedding + the constant positional encoding**</font>. When you apply dropout to a neuron, you kill the entire neuron. So if you have a sequence of length 10 and each token has 512 dimensional vectors, you kill on average 60% of the neurons in the 10 by 512 matrix that represents the data. If you only did this to the parametric embeddings and not the positional ones, you would not kill a neuron, you’d leave in its positional information, so it’s not really dropout.</p>
  </li>
  <li>Pass the output of the dropout layer through the stack of encoding layers using a for loop.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">1024</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.03125
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION
</span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="s">"""
    The entire Encoder starts by passing the input to an embedding layer 
    and using positional encoding to then pass the output through a stack of
    encoder Layers
        
    """</span>   
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">fully_connected_dim</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span>
               <span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">layernorm_eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">)</span> <span class="c1">#tensorflow Embedding function generate embedding matrix
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">maximum_position_encoding</span><span class="p">,</span>  <span class="c1">#generate the postion encoding matrix
</span>                                                <span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">)</span>

        <span class="c1">#init EncoderLayer 
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">enc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">,</span>
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                        <span class="n">fully_connected_dim</span><span class="o">=</span><span class="n">fully_connected_dim</span><span class="p">,</span>
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">layernorm_eps</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
    
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="s">"""
        Forward pass for the Encoder
        
        Arguments:
            x -- Tensor of shape (batch_size, input_seq_len)
            training -- Boolean, set to true to activate
                        the training mode for dropout layers
            mask -- Boolean mask to ensure that the padding is not 
                    treated as part of the input
        Returns:
            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)
        """</span>

        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># START CODE HERE
</span>        <span class="c1"># Pass input through the Embedding layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, embedding_dim)
</span>        <span class="c1"># Scale embedding by multiplying it by the square root of the embedding dimension
</span>        <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="c1"># Add the position encoding to embedding
</span>        <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
        <span class="c1"># Pass the encoded embedding through a dropout layer
</span>        <span class="c1"># why we first have one dropout layer ???
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="c1"># Pass the output through the stack of encoding layers 
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span><span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="c1"># END CODE HERE
</span>        <span class="c1">#Embedding layer also include FFN (feed forward network) +  dropout layer + normalize layer
</span>        <span class="c1">#the output is (batch_size, input_seq_len, embedding_dim) same as input
</span>
        <span class="k">return</span> <span class="n">x</span>  <span class="c1"># (batch_size, input_seq_len, embedding_dim)
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="k">def</span> <span class="nf">Encoder_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">4</span>
    
    <span class="n">encoderq</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                      <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                      <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                      <span class="n">fully_connected_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                      <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                      <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    
    <span class="n">encoderq_output</span> <span class="o">=</span> <span class="n">encoderq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">encoderq_output</span><span class="p">),</span> <span class="s">"Wrong type. Output must be a tensor"</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">encoderq_output</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embedding_dim</span><span class="p">),</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected (</span><span class="si">{</span><span class="n">eshape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">eshape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">embedding_dim</span><span class="si">}</span><span class="s">)"</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">encoderq_output</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> 
                       <span class="p">[[[</span><span class="o">-</span><span class="mf">0.40172306</span><span class="p">,</span>  <span class="mf">0.11519244</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2322885</span><span class="p">,</span>   <span class="mf">1.5188192</span> <span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.4017268</span><span class="p">,</span>   <span class="mf">0.33922842</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6836855</span><span class="p">,</span>   <span class="mf">0.9427304</span> <span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.4685002</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.6252842</span><span class="p">,</span>   <span class="mf">0.09368491</span><span class="p">,</span>  <span class="mf">1.063099</span>  <span class="p">]],</span>
                        <span class="p">[[</span><span class="o">-</span><span class="mf">0.3489219</span><span class="p">,</span>   <span class="mf">0.31335592</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3568854</span><span class="p">,</span>   <span class="mf">1.3924513</span> <span class="p">],</span>
                         <span class="p">[</span><span class="o">-</span><span class="mf">0.08761203</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1680029</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.2742313</span><span class="p">,</span>   <span class="mf">1.5298463</span> <span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.2627198</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.6140151</span><span class="p">,</span>   <span class="mf">0.2212624</span> <span class="p">,</span>  <span class="mf">1.130033</span>  <span class="p">]]]),</span> <span class="s">"Wrong values"</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>
    
<span class="n">Encoder_test</span><span class="p">(</span><span class="n">Encoder</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

AssertionError                            Traceback (most recent call last)

/var/tmp/ipykernel_27065/2836601725.py in &lt;module&gt;
     28     print("\033[92mAll tests passed")
     29 
---&gt; 30 Encoder_test(Encoder)


/var/tmp/ipykernel_27065/2836601725.py in Encoder_test(target)
     24                         [[-0.3489219,   0.31335592, -1.3568854,   1.3924513 ],
     25                          [-0.08761203, -0.1680029,  -1.2742313,   1.5298463 ],
---&gt; 26                          [ 0.2627198,  -1.6140151,   0.2212624 ,  1.130033  ]]]), "Wrong values"
     27 
     28     print("\033[92mAll tests passed")


AssertionError: Wrong values
</code></pre></div></div>

<p><a name="5"></a></p>
<h2 id="5---decoder">5 - Decoder</h2>

<p>The Decoder layer takes the K and V matrices generated by the Encoder and in computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).</p>

<center><img src="/assets//assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/decoder_layer.png" alt="Encoder" width="250" class="centerImage" /></center>
<caption><center><font color="purple">Figure 3a: Transformer Decoder layer</font></center></caption>

<p><a name="5-1"></a></p>
<h3 id="51---decoder-layer">5.1 - Decoder Layer</h3>
<p>Again, you’ll pair multi-head attention with a feed forward neural network, but this time you’ll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training (Figure 3a).</p>

<p><a name="ex-6"></a></p>
<h3 id="exercise-6---decoderlayer-include-ffndropoutnormlize-layer">Exercise 6 - DecoderLayer (include FFN+dropout+normlize layer)</h3>

<p>Implement <code class="language-plaintext highlighter-rouge">DecoderLayer()</code> using the <code class="language-plaintext highlighter-rouge">call()</code> method</p>

<ol>
  <li>Block 1 is a multi-head attention layer with a residual connection, dropout layer, and look-ahead mask.</li>
  <li>Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a dropout layer, layer normalization and a residual connection, just like you’ve done before.</li>
  <li>Finally, Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection.</li>
</ol>

<p><strong>Additional Hints:</strong></p>
<ul>
  <li>The first two blocks are fairly similar to the EncoderLayer except you will return <code class="language-plaintext highlighter-rouge">attention_scores</code> when computing self-attention</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION DecoderLayer
</span><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="s">"""
    The decoder layer is composed by two multi-head attention blocks, 
    one that takes the new input and uses self-attention, and the other 
    one that combines it with the output of the encoder, followed by a
    fully connected block. 
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">fully_connected_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">layernorm_eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">ownMultiHead</span><span class="o">==</span><span class="bp">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mha1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                          <span class="n">key_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mha2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                          <span class="n">key_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>            
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mha1</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="c1"># there are own wq1, wk1, wv1 for multihead 1
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">mha2</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="c1"># there are own wq2, wk2, wv2 for multihead 2
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FullyConnected</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                  <span class="n">fully_connected_dim</span><span class="o">=</span><span class="n">fully_connected_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layernorm3</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>
        <span class="s">"""
        Forward pass for the Decoder Layer
        
        Arguments:
            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)
            training -- Boolean, set to true to activate
                        the training mode for dropout layers
            look_ahead_mask -- Boolean mask for the target_input
            padding_mask -- Boolean mask for the second multihead attention layer
        Returns:
            out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)
            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)
        """</span>
        
        <span class="c1"># START CODE HERE
</span>        <span class="c1"># enc_output.shape == (batch_size, input_seq_len, embedding_dim)
</span>        
        <span class="c1"># BLOCK 1
</span>        <span class="c1"># calculate self-attention and return attention scores as attn_weights_block1 (~1 line)
</span>        <span class="c1"># decode first mh use x as input + look_ahead_mask
</span>        <span class="n">attn1</span><span class="p">,</span> <span class="n">attn_weights_block1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mha1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span><span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)
</span>        
        <span class="c1"># apply dropout layer on the attention output (~1 line)
</span>        <span class="n">attn1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn1</span><span class="p">,</span> <span class="n">training</span> <span class="o">=</span> <span class="n">training</span><span class="p">)</span>
        
        <span class="c1"># apply layer normalization to the sum of the attention output and the input (~1 line)
</span>        <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">attn1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>

        <span class="c1"># BLOCK 2
</span>        <span class="c1"># calculate self-attention using the Q from the first block and K and V from the encoder output.  so , K V is same from the output??
</span>        <span class="c1"># MultiHeadAttention's call takes input (Query, Value, Key, attention_mask, return_attention_scores, training)
</span>        <span class="c1"># Return attention scores as attn_weights_block2 (~1 line)
</span>        <span class="n">attn2</span><span class="p">,</span> <span class="n">attn_weights_block2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mha2</span><span class="p">(</span> <span class="n">out1</span><span class="p">,</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">,</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)
</span>        
        <span class="c1"># apply dropout layer on the attention output (~1 line)
</span>        <span class="n">attn2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">attn2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        
        <span class="c1"># apply layer normalization to the sum of the attention output and the output of the first block (~1 line)
</span>        <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">attn2</span> <span class="o">+</span> <span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, embedding_dim)
</span>        
        <span class="c1">#BLOCK 3
</span>        <span class="c1"># pass the output of the second block through a ffn
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out2</span><span class="p">)</span> <span class="c1"># (batch_size, target_seq_len, embedding_dim)
</span>        
        <span class="c1"># apply a dropout layer to the ffn output
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout3</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        
        <span class="c1"># apply layer normalization to the sum of the ffn output and the output of the second block
</span>        <span class="n">out3</span> <span class="o">=</span>  <span class="bp">self</span><span class="p">.</span><span class="n">layernorm3</span><span class="p">(</span><span class="n">ffn_output</span> <span class="o">+</span> <span class="n">out2</span><span class="p">)</span> <span class="c1"># (batch_size, target_seq_len, embedding_dim)
</span>        <span class="c1"># END CODE HERE
</span>
        <span class="k">return</span> <span class="n">out3</span><span class="p">,</span> <span class="n">attn_weights_block1</span><span class="p">,</span> <span class="n">attn_weights_block2</span>
    
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="n">ownMultiHead</span><span class="o">=</span><span class="bp">True</span>
<span class="k">def</span> <span class="nf">DecoderLayer_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span>  <span class="c1"># change to smaller number than embedding_dim
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="n">decoderLayerq</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span>
        <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">fully_connected_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
        <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
        <span class="n">layernorm_eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    
    <span class="n">encoderq_output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.40172306</span><span class="p">,</span>  <span class="mf">0.11519244</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2322885</span><span class="p">,</span>   <span class="mf">1.5188192</span> <span class="p">],</span>
                                   <span class="p">[</span> <span class="mf">0.4017268</span><span class="p">,</span>   <span class="mf">0.33922842</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6836855</span><span class="p">,</span>   <span class="mf">0.9427304</span> <span class="p">],</span>
                                   <span class="p">[</span> <span class="mf">0.4685002</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.6252842</span><span class="p">,</span>   <span class="mf">0.09368491</span><span class="p">,</span>  <span class="mf">1.063099</span>  <span class="p">]]])</span>
    
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    
    <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                       <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                       <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
    
    <span class="n">padding_mask</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">attn_w_b1</span><span class="p">,</span> <span class="n">attn_w_b2</span> <span class="o">=</span> <span class="n">decoderLayerq</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">encoderq_output</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">attn_w_b1</span><span class="p">),</span> <span class="s">"Wrong type for attn_w_b1. Output must be a tensor"</span>
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">attn_w_b2</span><span class="p">),</span> <span class="s">"Wrong type for attn_w_b2. Output must be a tensor"</span>
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="s">"Wrong type for out. Output must be a tensor"</span>
    
    <span class="n">shape1</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attn_w_b1</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">shape1</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s">"</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attn_w_b2</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">shape1</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s">"</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">out</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span>

    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">attn_w_b1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5271505</span><span class="p">,</span>  <span class="mf">0.47284946</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">),</span> <span class="s">"Wrong values in attn_w_b1. Check the call to self.mha1"</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">attn_w_b2</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.33365652</span><span class="p">,</span> <span class="mf">0.32598493</span><span class="p">,</span> <span class="mf">0.34035856</span><span class="p">]),</span>  <span class="s">"Wrong values in attn_w_b2. Check the call to self.mha2"</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.04726627</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6235218</span><span class="p">,</span> <span class="mf">1.0327158</span><span class="p">,</span> <span class="mf">0.54353976</span><span class="p">]),</span> <span class="s">"Wrong values in out"</span>
    

    <span class="c1"># Now let's try a example with padding mask
</span>    <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">attn_w_b1</span><span class="p">,</span> <span class="n">attn_w_b2</span> <span class="o">=</span> <span class="n">decoderLayerq</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">encoderq_output</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.34323323</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4689083</span><span class="p">,</span> <span class="mf">1.1092525</span><span class="p">,</span> <span class="mf">0.7028891</span><span class="p">]),</span> <span class="s">"Wrong values in out when we mask the last word. Are you passing the padding_mask to the inner functions?"</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>
    
<span class="n">DecoderLayer_test</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

AssertionError                            Traceback (most recent call last)

/var/tmp/ipykernel_27065/3323492748.py in &lt;module&gt;
     48     print("\033[92mAll tests passed")
     49 
---&gt; 50 DecoderLayer_test(DecoderLayer)


/var/tmp/ipykernel_27065/3323492748.py in DecoderLayer_test(target)
     35     assert tuple(tf.shape(out).numpy()) == q.shape, f"Wrong shape. We expected {q.shape}"
     36 
---&gt; 37     assert np.allclose(attn_w_b1[0, 0, 1], [0.5271505,  0.47284946, 0.], atol=1e-2), "Wrong values in attn_w_b1. Check the call to self.mha1"
     38     assert np.allclose(attn_w_b2[0, 0, 1], [0.33365652, 0.32598493, 0.34035856]),  "Wrong values in attn_w_b2. Check the call to self.mha2"
     39     assert np.allclose(out[0, 0], [0.04726627, -1.6235218, 1.0327158, 0.54353976]), "Wrong values in out"


AssertionError: Wrong values in attn_w_b1. Check the call to self.mha1
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_encoder_layer_output</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorShape([64, 43, 512])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_decoder_layer</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
<span class="n">sample_decoder_layer_output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_decoder_layer</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="n">sample_encoder_layer_output</span><span class="p">,</span> 
    <span class="bp">False</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, target_seq_len, d_model)
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorShape([64, 50, 512])
</code></pre></div></div>

<p><a name="5-2"></a></p>
<h3 id="52---full-decoder">5.2 - Full Decoder</h3>
<p>You’re almost there! Time to use your Decoder layer to build a full Transformer Decoder (Figure 3b). You will embedd your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers.</p>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/4733f7c9-6d0c-4872-a73e-d0b4e937639d.png" alt="image.png" /></p>
<center><img src="/assets//assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/decoder.png" alt="Encoder" width="300" /></center>
<caption><center><font color="purple">Figure 3b: Transformer Decoder&lt;/b&gt;</font></center></caption>

<p><a name="ex-7"></a></p>
<h3 id="exercise-7---decoder">Exercise 7 - Decoder</h3>

<p>Implement <code class="language-plaintext highlighter-rouge">Decoder()</code> using the <code class="language-plaintext highlighter-rouge">call()</code> method to embed your output, add positional encoding, and implement multiple decoder layers</p>

<p>In this exercise, you will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your <code class="language-plaintext highlighter-rouge">call()</code> method will perform the following steps:</p>
<ol>
  <li>Pass your generated output through the Embedding layer.</li>
  <li>Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type <code class="language-plaintext highlighter-rouge">tf.float32</code> before computing the square root.</li>
  <li>Add the position encoding: self.pos_encoding <code class="language-plaintext highlighter-rouge">[:, :seq_len, :]</code> to your embedding.</li>
  <li>Pass the encoded embedding through a dropout layer, remembering to use the <code class="language-plaintext highlighter-rouge">training</code> parameter to set the model training mode.</li>
  <li>Pass the output of the dropout layer through the stack of Decoding layers using a for loop.</li>
</ol>

<p>解码器包括：</p>

<ul>
  <li>输出嵌入（Output Embedding）</li>
  <li>位置编码（Positional Encoding）</li>
  <li>N 个解码器层（decoder layers）
目标（target）经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION Decoder
</span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="s">"""
    The entire Encoder is starts by passing the target input to an embedding layer 
    and using positional encoding to then pass the output through a stack of
    decoder Layers
        
    """</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">fully_connected_dim</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span>
               <span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">layernorm_eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dec_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">,</span>
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                        <span class="n">fully_connected_dim</span><span class="o">=</span><span class="n">fully_connected_dim</span><span class="p">,</span>
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">layernorm_eps</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> 
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>
        <span class="s">"""
        Forward  pass for the Decoder
        
        Arguments:
            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)
            training -- Boolean, set to true to activate
                        the training mode for dropout layers
            look_ahead_mask -- Boolean mask for the target_input
            padding_mask -- Boolean mask for the second multihead attention layer
        Returns:
            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
            attention_weights - Dictionary of tensors containing all the attention weights
                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)
        """</span>

        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="c1"># START CODE HERE
</span>        <span class="c1"># create word embeddings 
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, embedding_dim)
</span>        
        <span class="c1"># scale embeddings by multiplying by the square root of their dimension
</span>        <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
        
        <span class="c1"># calculate positional encodings and add to word embedding
</span>        <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
        
        <span class="c1"># apply a dropout layer to x
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="c1"># pass x and the encoder output through a stack of decoder layers and save the attention weights
</span>            <span class="c1"># of block 1 and 2 (~1 line)
</span>            <span class="c1">#the layer(i) mh2's output will be the layer(i+1)'s input x
</span>            <span class="n">x</span><span class="p">,</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>

            <span class="c1">#update/store attention_weights dictionary with the attention weights of block 1 and block 2
</span>            <span class="n">attention_weights</span><span class="p">[</span><span class="s">'decoder_layer{}_block1_self_att'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block1</span>
            <span class="n">attention_weights</span><span class="p">[</span><span class="s">'decoder_layer{}_block2_decenc_att'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block2</span>
        <span class="c1"># END CODE HERE
</span>        
        <span class="c1"># x.shape == (batch_size, target_seq_len, embedding_dim)
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_weights</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="k">def</span> <span class="nf">Decoder_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    
    <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">7</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">4</span> 
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span>
    <span class="n">fully_connected_dim</span><span class="o">=</span><span class="mi">8</span>
    <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">33</span>
    <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="mi">6</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    
    <span class="n">encoderq_output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.40172306</span><span class="p">,</span>  <span class="mf">0.11519244</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2322885</span><span class="p">,</span>   <span class="mf">1.5188192</span> <span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.4017268</span><span class="p">,</span>   <span class="mf">0.33922842</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6836855</span><span class="p">,</span>   <span class="mf">0.9427304</span> <span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.4685002</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.6252842</span><span class="p">,</span>   <span class="mf">0.09368491</span><span class="p">,</span>  <span class="mf">1.063099</span>  <span class="p">]],</span>
                        <span class="p">[[</span><span class="o">-</span><span class="mf">0.3489219</span><span class="p">,</span>   <span class="mf">0.31335592</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3568854</span><span class="p">,</span>   <span class="mf">1.3924513</span> <span class="p">],</span>
                         <span class="p">[</span><span class="o">-</span><span class="mf">0.08761203</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1680029</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.2742313</span><span class="p">,</span>   <span class="mf">1.5298463</span> <span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.2627198</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.6140151</span><span class="p">,</span>   <span class="mf">0.2212624</span> <span class="p">,</span>  <span class="mf">1.130033</span>  <span class="p">]]])</span>
    
    <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                       <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                       <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
    
    <span class="n">decoderk</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span>
                    <span class="n">embedding_dim</span><span class="p">,</span> 
                    <span class="n">num_heads</span><span class="p">,</span> 
                    <span class="n">fully_connected_dim</span><span class="p">,</span>
                    <span class="n">target_vocab_size</span><span class="p">,</span>
                    <span class="n">maximum_position_encoding</span><span class="p">)</span>
    <span class="n">outd</span><span class="p">,</span> <span class="n">att_weights</span> <span class="o">=</span> <span class="n">decoderk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoderq_output</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">outd</span><span class="p">),</span> <span class="s">"Wrong type for outd. It must be a dict"</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">outd</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">encoderq_output</span><span class="p">)),</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">encoderq_output</span><span class="p">)</span><span class="si">}</span><span class="s">"</span>
    <span class="k">print</span><span class="p">(</span><span class="n">outd</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">outd</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.2715261</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5606001</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.861783</span><span class="p">,</span> <span class="mf">1.69390933</span><span class="p">]),</span> <span class="s">"Wrong values in outd"</span>
    
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">att_weights</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">att_weights</span><span class="p">)</span> <span class="o">==</span> <span class="nb">dict</span><span class="p">,</span> <span class="s">"Wrong type for att_weights[0]. Output must be a tensor"</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong length for attention weights. It must be 2 x num_layers = </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">num_layers</span><span class="si">}</span><span class="s">"</span>
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">att_weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="sa">f</span><span class="s">"Wrong type for att_weights[</span><span class="si">{</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">]. Output must be a tensor"</span>
    <span class="n">shape1</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">att_weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">]]).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">shape1</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s">"</span> 
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">att_weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.52145624</span><span class="p">,</span> <span class="mf">0.47854376</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]),</span> <span class="sa">f</span><span class="s">"Wrong values in att_weights[</span><span class="si">{</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">]"</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>
    
<span class="n">Decoder_test</span><span class="p">(</span><span class="n">Decoder</span><span class="p">)</span>
</code></pre></div></div>

<p><a name="6"></a></p>
<h2 id="6---transformer">6 - Transformer</h2>

<p>Phew! This has been quite the assignment, and now you’ve made it to your last exercise of the Deep Learning Specialization. Congratulations! You’ve done all the hard work, now it’s time to put it all together.</p>

<center><img src="/assets//assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/transformer.png" alt="Transformer" width="550" /></center>
<caption><center><font color="purple">Figure 4: Transformer</font></center></caption>

<p>The flow of data through the Transformer Architecture is as follows:</p>
<ul>
  <li>First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:
    <ul>
      <li>embedding and positional encoding of your input</li>
      <li>multi-head attention on your input</li>
      <li>feed forward neural network to help detect features</li>
    </ul>
  </li>
  <li>Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:
    <ul>
      <li>embedding and positional encoding of the output</li>
      <li>multi-head attention on your generated output</li>
      <li>multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder</li>
      <li><strong>the decode input:tar is the target stenece? —  the encodeing input’s shifted right???</strong></li>
      <li>a feed forward neural network to help detect features</li>
    </ul>
  </li>
  <li>Finally, after the Nth Decoder layer, two dense layers and a softmax are applied to generate prediction for the next output in your sequence.</li>
</ul>

<p><a name="ex-8"></a></p>
<h3 id="exercise-8---transformer">Exercise 8 - Transformer</h3>

<p>Implement <code class="language-plaintext highlighter-rouge">Transformer()</code> using the <code class="language-plaintext highlighter-rouge">call()</code> method</p>
<ol>
  <li>Pass the input through the Encoder with the appropiate mask.</li>
  <li>Pass the encoder output and the target through the Decoder with the appropiate mask.</li>
  <li>Apply a linear transformation and a softmax to get a prediction.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION Transformer
# max_positional_encoding_target is used for decoder maximum_position_encoding
# max_positional_encoding_input is used for encoder maximum_position_encoding
# tar as the decoder X
</span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="s">"""
    Complete transformer with an Encoder and a Decoder
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">fully_connected_dim</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span> 
               <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">max_positional_encoding_input</span><span class="p">,</span>
               <span class="n">max_positional_encoding_target</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">layernorm_eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                               <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                               <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                               <span class="n">fully_connected_dim</span><span class="o">=</span><span class="n">fully_connected_dim</span><span class="p">,</span>
                               <span class="n">input_vocab_size</span><span class="o">=</span><span class="n">input_vocab_size</span><span class="p">,</span>
                               <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="n">max_positional_encoding_input</span><span class="p">,</span>
                               <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
                               <span class="n">layernorm_eps</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> 
                               <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                               <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                               <span class="n">fully_connected_dim</span><span class="o">=</span><span class="n">fully_connected_dim</span><span class="p">,</span>
                               <span class="n">target_vocab_size</span><span class="o">=</span><span class="n">target_vocab_size</span><span class="p">,</span> 
                               <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="n">max_positional_encoding_target</span><span class="p">,</span>
                               <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
                               <span class="n">layernorm_eps</span><span class="o">=</span><span class="n">layernorm_eps</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">):</span>
        <span class="s">"""
        Forward pass for the entire Transformer
        Arguments:
            inp -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)
            tar -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)
            training -- Boolean, set to true to activate
                        the training mode for dropout layers
            enc_padding_mask -- Boolean mask to ensure that the padding is not 
                    treated as part of the input
            look_ahead_mask -- Boolean mask for the target_input
            padding_mask -- Boolean mask for the second multihead attention layer
        Returns:
            final_output -- Describe me
            attention_weights - Dictionary of tensors containing all the attention weights for the decoder
                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)
        
        """</span>
        <span class="c1"># START CODE HERE
</span>        <span class="c1"># call self.encoder with the appropriate arguments to get the encoder output
</span>        <span class="n">enc_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span><span class="n">training</span><span class="p">,</span><span class="n">enc_padding_mask</span><span class="p">)</span> <span class="c1"># (batch_size, inp_seq_len, fully_connected_dim)
</span>        
        <span class="c1"># call self.decoder with the appropriate arguments to get the decoder output
</span>        <span class="c1"># dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)
</span>        <span class="c1"># so the tar is the target stenece? ---  the encodeing input's shifted right???
</span>        <span class="n">dec_output</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">tar</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">)</span>
        
        <span class="c1"># pass decoder output through a linear layer and softmax (~2 lines)
</span>        <span class="n">final_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)
</span>        <span class="c1"># START CODE HERE
</span>
        <span class="k">return</span> <span class="n">final_output</span><span class="p">,</span> <span class="n">attention_weights</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="n">ownMultiHead</span><span class="o">=</span><span class="bp">False</span>
<span class="k">def</span> <span class="nf">Transformer_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    
    <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>


    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">fully_connected_dim</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">input_vocab_size</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">target_vocab_size</span> <span class="o">=</span> <span class="mi">35</span>
    <span class="n">max_positional_encoding_input</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">max_positional_encoding_target</span> <span class="o">=</span> <span class="mi">6</span>

    <span class="n">trans</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> 
                        <span class="n">embedding_dim</span><span class="p">,</span> 
                        <span class="n">num_heads</span><span class="p">,</span> 
                        <span class="n">fully_connected_dim</span><span class="p">,</span> 
                        <span class="n">input_vocab_size</span><span class="p">,</span> 
                        <span class="n">target_vocab_size</span><span class="p">,</span> 
                        <span class="n">max_positional_encoding_input</span><span class="p">,</span>
                        <span class="n">max_positional_encoding_target</span><span class="p">)</span>
    <span class="c1"># 0 is the padding value
</span>    <span class="n">sentence_lang_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="n">sentence_lang_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
    <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

    <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">translation</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">trans</span><span class="p">(</span>
        <span class="n">sentence_lang_a</span><span class="p">,</span>
        <span class="n">sentence_lang_b</span><span class="p">,</span>
        <span class="bp">True</span><span class="p">,</span>
        <span class="n">enc_padding_mask</span><span class="p">,</span>
        <span class="n">look_ahead_mask</span><span class="p">,</span>
        <span class="n">dec_padding_mask</span>
    <span class="p">)</span>
    
    
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">translation</span><span class="p">),</span> <span class="s">"Wrong type for translation. Output must be a tensor"</span>
    <span class="n">shape1</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_positional_encoding_input</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">translation</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">shape1</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s">"</span>
        
    <span class="k">print</span><span class="p">(</span><span class="n">translation</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">translation</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span>
                       <span class="p">[[</span><span class="mf">0.02616475</span><span class="p">,</span> <span class="mf">0.02074359</span><span class="p">,</span> <span class="mf">0.01675757</span><span class="p">,</span> 
                         <span class="mf">0.025527</span><span class="p">,</span> <span class="mf">0.04473696</span><span class="p">,</span> <span class="mf">0.02171909</span><span class="p">,</span> 
                         <span class="mf">0.01542725</span><span class="p">,</span> <span class="mf">0.03658631</span><span class="p">]]),</span> <span class="s">"Wrong values in outd"</span>
    
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="nb">dict</span><span class="p">,</span> <span class="s">"Wrong type for weights. It must be a dict"</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong length for attention weights. It must be 2 x num_layers = </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">num_layers</span><span class="si">}</span><span class="s">"</span>
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="sa">f</span><span class="s">"Wrong type for att_weights[</span><span class="si">{</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">]. Output must be a tensor"</span>

    <span class="n">shape1</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">]]).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">shape1</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s">"</span> 
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4992985</span><span class="p">,</span> <span class="mf">0.5007015</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]),</span> <span class="sa">f</span><span class="s">"Wrong values in weights[</span><span class="si">{</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">]"</span>
    
    <span class="k">print</span><span class="p">(</span><span class="n">translation</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>

    
<span class="n">Transformer_test</span><span class="p">(</span><span class="n">Transformer</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.Tensor(
[0.02616475 0.02074359 0.01675757 0.025527   0.04473695 0.02171909
 0.01542725 0.0365863 ], shape=(8,), dtype=float32)
tf.Tensor(
[[[0.02616475 0.02074359 0.01675757 0.025527   0.04473695 0.02171909
   0.01542725 0.0365863  0.02433536 0.02948791 0.01698964 0.02147779
   0.05749574 0.02669398 0.01277918 0.03276358 0.0253941  0.01698772
   0.02758246 0.02529753 0.04394253 0.06258808 0.03667333 0.03009711
   0.05011231 0.01414333 0.01601289 0.01800467 0.02506283 0.01607273
   0.06204056 0.02099288 0.03005534 0.03070701 0.01854689]
  [0.02490053 0.017258   0.01794803 0.02998916 0.05038005 0.01997477
   0.01526351 0.03385608 0.03138068 0.02608407 0.01852771 0.01744511
   0.05923333 0.03287778 0.01450072 0.02815487 0.02676623 0.01684978
   0.02482791 0.02307897 0.04122656 0.05552058 0.03742857 0.03390088
   0.04666695 0.01667501 0.01400229 0.01981527 0.02202851 0.01818
   0.05918451 0.02173372 0.03040997 0.03337187 0.02055808]
  [0.01867789 0.01225462 0.02509719 0.04180384 0.06244645 0.02000666
   0.01934388 0.03032456 0.05771376 0.02616111 0.01742368 0.01100331
   0.05456048 0.04248188 0.02078063 0.02245298 0.03337655 0.02052129
   0.0239658  0.02193134 0.04068131 0.03323278 0.04556258 0.03676546
   0.04394966 0.01574801 0.01223158 0.02734469 0.01154951 0.02240609
   0.03563077 0.02169302 0.02025472 0.02886864 0.02175329]
  [0.02305287 0.01215192 0.02248081 0.0418811  0.05324595 0.016529
   0.01626855 0.02452858 0.05319852 0.01741914 0.02720063 0.01175192
   0.04887011 0.05262585 0.02324445 0.01787254 0.02867536 0.01768711
   0.01800392 0.01797924 0.02830286 0.03332606 0.0324963  0.04277937
   0.03038614 0.0323176  0.01166379 0.02618811 0.01842924 0.02784598
   0.04346567 0.02524558 0.03285819 0.0404315  0.02959607]
  [0.01859851 0.01163484 0.02560123 0.04363471 0.06270956 0.01928385
   0.01924486 0.02882556 0.06161031 0.02436098 0.01855855 0.01041807
   0.05321557 0.04556077 0.0220504  0.02093103 0.03341144 0.02041205
   0.02265851 0.02099104 0.03823084 0.03121315 0.04416506 0.03813418
   0.04104865 0.01757099 0.01183266 0.0281889  0.0114538  0.02377767
   0.03464996 0.02217591 0.02084129 0.03000083 0.02300425]]], shape=(1, 5, 35), dtype=float32)
[92mAll tests passed
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># UNIT TEST
</span><span class="n">ownMultiHead</span><span class="o">=</span><span class="bp">True</span>
<span class="k">def</span> <span class="nf">Transformer_test</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
    
    <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>


    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">fully_connected_dim</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">input_vocab_size</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">target_vocab_size</span> <span class="o">=</span> <span class="mi">35</span>
    <span class="n">max_positional_encoding_input</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">max_positional_encoding_target</span> <span class="o">=</span> <span class="mi">6</span>

    <span class="n">trans</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> 
                        <span class="n">embedding_dim</span><span class="p">,</span> 
                        <span class="n">num_heads</span><span class="p">,</span> 
                        <span class="n">fully_connected_dim</span><span class="p">,</span> 
                        <span class="n">input_vocab_size</span><span class="p">,</span> 
                        <span class="n">target_vocab_size</span><span class="p">,</span> 
                        <span class="n">max_positional_encoding_input</span><span class="p">,</span>
                        <span class="n">max_positional_encoding_target</span><span class="p">)</span>
    <span class="c1"># 0 is the padding value
</span>    <span class="n">sentence_lang_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="n">sentence_lang_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
    <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

    <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">translation</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">trans</span><span class="p">(</span>
        <span class="n">sentence_lang_a</span><span class="p">,</span>
        <span class="n">sentence_lang_b</span><span class="p">,</span>
        <span class="bp">True</span><span class="p">,</span>
        <span class="n">enc_padding_mask</span><span class="p">,</span>
        <span class="n">look_ahead_mask</span><span class="p">,</span>
        <span class="n">dec_padding_mask</span>
    <span class="p">)</span>
    
    
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">translation</span><span class="p">),</span> <span class="s">"Wrong type for translation. Output must be a tensor"</span>
    <span class="n">shape1</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_positional_encoding_input</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">translation</span><span class="p">).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">shape1</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s">"</span>
        
    <span class="k">print</span><span class="p">(</span><span class="n">translation</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">])</span>
    
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="nb">dict</span><span class="p">,</span> <span class="s">"Wrong type for weights. It must be a dict"</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong length for attention weights. It must be 2 x num_layers = </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">num_layers</span><span class="si">}</span><span class="s">"</span>
    <span class="k">assert</span> <span class="n">tf</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="sa">f</span><span class="s">"Wrong type for att_weights[</span><span class="si">{</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">]. Output must be a tensor"</span>

    <span class="n">shape1</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sentence_lang_a</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">]]).</span><span class="n">numpy</span><span class="p">())</span> <span class="o">==</span> <span class="n">shape1</span><span class="p">,</span> <span class="sa">f</span><span class="s">"Wrong shape. We expected </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s">"</span> 
    <span class="c1">#assert np.allclose(weights[keys[0]][0, 0, 1], [0.4992985, 0.5007015, 0., 0., 0.]), f"Wrong values in weights[{keys[0]}]"
</span>    
    <span class="k">print</span><span class="p">(</span><span class="n">translation</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[92mAll tests passed"</span><span class="p">)</span>
<span class="n">Transformer_test</span><span class="p">(</span><span class="n">Transformer</span><span class="p">)</span>    
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.Tensor(
[0.0138899  0.03013041 0.0194122  0.02183245 0.0163418  0.03393482
 0.05421878 0.04387819], shape=(8,), dtype=float32)
tf.Tensor(
[[[0.0138899  0.03013041 0.0194122  0.02183245 0.0163418  0.03393482
   0.05421878 0.04387819 0.0329827  0.01397073 0.03924095 0.01440488
   0.03002763 0.0154879  0.0176722  0.01628551 0.01585947 0.01415133
   0.03792994 0.02771122 0.02725577 0.02476936 0.04830182 0.07570544
   0.02002317 0.02237192 0.02127243 0.02123942 0.01698944 0.02112064
   0.043529   0.03428836 0.04166754 0.04055457 0.03154815]
  [0.01227451 0.02959899 0.0166487  0.02353616 0.01820176 0.03310344
   0.05079214 0.05486127 0.03501931 0.01502464 0.03344036 0.01418775
   0.03232233 0.01766093 0.01824826 0.01715104 0.01604581 0.01408619
   0.04389434 0.02434369 0.03227949 0.02387203 0.03963812 0.07077137
   0.01703417 0.01782698 0.02154371 0.02032982 0.01434441 0.02211295
   0.05294901 0.03850862 0.03668781 0.04071193 0.03094796]
  [0.01763018 0.03471405 0.01863351 0.02474838 0.02410761 0.03263579
   0.03585017 0.06357922 0.0360616  0.02341402 0.02914601 0.01382798
   0.03207213 0.02026083 0.02584696 0.01884262 0.01772998 0.01937712
   0.04135303 0.02141738 0.04634167 0.02295651 0.02191478 0.03458349
   0.01216602 0.01243537 0.03257969 0.0191417  0.01196113 0.02268301
   0.06083624 0.05173917 0.02795435 0.0412199  0.03023846]
  [0.01262886 0.02906489 0.01768985 0.02290371 0.01714453 0.03321528
   0.05317392 0.04823885 0.03380431 0.01412825 0.03580388 0.01471408
   0.03130832 0.01684265 0.01749932 0.01695023 0.01611957 0.01388939
   0.04119689 0.02608779 0.02872973 0.02446639 0.04545375 0.07689973
   0.01928073 0.02068552 0.02047833 0.02108692 0.01605351 0.02187557
   0.04714353 0.03499512 0.03939003 0.03994213 0.03111436]
  [0.01367486 0.02952728 0.01929193 0.02200459 0.01632005 0.03364716
   0.0543959  0.04299618 0.03275498 0.01379251 0.03886067 0.01478795
   0.03005839 0.01573534 0.01740936 0.01650989 0.01605004 0.01407218
   0.03796678 0.02783582 0.02657028 0.02490305 0.04947045 0.07766728
   0.02074305 0.02304619 0.0206537  0.02151686 0.01744339 0.02133087
   0.04263789 0.03324496 0.04177879 0.03989565 0.03140577]]], shape=(1, 5, 35), dtype=float32)
[92mAll tests passed
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ownMultiHead</span><span class="o">=</span><span class="bp">True</span>
<span class="n">sample_transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">fully_connected_dim</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> 
    <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> 
    <span class="n">max_positional_encoding_input</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> 
    <span class="n">max_positional_encoding_target</span><span class="o">=</span><span class="mi">6000</span><span class="p">)</span>

<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">62</span><span class="p">))</span>
<span class="n">temp_target</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">26</span><span class="p">))</span>

<span class="n">fn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_transformer</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> <span class="n">temp_target</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                               <span class="n">enc_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                               <span class="n">look_ahead_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                               <span class="n">dec_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">fn_out</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorShape([64, 26, 8000])
</code></pre></div></div>

<p>配置超参数（hyperparameters）
为了让本示例小且相对较快，已经减小了num_layers、 d_model 和 dff 的值。</p>

<p>Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048。关于所有其他版本的 Transformer，请查阅论文。</p>

<p>Note：通过改变以下数值，您可以获得在许多任务上达到最先进水平的模型。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">examples</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'ted_hrlr_translate/pt_to_en'</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                               <span class="n">as_supervised</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train_examples</span><span class="p">,</span> <span class="n">val_examples</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">'train'</span><span class="p">],</span> <span class="n">examples</span><span class="p">[</span><span class="s">'validation'</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1mDownloading and preparing dataset ted_hrlr_translate (124.94 MiB) to /home/jupyter/tensorflow_datasets/ted_hrlr_translate/pt_to_en/0.0.1...[0m


Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...:   0%|          | 0/124 [00:00&lt;?, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]iB/s][A
Dl Size...:   1%|          | 1/124 [00:00&lt;01:27,  1.41 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...:   2%|▏         | 2/124 [00:00&lt;01:26,  1.41 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...:   2%|▏         | 3/124 [00:00&lt;01:25,  1.41 MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]iB/s][A
Dl Size...:   3%|▎         | 4/124 [00:00&lt;00:20,  5.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...:   4%|▍         | 5/124 [00:00&lt;00:20,  5.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...:   5%|▍         | 6/124 [00:00&lt;00:20,  5.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...:   6%|▌         | 7/124 [00:00&lt;00:20,  5.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]
Dl Size...:   6%|▋         | 8/124 [00:00&lt;00:19,  5.82 MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]iB/s][A
Dl Size...:   7%|▋         | 9/124 [00:00&lt;00:08, 13.05 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:   8%|▊         | 10/124 [00:01&lt;00:08, 13.05 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:   9%|▉         | 11/124 [00:01&lt;00:08, 13.05 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  10%|▉         | 12/124 [00:01&lt;00:08, 13.05 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  10%|█         | 13/124 [00:01&lt;00:08, 13.05 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  11%|█▏        | 14/124 [00:01&lt;00:05, 19.04 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  12%|█▏        | 15/124 [00:01&lt;00:05, 19.04 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  13%|█▎        | 16/124 [00:01&lt;00:05, 19.04 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  14%|█▎        | 17/124 [00:01&lt;00:05, 19.04 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  15%|█▍        | 18/124 [00:01&lt;00:05, 19.04 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  15%|█▌        | 19/124 [00:01&lt;00:04, 24.24 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  16%|█▌        | 20/124 [00:01&lt;00:04, 24.24 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  17%|█▋        | 21/124 [00:01&lt;00:04, 24.24 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  18%|█▊        | 22/124 [00:01&lt;00:04, 24.24 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  19%|█▊        | 23/124 [00:01&lt;00:03, 27.63 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  19%|█▉        | 24/124 [00:01&lt;00:03, 27.63 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  20%|██        | 25/124 [00:01&lt;00:03, 27.63 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  21%|██        | 26/124 [00:01&lt;00:03, 27.63 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  22%|██▏       | 27/124 [00:01&lt;00:03, 30.56 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  23%|██▎       | 28/124 [00:01&lt;00:03, 30.56 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  23%|██▎       | 29/124 [00:01&lt;00:03, 30.56 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  24%|██▍       | 30/124 [00:01&lt;00:03, 30.56 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  25%|██▌       | 31/124 [00:01&lt;00:02, 32.46 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  26%|██▌       | 32/124 [00:01&lt;00:02, 32.46 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  27%|██▋       | 33/124 [00:01&lt;00:02, 32.46 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  27%|██▋       | 34/124 [00:01&lt;00:02, 32.46 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  28%|██▊       | 35/124 [00:01&lt;00:02, 32.46 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  29%|██▉       | 36/124 [00:01&lt;00:02, 35.21 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  30%|██▉       | 37/124 [00:01&lt;00:02, 35.21 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  31%|███       | 38/124 [00:01&lt;00:02, 35.21 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  31%|███▏      | 39/124 [00:01&lt;00:02, 35.21 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  32%|███▏      | 40/124 [00:01&lt;00:02, 35.71 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  33%|███▎      | 41/124 [00:01&lt;00:02, 35.71 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  34%|███▍      | 42/124 [00:01&lt;00:02, 35.71 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  35%|███▍      | 43/124 [00:01&lt;00:02, 35.71 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  35%|███▌      | 44/124 [00:01&lt;00:02, 35.71 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]MiB/s][A
Dl Size...:  36%|███▋      | 45/124 [00:01&lt;00:02, 37.55 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  37%|███▋      | 46/124 [00:01&lt;00:02, 37.55 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  38%|███▊      | 47/124 [00:01&lt;00:02, 37.55 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]
Dl Size...:  39%|███▊      | 48/124 [00:01&lt;00:02, 37.55 MiB/s][A

Extraction completed...: 0 file [00:01, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  40%|███▉      | 49/124 [00:02&lt;00:02, 37.24 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  40%|████      | 50/124 [00:02&lt;00:01, 37.24 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  41%|████      | 51/124 [00:02&lt;00:01, 37.24 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  42%|████▏     | 52/124 [00:02&lt;00:01, 37.24 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  43%|████▎     | 53/124 [00:02&lt;00:01, 37.24 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  44%|████▎     | 54/124 [00:02&lt;00:01, 39.37 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  44%|████▍     | 55/124 [00:02&lt;00:01, 39.37 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  45%|████▌     | 56/124 [00:02&lt;00:01, 39.37 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  46%|████▌     | 57/124 [00:02&lt;00:01, 39.37 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  47%|████▋     | 58/124 [00:02&lt;00:01, 39.37 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  48%|████▊     | 59/124 [00:02&lt;00:01, 38.63 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  48%|████▊     | 60/124 [00:02&lt;00:01, 38.63 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  49%|████▉     | 61/124 [00:02&lt;00:01, 38.63 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  50%|█████     | 62/124 [00:02&lt;00:01, 38.63 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  51%|█████     | 63/124 [00:02&lt;00:01, 38.63 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  52%|█████▏    | 64/124 [00:02&lt;00:01, 38.13 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  52%|█████▏    | 65/124 [00:02&lt;00:01, 38.13 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  53%|█████▎    | 66/124 [00:02&lt;00:01, 38.13 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  54%|█████▍    | 67/124 [00:02&lt;00:01, 38.13 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  55%|█████▍    | 68/124 [00:02&lt;00:01, 38.13 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  56%|█████▌    | 69/124 [00:02&lt;00:01, 39.58 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  56%|█████▋    | 70/124 [00:02&lt;00:01, 39.58 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  57%|█████▋    | 71/124 [00:02&lt;00:01, 39.58 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  58%|█████▊    | 72/124 [00:02&lt;00:01, 39.58 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  59%|█████▉    | 73/124 [00:02&lt;00:01, 38.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  60%|█████▉    | 74/124 [00:02&lt;00:01, 38.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  60%|██████    | 75/124 [00:02&lt;00:01, 38.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  61%|██████▏   | 76/124 [00:02&lt;00:01, 38.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  62%|██████▏   | 77/124 [00:02&lt;00:01, 38.34 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  63%|██████▎   | 78/124 [00:02&lt;00:01, 39.90 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  64%|██████▎   | 79/124 [00:02&lt;00:01, 39.90 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  65%|██████▍   | 80/124 [00:02&lt;00:01, 39.90 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  65%|██████▌   | 81/124 [00:02&lt;00:01, 39.90 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  66%|██████▌   | 82/124 [00:02&lt;00:01, 39.90 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]MiB/s][A
Dl Size...:  67%|██████▋   | 83/124 [00:02&lt;00:01, 39.35 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  68%|██████▊   | 84/124 [00:02&lt;00:01, 39.35 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  69%|██████▊   | 85/124 [00:02&lt;00:00, 39.35 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  69%|██████▉   | 86/124 [00:02&lt;00:00, 39.35 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]
Dl Size...:  70%|███████   | 87/124 [00:02&lt;00:00, 39.35 MiB/s][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]MiB/s][A
Dl Size...:  71%|███████   | 88/124 [00:03&lt;00:00, 38.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  72%|███████▏  | 89/124 [00:03&lt;00:00, 38.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  73%|███████▎  | 90/124 [00:03&lt;00:00, 38.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  73%|███████▎  | 91/124 [00:03&lt;00:00, 38.82 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  74%|███████▍  | 92/124 [00:03&lt;00:00, 38.82 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]MiB/s][A
Dl Size...:  75%|███████▌  | 93/124 [00:03&lt;00:00, 40.16 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  76%|███████▌  | 94/124 [00:03&lt;00:00, 40.16 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  77%|███████▋  | 95/124 [00:03&lt;00:00, 40.16 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  77%|███████▋  | 96/124 [00:03&lt;00:00, 40.16 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  78%|███████▊  | 97/124 [00:03&lt;00:00, 40.16 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]MiB/s][A
Dl Size...:  79%|███████▉  | 98/124 [00:03&lt;00:00, 39.51 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  80%|███████▉  | 99/124 [00:03&lt;00:00, 39.51 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  81%|████████  | 100/124 [00:03&lt;00:00, 39.51 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  81%|████████▏ | 101/124 [00:03&lt;00:00, 39.51 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  82%|████████▏ | 102/124 [00:03&lt;00:00, 39.51 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s] MiB/s][A
Dl Size...:  83%|████████▎ | 103/124 [00:03&lt;00:00, 40.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  84%|████████▍ | 104/124 [00:03&lt;00:00, 40.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  85%|████████▍ | 105/124 [00:03&lt;00:00, 40.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  85%|████████▌ | 106/124 [00:03&lt;00:00, 40.34 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  86%|████████▋ | 107/124 [00:03&lt;00:00, 40.34 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s] MiB/s][A
Dl Size...:  87%|████████▋ | 108/124 [00:03&lt;00:00, 39.83 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  88%|████████▊ | 109/124 [00:03&lt;00:00, 39.83 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  89%|████████▊ | 110/124 [00:03&lt;00:00, 39.83 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  90%|████████▉ | 111/124 [00:03&lt;00:00, 39.83 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s] MiB/s][A
Dl Size...:  90%|█████████ | 112/124 [00:03&lt;00:00, 39.72 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  91%|█████████ | 113/124 [00:03&lt;00:00, 39.72 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  92%|█████████▏| 114/124 [00:03&lt;00:00, 39.72 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  93%|█████████▎| 115/124 [00:03&lt;00:00, 39.72 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s] MiB/s][A
Dl Size...:  94%|█████████▎| 116/124 [00:03&lt;00:00, 38.85 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  94%|█████████▍| 117/124 [00:03&lt;00:00, 38.85 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  95%|█████████▌| 118/124 [00:03&lt;00:00, 38.85 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  96%|█████████▌| 119/124 [00:03&lt;00:00, 38.85 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  97%|█████████▋| 120/124 [00:03&lt;00:00, 38.85 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s] MiB/s][A
Dl Size...:  98%|█████████▊| 121/124 [00:03&lt;00:00, 39.96 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  98%|█████████▊| 122/124 [00:03&lt;00:00, 39.96 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...:  99%|█████████▉| 123/124 [00:03&lt;00:00, 39.96 MiB/s][A

Dl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]
Dl Size...: 100%|██████████| 124/124 [00:03&lt;00:00, 39.96 MiB/s][A

Dl Completed...: 100%|██████████| 1/1 [00:03&lt;00:00,  3.93s/ url]
Dl Size...: 100%|██████████| 124/124 [00:03&lt;00:00, 39.96 MiB/s][A

Dl Completed...: 100%|██████████| 1/1 [00:03&lt;00:00,  3.93s/ url]
Dl Size...: 100%|██████████| 124/124 [00:03&lt;00:00, 39.96 MiB/s][A

Extraction completed...:   0%|          | 0/1 [00:03&lt;?, ? file/s][A[A

Dl Completed...: 100%|██████████| 1/1 [00:08&lt;00:00,  3.93s/ url]7s/ file][A[A
Dl Size...: 100%|██████████| 124/124 [00:08&lt;00:00, 39.96 MiB/s][A

Extraction completed...: 100%|██████████| 1/1 [00:08&lt;00:00,  8.09s/ file][A[A
Dl Size...: 100%|██████████| 124/124 [00:08&lt;00:00, 15.33 MiB/s]
Dl Completed...: 100%|██████████| 1/1 [00:08&lt;00:00,  8.09s/ url]







Shuffling...:   0%|          | 0/1 [00:00&lt;?, ? shard/s]

WARNING:tensorflow:From /opt/conda/envs/tf/lib/python3.7/site-packages/tensorflow_datasets/core/file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`


WARNING:tensorflow:From /opt/conda/envs/tf/lib/python3.7/site-packages/tensorflow_datasets/core/file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`

Reading...: 0 examples [00:00, ? examples/s][A
                                            [A
Writing...:   0%|          | 0/51785 [00:00&lt;?, ? examples/s][A
Shuffling...:   0%|          | 0/1 [00:00&lt;?, ? shard/s]        
Reading...: 0 examples [00:00, ? examples/s][A
                                            [A
Writing...:   0%|          | 0/1193 [00:00&lt;?, ? examples/s][A
Shuffling...:   0%|          | 0/1 [00:00&lt;?, ? shard/s]    [A
Reading...: 0 examples [00:00, ? examples/s][A
                                            [A
Writing...:   0%|          | 0/1803 [00:00&lt;?, ? examples/s][A
                                                           [A

[1mDataset ted_hrlr_translate downloaded and prepared to /home/jupyter/tensorflow_datasets/ted_hrlr_translate/pt_to_en/0.0.1. Subsequent calls will reuse this data.[0m


WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer_en</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">features</span><span class="p">.</span><span class="n">text</span><span class="p">.</span><span class="n">SubwordTextEncoder</span><span class="p">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="p">(</span><span class="n">en</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">),</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">)</span>

<span class="n">tokenizer_pt</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">features</span><span class="p">.</span><span class="n">text</span><span class="p">.</span><span class="n">SubwordTextEncoder</span><span class="p">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pt</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">),</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2023-05-09 11:56:13.233103: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_string</span> <span class="o">=</span> <span class="s">'Transformer is awesome.'</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'Tokenized string is {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>

<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'The original string: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">original_string</span> <span class="o">==</span> <span class="n">sample_string</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]
The original string: Transformer is awesome.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">'{} ----&gt; {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer_en</span><span class="p">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>7915 ----&gt; T
1248 ----&gt; ran
7946 ----&gt; s
7194 ----&gt; former 
13 ----&gt; is 
2799 ----&gt; awesome
7877 ----&gt; .
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">):</span>
    <span class="n">lang1</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_pt</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang1</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="p">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">lang2</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_en</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang2</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="p">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">40</span>
<span class="k">def</span> <span class="nf">filter_max_length</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">,</span>
                        <span class="n">tf</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tf_encode</span><span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">):</span>
    <span class="n">result_pt</span><span class="p">,</span> <span class="n">result_en</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">py_function</span><span class="p">(</span><span class="n">encode</span><span class="p">,</span> <span class="p">[</span><span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">int64</span><span class="p">])</span>
    <span class="n">result_pt</span><span class="p">.</span><span class="n">set_shape</span><span class="p">([</span><span class="bp">None</span><span class="p">])</span>
    <span class="n">result_en</span><span class="p">.</span><span class="n">set_shape</span><span class="p">([</span><span class="bp">None</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">result_pt</span><span class="p">,</span> <span class="n">result_en</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_examples</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span>
<span class="c1"># 将数据集缓存到内存中以加快读取速度。
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">).</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>


<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_examples</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_dataset</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">).</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pt_batch</span><span class="p">,</span> <span class="n">en_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">))</span>
<span class="n">pt_batch</span><span class="p">,</span> <span class="n">en_batch</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;tf.Tensor: shape=(64, 40), dtype=int64, numpy=
 array([[8214, 1259,    5, ...,    0,    0,    0],
        [8214,  299,   13, ...,    0,    0,    0],
        [8214,   59,    8, ...,    0,    0,    0],
        ...,
        [8214,   95,    3, ...,    0,    0,    0],
        [8214, 5157,    1, ...,    0,    0,    0],
        [8214, 4479, 7990, ...,    0,    0,    0]])&gt;,
 &lt;tf.Tensor: shape=(64, 40), dtype=int64, numpy=
 array([[8087,   18,   12, ...,    0,    0,    0],
        [8087,  634,   30, ...,    0,    0,    0],
        [8087,   16,   13, ...,    0,    0,    0],
        ...,
        [8087,   12,   20, ...,    0,    0,    0],
        [8087,   17, 4981, ...,    0,    0,    0],
        [8087,   12, 5453, ...,    0,    0,    0]])&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">dff</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_pt</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
</code></pre></div></div>

<p>优化器（Optimizer）
根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用。</p>

\[\Large{lrate = d_{model}^{-0.5} * min(step{\_}num^{-0.5}, step{\_}num * warmup{\_}steps^{-1.5})}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CustomSchedule</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">schedules</span><span class="p">.</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomSchedule</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">arg1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="n">arg2</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">warmup_steps</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> 
                                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
<span class="n">temp_learning_rate_schedule</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">temp_learning_rate_schedule</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Learning Rate"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Train Step"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'Train Step')
</code></pre></div></div>

<p><img src="/assets/2023-12-03-Transformer-learning-C5W4A1SubclassV1_files/2023-12-03-Transformer-learning-C5W4A1SubclassV1_89_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
    <span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss_</span> <span class="o">*=</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'train_loss'</span><span class="p">)</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s">'train_accuracy'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                          <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> 
                          <span class="n">max_positional_encoding_input</span><span class="o">=</span><span class="n">input_vocab_size</span><span class="p">,</span> 
                          <span class="n">max_positional_encoding_target</span><span class="o">=</span><span class="n">target_vocab_size</span><span class="p">,</span>
                          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
    <span class="c1"># 编码器填充遮挡
</span>    <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
    <span class="c1"># 在解码器的第二个注意力模块使用。
</span>    <span class="c1"># 该填充遮挡用于遮挡编码器的输出。
</span>    <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

    <span class="c1"># 在解码器的第一个注意力模块使用。
</span>    <span class="c1"># 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。
</span>    <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tar</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">dec_target_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">tar</span><span class="p">)</span>
    <span class="n">combined_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dec_target_padding_mask</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s">"./checkpoints/train"</span>

<span class="n">ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="n">ckpt_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">CheckpointManager</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># 如果检查点存在，则恢复最新的检查点。
</span><span class="k">if</span> <span class="n">ckpt_manager</span><span class="p">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
    <span class="n">ckpt</span><span class="p">.</span><span class="n">restore</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="p">.</span><span class="n">latest_checkpoint</span><span class="p">)</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">'Latest checkpoint restored!!'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1"># 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地
# 执行。该函数专用于参数张量的精确形状。为了避免由于可变序列长度或可变
# 批次大小（最后一批次较小）导致的再追踪，使用 input_signature 指定
# 更多的通用形状。
</span>
<span class="n">train_step_signature</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int64</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int64</span><span class="p">),</span>
<span class="p">]</span>

<span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="n">train_step_signature</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
    <span class="n">tar_inp</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tar_real</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

    <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">,</span> 
                                     <span class="bp">True</span><span class="p">,</span> 
                                     <span class="n">enc_padding_mask</span><span class="p">,</span> 
                                     <span class="n">combined_mask</span><span class="p">,</span> 
                                     <span class="n">dec_padding_mask</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">transformer</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>    
        <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">transformer</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
        
        <span class="n">train_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">train_accuracy</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">train_loss</span><span class="p">.</span><span class="n">reset_states</span><span class="p">()</span>
    <span class="n">train_accuracy</span><span class="p">.</span><span class="n">reset_states</span><span class="p">()</span>

    <span class="c1"># inp -&gt; portuguese, tar -&gt; english
</span>    <span class="k">for</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">):</span>
        <span class="n">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="s">'Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
                <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">train_accuracy</span><span class="p">.</span><span class="n">result</span><span class="p">()))</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ckpt_save_path</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="p">.</span><span class="n">save</span><span class="p">()</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">'Saving checkpoint for epoch {} at {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                                                         <span class="n">ckpt_save_path</span><span class="p">))</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">'Epoch {} Loss {:.4f} Accuracy {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> 
                                            <span class="n">train_loss</span><span class="p">.</span><span class="n">result</span><span class="p">(),</span> 
                                            <span class="n">train_accuracy</span><span class="p">.</span><span class="n">result</span><span class="p">()))</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">'Time taken for 1 epoch: {} secs</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2023-05-09 12:09:35.813744: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 13805 of 20000
2023-05-09 12:09:40.167687: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.
/opt/conda/envs/tf/lib/python3.7/site-packages/keras/backend.py:4907: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
  '"`sparse_categorical_crossentropy` received `from_logits=True`, but '


Epoch 1 Batch 0 Loss 4.4735 Accuracy 0.0000
Epoch 1 Batch 50 Loss 4.2140 Accuracy 0.0030
Epoch 1 Batch 100 Loss 4.1867 Accuracy 0.0151
Epoch 1 Batch 150 Loss 4.1394 Accuracy 0.0195
Epoch 1 Batch 200 Loss 4.0686 Accuracy 0.0261
Epoch 1 Batch 250 Loss 3.9926 Accuracy 0.0342
Epoch 1 Batch 300 Loss 3.9032 Accuracy 0.0396
Epoch 1 Batch 350 Loss 3.8193 Accuracy 0.0438
Epoch 1 Batch 400 Loss 3.7331 Accuracy 0.0472
Epoch 1 Batch 450 Loss 3.6467 Accuracy 0.0520
Epoch 1 Batch 500 Loss 3.5620 Accuracy 0.0580
Epoch 1 Batch 550 Loss 3.4791 Accuracy 0.0647
Epoch 1 Batch 600 Loss 3.3928 Accuracy 0.0722
Epoch 1 Batch 650 Loss 3.3050 Accuracy 0.0804
Epoch 1 Batch 700 Loss 3.2147 Accuracy 0.0884
Epoch 1 Loss 3.2126 Accuracy 0.0887
Time taken for 1 epoch: 931.3518960475922 secs

Epoch 2 Batch 0 Loss 2.1738 Accuracy 0.2027
Epoch 2 Batch 50 Loss 1.8804 Accuracy 0.2144
Epoch 2 Batch 100 Loss 1.8061 Accuracy 0.2266
Epoch 2 Batch 150 Loss 1.7319 Accuracy 0.2366
Epoch 2 Batch 200 Loss 1.6622 Accuracy 0.2467
Epoch 2 Batch 250 Loss 1.6010 Accuracy 0.2561
Epoch 2 Batch 300 Loss 1.5416 Accuracy 0.2644
Epoch 2 Batch 350 Loss 1.4910 Accuracy 0.2720
Epoch 2 Batch 400 Loss 1.4425 Accuracy 0.2788
Epoch 2 Batch 450 Loss 1.3931 Accuracy 0.2846
Epoch 2 Batch 500 Loss 1.3492 Accuracy 0.2897
Epoch 2 Batch 550 Loss 1.3049 Accuracy 0.2945
Epoch 2 Batch 600 Loss 1.2646 Accuracy 0.2993
Epoch 2 Batch 650 Loss 1.2268 Accuracy 0.3045
Epoch 2 Batch 700 Loss 1.1889 Accuracy 0.3088
Epoch 2 Loss 1.1872 Accuracy 0.3089
Time taken for 1 epoch: 878.3964555263519 secs

Epoch 3 Batch 0 Loss 0.7012 Accuracy 0.3709
Epoch 3 Batch 50 Loss 0.6513 Accuracy 0.3779
Epoch 3 Batch 100 Loss 0.6306 Accuracy 0.3825
Epoch 3 Batch 150 Loss 0.6076 Accuracy 0.3854
Epoch 3 Batch 200 Loss 0.5862 Accuracy 0.3889
Epoch 3 Batch 250 Loss 0.5626 Accuracy 0.3923
Epoch 3 Batch 300 Loss 0.5408 Accuracy 0.3962
Epoch 3 Batch 350 Loss 0.5193 Accuracy 0.3995
Epoch 3 Batch 400 Loss 0.4992 Accuracy 0.4032
Epoch 3 Batch 450 Loss 0.4803 Accuracy 0.4065
Epoch 3 Batch 500 Loss 0.4607 Accuracy 0.4088
Epoch 3 Batch 550 Loss 0.4430 Accuracy 0.4120
Epoch 3 Batch 600 Loss 0.4256 Accuracy 0.4150
Epoch 3 Batch 650 Loss 0.4084 Accuracy 0.4172
Epoch 3 Batch 700 Loss 0.3918 Accuracy 0.4201
Epoch 3 Loss 0.3911 Accuracy 0.4201
Time taken for 1 epoch: 878.9100172519684 secs

Epoch 4 Batch 0 Loss 0.1494 Accuracy 0.4347
Epoch 4 Batch 50 Loss 0.1458 Accuracy 0.4610
Epoch 4 Batch 100 Loss 0.1356 Accuracy 0.4609
Epoch 4 Batch 150 Loss 0.1269 Accuracy 0.4615
Epoch 4 Batch 200 Loss 0.1185 Accuracy 0.4626
Epoch 4 Batch 250 Loss 0.1106 Accuracy 0.4639
Epoch 4 Batch 300 Loss 0.1035 Accuracy 0.4654
Epoch 4 Batch 350 Loss 0.0966 Accuracy 0.4653
Epoch 4 Batch 400 Loss 0.0901 Accuracy 0.4656
Epoch 4 Batch 450 Loss 0.0843 Accuracy 0.4658
Epoch 4 Batch 500 Loss 0.0791 Accuracy 0.4667
Epoch 4 Batch 550 Loss 0.0743 Accuracy 0.4667
Epoch 4 Batch 600 Loss 0.0698 Accuracy 0.4671
Epoch 4 Batch 650 Loss 0.0659 Accuracy 0.4673
Epoch 4 Batch 700 Loss 0.0623 Accuracy 0.4676
Epoch 4 Loss 0.0622 Accuracy 0.4675
Time taken for 1 epoch: 876.3038239479065 secs

Epoch 5 Batch 0 Loss 0.0117 Accuracy 0.4605
Epoch 5 Batch 50 Loss 0.0130 Accuracy 0.4672
Epoch 5 Batch 100 Loss 0.0126 Accuracy 0.4679
Epoch 5 Batch 150 Loss 0.0121 Accuracy 0.4712
Epoch 5 Batch 200 Loss 0.0114 Accuracy 0.4714
Epoch 5 Batch 250 Loss 0.0108 Accuracy 0.4705
Epoch 5 Batch 300 Loss 0.0102 Accuracy 0.4697
Epoch 5 Batch 350 Loss 0.0097 Accuracy 0.4688
Epoch 5 Batch 400 Loss 0.0095 Accuracy 0.4699
Epoch 5 Batch 450 Loss 0.0093 Accuracy 0.4699
Epoch 5 Batch 500 Loss 0.0092 Accuracy 0.4711
Epoch 5 Batch 550 Loss 0.0089 Accuracy 0.4713
Epoch 5 Batch 600 Loss 0.0087 Accuracy 0.4709
Epoch 5 Batch 650 Loss 0.0085 Accuracy 0.4710
Epoch 5 Batch 700 Loss 0.0082 Accuracy 0.4712
Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1
Epoch 5 Loss 0.0082 Accuracy 0.4712
Time taken for 1 epoch: 878.6954836845398 secs

Epoch 6 Batch 0 Loss 0.0059 Accuracy 0.4674
Epoch 6 Batch 50 Loss 0.0077 Accuracy 0.4734
Epoch 6 Batch 100 Loss 0.0066 Accuracy 0.4704
Epoch 6 Batch 150 Loss 0.0062 Accuracy 0.4702
Epoch 6 Batch 200 Loss 0.0064 Accuracy 0.4696
Epoch 6 Batch 250 Loss 0.0062 Accuracy 0.4698
Epoch 6 Batch 300 Loss 0.0061 Accuracy 0.4709
Epoch 6 Batch 350 Loss 0.0060 Accuracy 0.4704
Epoch 6 Batch 400 Loss 0.0059 Accuracy 0.4702
Epoch 6 Batch 450 Loss 0.0059 Accuracy 0.4703
Epoch 6 Batch 500 Loss 0.0059 Accuracy 0.4706
Epoch 6 Batch 550 Loss 0.0058 Accuracy 0.4706
Epoch 6 Batch 600 Loss 0.0057 Accuracy 0.4709
Epoch 6 Batch 650 Loss 0.0055 Accuracy 0.4711
Epoch 6 Batch 700 Loss 0.0055 Accuracy 0.4710
Epoch 6 Loss 0.0056 Accuracy 0.4709
Time taken for 1 epoch: 877.8011014461517 secs

Epoch 7 Batch 0 Loss 0.0037 Accuracy 0.4639
Epoch 7 Batch 50 Loss 0.0053 Accuracy 0.4670
Epoch 7 Batch 100 Loss 0.0048 Accuracy 0.4698
Epoch 7 Batch 150 Loss 0.0049 Accuracy 0.4722
Epoch 7 Batch 200 Loss 0.0048 Accuracy 0.4723
Epoch 7 Batch 250 Loss 0.0048 Accuracy 0.4707
Epoch 7 Batch 300 Loss 0.0046 Accuracy 0.4708
Epoch 7 Batch 350 Loss 0.0046 Accuracy 0.4704
Epoch 7 Batch 400 Loss 0.0047 Accuracy 0.4694
Epoch 7 Batch 450 Loss 0.0046 Accuracy 0.4699
Epoch 7 Batch 500 Loss 0.0045 Accuracy 0.4704
Epoch 7 Batch 550 Loss 0.0045 Accuracy 0.4705
Epoch 7 Batch 600 Loss 0.0044 Accuracy 0.4704
Epoch 7 Batch 650 Loss 0.0044 Accuracy 0.4703
Epoch 7 Batch 700 Loss 0.0044 Accuracy 0.4707
Epoch 7 Loss 0.0044 Accuracy 0.4707
Time taken for 1 epoch: 877.6704905033112 secs

Epoch 8 Batch 0 Loss 0.0043 Accuracy 0.4354
Epoch 8 Batch 50 Loss 0.0041 Accuracy 0.4712
Epoch 8 Batch 100 Loss 0.0040 Accuracy 0.4708
Epoch 8 Batch 150 Loss 0.0037 Accuracy 0.4711
Epoch 8 Batch 200 Loss 0.0035 Accuracy 0.4703
Epoch 8 Batch 250 Loss 0.0034 Accuracy 0.4696
Epoch 8 Batch 300 Loss 0.0033 Accuracy 0.4704
Epoch 8 Batch 350 Loss 0.0032 Accuracy 0.4705
Epoch 8 Batch 400 Loss 0.0033 Accuracy 0.4708
Epoch 8 Batch 450 Loss 0.0033 Accuracy 0.4707
Epoch 8 Batch 500 Loss 0.0033 Accuracy 0.4708
Epoch 8 Batch 550 Loss 0.0033 Accuracy 0.4714
Epoch 8 Batch 600 Loss 0.0033 Accuracy 0.4717
Epoch 8 Batch 650 Loss 0.0033 Accuracy 0.4720
Epoch 8 Batch 700 Loss 0.0033 Accuracy 0.4717
Epoch 8 Loss 0.0033 Accuracy 0.4717
Time taken for 1 epoch: 871.3958556652069 secs

Epoch 9 Batch 0 Loss 0.0122 Accuracy 0.4700
Epoch 9 Batch 50 Loss 0.0029 Accuracy 0.4676
Epoch 9 Batch 100 Loss 0.0030 Accuracy 0.4734
Epoch 9 Batch 150 Loss 0.0030 Accuracy 0.4718
Epoch 9 Batch 200 Loss 0.0029 Accuracy 0.4721
Epoch 9 Batch 250 Loss 0.0028 Accuracy 0.4713
Epoch 9 Batch 300 Loss 0.0028 Accuracy 0.4717
Epoch 9 Batch 350 Loss 0.0028 Accuracy 0.4704
Epoch 9 Batch 400 Loss 0.0028 Accuracy 0.4703
Epoch 9 Batch 450 Loss 0.0028 Accuracy 0.4701
Epoch 9 Batch 500 Loss 0.0028 Accuracy 0.4710
Epoch 9 Batch 550 Loss 0.0028 Accuracy 0.4709
Epoch 9 Batch 600 Loss 0.0028 Accuracy 0.4704
Epoch 9 Batch 650 Loss 0.0028 Accuracy 0.4704
Epoch 9 Batch 700 Loss 0.0028 Accuracy 0.4706
Epoch 9 Loss 0.0028 Accuracy 0.4707
Time taken for 1 epoch: 877.2159721851349 secs

Epoch 10 Batch 0 Loss 0.0082 Accuracy 0.4655
Epoch 10 Batch 50 Loss 0.0024 Accuracy 0.4676
Epoch 10 Batch 100 Loss 0.0026 Accuracy 0.4686
Epoch 10 Batch 150 Loss 0.0025 Accuracy 0.4703
Epoch 10 Batch 200 Loss 0.0026 Accuracy 0.4698
Epoch 10 Batch 250 Loss 0.0027 Accuracy 0.4690
Epoch 10 Batch 300 Loss 0.0027 Accuracy 0.4687
Epoch 10 Batch 350 Loss 0.0027 Accuracy 0.4704
Epoch 10 Batch 400 Loss 0.0028 Accuracy 0.4704
Epoch 10 Batch 450 Loss 0.0027 Accuracy 0.4700
Epoch 10 Batch 500 Loss 0.0027 Accuracy 0.4707
Epoch 10 Batch 550 Loss 0.0027 Accuracy 0.4705
Epoch 10 Batch 600 Loss 0.0026 Accuracy 0.4704
Epoch 10 Batch 650 Loss 0.0026 Accuracy 0.4707
Epoch 10 Batch 700 Loss 0.0026 Accuracy 0.4708
Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2
Epoch 10 Loss 0.0026 Accuracy 0.4709
Time taken for 1 epoch: 879.2923848628998 secs

Epoch 11 Batch 0 Loss 0.0007 Accuracy 0.4331
Epoch 11 Batch 50 Loss 0.0025 Accuracy 0.4653
Epoch 11 Batch 100 Loss 0.0024 Accuracy 0.4699
Epoch 11 Batch 150 Loss 0.0024 Accuracy 0.4715
Epoch 11 Batch 200 Loss 0.0022 Accuracy 0.4713
Epoch 11 Batch 250 Loss 0.0023 Accuracy 0.4725
Epoch 11 Batch 300 Loss 0.0023 Accuracy 0.4724
Epoch 11 Batch 350 Loss 0.0023 Accuracy 0.4721
Epoch 11 Batch 400 Loss 0.0022 Accuracy 0.4718
Epoch 11 Batch 450 Loss 0.0022 Accuracy 0.4718
Epoch 11 Batch 500 Loss 0.0022 Accuracy 0.4719
Epoch 11 Batch 550 Loss 0.0021 Accuracy 0.4719
Epoch 11 Batch 600 Loss 0.0021 Accuracy 0.4715
Epoch 11 Batch 650 Loss 0.0021 Accuracy 0.4710
Epoch 11 Batch 700 Loss 0.0022 Accuracy 0.4711
Epoch 11 Loss 0.0022 Accuracy 0.4711
Time taken for 1 epoch: 878.5343689918518 secs

Epoch 12 Batch 0 Loss 0.0007 Accuracy 0.4329
Epoch 12 Batch 50 Loss 0.0025 Accuracy 0.4751
Epoch 12 Batch 100 Loss 0.0027 Accuracy 0.4730
Epoch 12 Batch 150 Loss 0.0025 Accuracy 0.4719
Epoch 12 Batch 200 Loss 0.0023 Accuracy 0.4706
Epoch 12 Batch 250 Loss 0.0022 Accuracy 0.4717
Epoch 12 Batch 300 Loss 0.0023 Accuracy 0.4711
Epoch 12 Batch 350 Loss 0.0023 Accuracy 0.4705
Epoch 12 Batch 400 Loss 0.0022 Accuracy 0.4713
Epoch 12 Batch 450 Loss 0.0022 Accuracy 0.4710
Epoch 12 Batch 500 Loss 0.0022 Accuracy 0.4708
Epoch 12 Batch 550 Loss 0.0022 Accuracy 0.4709
Epoch 12 Batch 600 Loss 0.0022 Accuracy 0.4708
Epoch 12 Batch 650 Loss 0.0021 Accuracy 0.4704
Epoch 12 Batch 700 Loss 0.0022 Accuracy 0.4703
Epoch 12 Loss 0.0022 Accuracy 0.4703
Time taken for 1 epoch: 876.938019990921 secs

Epoch 13 Batch 0 Loss 0.0006 Accuracy 0.4633
Epoch 13 Batch 50 Loss 0.0013 Accuracy 0.4619
Epoch 13 Batch 200 Loss 0.0016 Accuracy 0.4688
Epoch 13 Batch 250 Loss 0.0017 Accuracy 0.4707
Epoch 13 Batch 300 Loss 0.0018 Accuracy 0.4710
Epoch 13 Batch 350 Loss 0.0017 Accuracy 0.4714
Epoch 13 Batch 400 Loss 0.0018 Accuracy 0.4710
Epoch 13 Batch 450 Loss 0.0017 Accuracy 0.4712
Epoch 13 Batch 500 Loss 0.0017 Accuracy 0.4714
Epoch 13 Batch 550 Loss 0.0017 Accuracy 0.4716
Epoch 13 Batch 600 Loss 0.0018 Accuracy 0.4713
Epoch 13 Batch 650 Loss 0.0017 Accuracy 0.4712
Epoch 13 Batch 700 Loss 0.0017 Accuracy 0.4712
Epoch 13 Loss 0.0017 Accuracy 0.4712
Time taken for 1 epoch: 859.017169713974 secs

Epoch 14 Batch 0 Loss 0.0024 Accuracy 0.4848
Epoch 14 Batch 50 Loss 0.0016 Accuracy 0.4745
Epoch 14 Batch 100 Loss 0.0014 Accuracy 0.4715
Epoch 14 Batch 150 Loss 0.0016 Accuracy 0.4719
Epoch 14 Batch 200 Loss 0.0017 Accuracy 0.4732
Epoch 14 Batch 250 Loss 0.0017 Accuracy 0.4718
Epoch 14 Batch 300 Loss 0.0018 Accuracy 0.4714
Epoch 14 Batch 350 Loss 0.0017 Accuracy 0.4710
Epoch 14 Batch 400 Loss 0.0018 Accuracy 0.4711
Epoch 14 Batch 450 Loss 0.0017 Accuracy 0.4701
Epoch 14 Batch 500 Loss 0.0018 Accuracy 0.4709
Epoch 14 Batch 550 Loss 0.0018 Accuracy 0.4711
Epoch 14 Batch 600 Loss 0.0018 Accuracy 0.4713
Epoch 14 Batch 650 Loss 0.0018 Accuracy 0.4717
Epoch 14 Batch 700 Loss 0.0018 Accuracy 0.4716
Epoch 14 Loss 0.0018 Accuracy 0.4716
Time taken for 1 epoch: 857.9521288871765 secs

Epoch 15 Batch 0 Loss 0.0056 Accuracy 0.4800
Epoch 15 Batch 50 Loss 0.0022 Accuracy 0.4709
Epoch 15 Batch 100 Loss 0.0019 Accuracy 0.4722
Epoch 15 Batch 150 Loss 0.0018 Accuracy 0.4707
Epoch 15 Batch 200 Loss 0.0018 Accuracy 0.4706
Epoch 15 Batch 250 Loss 0.0018 Accuracy 0.4715
Epoch 15 Batch 300 Loss 0.0017 Accuracy 0.4703
Epoch 15 Batch 350 Loss 0.0017 Accuracy 0.4699
Epoch 15 Batch 400 Loss 0.0017 Accuracy 0.4700
Epoch 15 Batch 450 Loss 0.0017 Accuracy 0.4707
Epoch 15 Batch 500 Loss 0.0017 Accuracy 0.4711
Epoch 15 Batch 550 Loss 0.0017 Accuracy 0.4699
Epoch 15 Batch 600 Loss 0.0016 Accuracy 0.4702
Epoch 15 Batch 650 Loss 0.0016 Accuracy 0.4706
Epoch 15 Batch 700 Loss 0.0016 Accuracy 0.4708
Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3
Epoch 15 Loss 0.0016 Accuracy 0.4707
Time taken for 1 epoch: 854.1302998065948 secs

Epoch 16 Batch 0 Loss 0.0003 Accuracy 0.4543
Epoch 16 Batch 50 Loss 0.0015 Accuracy 0.4704
Epoch 16 Batch 100 Loss 0.0013 Accuracy 0.4686
Epoch 16 Batch 150 Loss 0.0013 Accuracy 0.4687
Epoch 16 Batch 200 Loss 0.0014 Accuracy 0.4682
Epoch 16 Batch 250 Loss 0.0014 Accuracy 0.4698
Epoch 16 Batch 300 Loss 0.0014 Accuracy 0.4697
Epoch 16 Batch 350 Loss 0.0015 Accuracy 0.4707
Epoch 16 Batch 400 Loss 0.0015 Accuracy 0.4715
Epoch 16 Batch 450 Loss 0.0016 Accuracy 0.4712
Epoch 16 Batch 500 Loss 0.0016 Accuracy 0.4716
Epoch 16 Batch 550 Loss 0.0016 Accuracy 0.4715
Epoch 16 Batch 600 Loss 0.0016 Accuracy 0.4721
Epoch 16 Batch 650 Loss 0.0016 Accuracy 0.4717
Epoch 16 Batch 700 Loss 0.0015 Accuracy 0.4714
Epoch 16 Loss 0.0015 Accuracy 0.4714
Time taken for 1 epoch: 851.0064783096313 secs

Epoch 17 Batch 0 Loss 0.0003 Accuracy 0.4688
Epoch 17 Batch 50 Loss 0.0011 Accuracy 0.4727
Epoch 17 Batch 100 Loss 0.0014 Accuracy 0.4711
Epoch 17 Batch 150 Loss 0.0017 Accuracy 0.4718
Epoch 17 Batch 200 Loss 0.0017 Accuracy 0.4718
Epoch 17 Batch 250 Loss 0.0016 Accuracy 0.4724
Epoch 17 Batch 300 Loss 0.0016 Accuracy 0.4720
Epoch 17 Batch 350 Loss 0.0016 Accuracy 0.4707
Epoch 17 Batch 400 Loss 0.0015 Accuracy 0.4709
Epoch 17 Batch 450 Loss 0.0015 Accuracy 0.4717
Epoch 17 Batch 500 Loss 0.0014 Accuracy 0.4711
Epoch 17 Batch 550 Loss 0.0014 Accuracy 0.4714
Epoch 17 Batch 600 Loss 0.0014 Accuracy 0.4717
Epoch 17 Batch 650 Loss 0.0014 Accuracy 0.4723
Epoch 17 Batch 700 Loss 0.0014 Accuracy 0.4714
Epoch 17 Loss 0.0014 Accuracy 0.4715
Time taken for 1 epoch: 853.1788289546967 secs

Epoch 18 Batch 0 Loss 0.0006 Accuracy 0.4642
Epoch 18 Batch 50 Loss 0.0013 Accuracy 0.4678
Epoch 18 Batch 100 Loss 0.0012 Accuracy 0.4722
Epoch 18 Batch 150 Loss 0.0011 Accuracy 0.4726
Epoch 18 Batch 200 Loss 0.0010 Accuracy 0.4732
Epoch 18 Batch 250 Loss 0.0012 Accuracy 0.4719
Epoch 18 Batch 300 Loss 0.0012 Accuracy 0.4713
Epoch 18 Batch 350 Loss 0.0012 Accuracy 0.4721
Epoch 18 Batch 400 Loss 0.0012 Accuracy 0.4721
Epoch 18 Batch 450 Loss 0.0012 Accuracy 0.4715
Epoch 18 Batch 500 Loss 0.0012 Accuracy 0.4710
Epoch 18 Batch 550 Loss 0.0012 Accuracy 0.4704
Epoch 18 Batch 600 Loss 0.0013 Accuracy 0.4706
Epoch 18 Batch 650 Loss 0.0013 Accuracy 0.4710
Epoch 18 Batch 700 Loss 0.0013 Accuracy 0.4709
Epoch 18 Loss 0.0013 Accuracy 0.4710
Time taken for 1 epoch: 857.0118782520294 secs

Epoch 19 Batch 0 Loss 0.0002 Accuracy 0.4570
Epoch 19 Batch 50 Loss 0.0011 Accuracy 0.4778
Epoch 19 Batch 100 Loss 0.0016 Accuracy 0.4747
Epoch 19 Batch 150 Loss 0.0016 Accuracy 0.4735
Epoch 19 Batch 200 Loss 0.0016 Accuracy 0.4714
Epoch 19 Batch 250 Loss 0.0015 Accuracy 0.4710
Epoch 19 Batch 300 Loss 0.0015 Accuracy 0.4713
Epoch 19 Batch 350 Loss 0.0015 Accuracy 0.4711
Epoch 19 Batch 400 Loss 0.0015 Accuracy 0.4705
Epoch 19 Batch 450 Loss 0.0015 Accuracy 0.4702
Epoch 19 Batch 500 Loss 0.0014 Accuracy 0.4699
Epoch 19 Batch 550 Loss 0.0014 Accuracy 0.4699
Epoch 19 Batch 600 Loss 0.0014 Accuracy 0.4703
Epoch 19 Batch 650 Loss 0.0014 Accuracy 0.4708
Epoch 19 Batch 700 Loss 0.0014 Accuracy 0.4712
Epoch 19 Loss 0.0014 Accuracy 0.4712
Time taken for 1 epoch: 856.3480639457703 secs

Epoch 20 Batch 0 Loss 0.0008 Accuracy 0.4688
Epoch 20 Batch 50 Loss 0.0017 Accuracy 0.4751
Epoch 20 Batch 100 Loss 0.0011 Accuracy 0.4722
Epoch 20 Batch 150 Loss 0.0011 Accuracy 0.4716
Epoch 20 Batch 200 Loss 0.0012 Accuracy 0.4714
Epoch 20 Batch 250 Loss 0.0011 Accuracy 0.4724
Epoch 20 Batch 300 Loss 0.0011 Accuracy 0.4720
Epoch 20 Batch 350 Loss 0.0012 Accuracy 0.4723
Epoch 20 Batch 400 Loss 0.0012 Accuracy 0.4724
Epoch 20 Batch 450 Loss 0.0012 Accuracy 0.4718
Epoch 20 Batch 500 Loss 0.0012 Accuracy 0.4716
Epoch 20 Batch 550 Loss 0.0012 Accuracy 0.4711
Epoch 20 Batch 600 Loss 0.0012 Accuracy 0.4712
Epoch 20 Batch 650 Loss 0.0013 Accuracy 0.4712
Epoch 20 Batch 700 Loss 0.0013 Accuracy 0.4709
Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4
Epoch 20 Loss 0.0013 Accuracy 0.4709
Time taken for 1 epoch: 858.4996719360352 secs
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>You’ve come to the end of the graded portion of the assignment. By now, you’ve:</p>

<ul>
  <li>Create positional encodings to capture sequential relationships in data</li>
  <li>Calculate scaled dot-product self-attention with word embeddings</li>
  <li>Implement masked multi-head attention</li>
  <li>Build and train a Transformer model</li>
</ul>

<p><b>What you should remember</b>:</p>

<ul>
  <li>The combination of self-attention and convolutional network layers allows of parallization of training and <em>faster training</em>.</li>
  <li>Self-attention is calculated using the generated query Q, key K, and value V matrices.</li>
  <li>Adding positional encoding to word embeddings is an effective way of include sequence information in self-attention calculations.</li>
  <li>Multi-head attention can help detect multiple features in your sentence.</li>
  <li>Masking stops the model from ‘looking ahead’ during training, or weighting zeroes too much when processing cropped sentences.</li>
</ul>

<p>Now that you have completed the Transformer assignment, make sure you check out the ungraded labs to apply the Transformer model to practical use cases such as Name Entity Recogntion (NER) and Question Answering (QA).</p>

<h1 id="congratulations-on-finishing-the-deep-learning-specialization-">Congratulations on finishing the Deep Learning Specialization!!!!!! 🎉</h1>

<p>This was the last graded assignment of the specialization. It is now time to celebrate all your hard work and dedication!</p>

<p><a name="7"></a></p>
<h2 id="7---references">7 - References</h2>

<p>The Transformer algorithm was due to Vaswani et al. (2017).</p>

<ul>
  <li>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
</ul>

<p><strong>Training</strong>
This section describes the training regime for our models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Batch</span><span class="p">:</span>
    <span class="s">"Object for holding a batch of data with mask during training."</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">src</span> <span class="o">=</span> <span class="n">src</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">trg</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">trg_y</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">trg_mask</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="p">.</span><span class="n">make_std_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">trg</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">ntokens</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">trg_y</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
    
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">make_std_mask</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">pad</span><span class="p">):</span>
        <span class="s">"Create a mask to hide padding and future words."</span>
        <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">tgt_mask</span> <span class="o">&amp;</span> <span class="n">Variable</span><span class="p">(</span>
            <span class="n">subsequent_mask</span><span class="p">(</span><span class="n">tgt</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="n">type_as</span><span class="p">(</span><span class="n">tgt_mask</span><span class="p">.</span><span class="n">data</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tgt_mask</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_epoch</span><span class="p">(</span><span class="n">data_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_compute</span><span class="p">):</span>
    <span class="s">"Standard Training and Logging Function"</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_iter</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">src</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">trg</span><span class="p">,</span> 
                            <span class="n">batch</span><span class="p">.</span><span class="n">src_mask</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">trg_mask</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_compute</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">trg_y</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">ntokens</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">batch</span><span class="p">.</span><span class="n">ntokens</span>
        <span class="n">tokens</span> <span class="o">+=</span> <span class="n">batch</span><span class="p">.</span><span class="n">ntokens</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> <span class="o">%</span>
                    <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">batch</span><span class="p">.</span><span class="n">ntokens</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">))</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_tokens</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">global</span> <span class="n">max_src_in_batch</span><span class="p">,</span> <span class="n">max_tgt_in_batch</span>
<span class="k">def</span> <span class="nf">batch_size_fn</span><span class="p">(</span><span class="n">new</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">sofar</span><span class="p">):</span>
    <span class="s">"Keep augmenting batch and calculate total number of tokens + padding."</span>
    <span class="k">global</span> <span class="n">max_src_in_batch</span><span class="p">,</span> <span class="n">max_tgt_in_batch</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">max_src_in_batch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">max_tgt_in_batch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">max_src_in_batch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_src_in_batch</span><span class="p">,</span>  <span class="nb">len</span><span class="p">(</span><span class="n">new</span><span class="p">.</span><span class="n">src</span><span class="p">))</span>
    <span class="n">max_tgt_in_batch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_tgt_in_batch</span><span class="p">,</span>  <span class="nb">len</span><span class="p">(</span><span class="n">new</span><span class="p">.</span><span class="n">trg</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">src_elements</span> <span class="o">=</span> <span class="n">count</span> <span class="o">*</span> <span class="n">max_src_in_batch</span>
    <span class="n">tgt_elements</span> <span class="o">=</span> <span class="n">count</span> <span class="o">*</span> <span class="n">max_tgt_in_batch</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">src_elements</span><span class="p">,</span> <span class="n">tgt_elements</span><span class="p">)</span>
</code></pre></div></div>

</div>

<span class="post-tags">
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/AI"><nobr>AI</nobr></code>&nbsp;</a>    
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/Transformer"><nobr>Transformer</nobr></code>&nbsp;</a>    
    
</span>

<div class="recent">
  <h2>Recent Posts</h2>
  <ul class="recent-posts">
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo2.html">
            A Diffusion Model from Scratch in Pytorch
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo1.html">
            Diffusion Models Tutorial
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/02/Inspect-BERT-Vocabulary.html">
            Inspect BERT Vocabulary
          </a>
          <small>[02 Dec 2023]</small>
        </h4>
      </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
