<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Bayesian Learning &middot; Zhu Philip's AI Journey
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- merge something else -->
  
  <!-- merge something else 
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" /> -->
  
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>  
  
  <script defer src="/assets/js/lbox.js"></script>
   

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script> 

</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <link rel="stylesheet" href="/assets/style-3.css">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <div align="center">
          <img src="/assets/profile-pixel.png" class="profilepic pt-3 pb-2">
        </div>
        <!-- <a href="/"> -->
          Zhu Philip's AI Journey
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/working">Working</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <!-- <a class="sidebar-nav-item" href="/projects">Projects</a> -->
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">publications</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/working/">Working</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/zphilip/zphilip.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:zphilip48@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/tianda-zhu-37a5b031" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/zphilip" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/zphilip" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Bayesian Learning</h1>
  <span class="post-date">26 Jan 2023</span>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Auto-setup when running on Google Colab
</span><span class="kn">import</span> <span class="nn">os</span>
<span class="k">if</span> <span class="s">'google.colab'</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">())</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'/content/master'</span><span class="p">):</span>
    <span class="err">!</span><span class="n">git</span> <span class="n">clone</span> <span class="o">-</span><span class="n">q</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">ML</span><span class="o">-</span><span class="n">course</span><span class="o">/</span><span class="n">master</span><span class="p">.</span><span class="n">git</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">master</span>
    <span class="err">!</span><span class="n">pip</span> <span class="o">--</span><span class="n">quiet</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">requirements_colab</span><span class="p">.</span><span class="n">txt</span>
    <span class="o">%</span><span class="n">cd</span> <span class="n">master</span><span class="o">/</span><span class="n">notebooks</span>

<span class="c1"># Global imports and settings
</span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">preamble</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">interactive</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># Set to True for interactive plots
</span><span class="k">if</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">fig_scale</span> <span class="o">=</span> <span class="mf">0.61</span>
    <span class="n">print_config</span><span class="p">[</span><span class="s">'font.size'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="n">print_config</span><span class="p">[</span><span class="s">'xtick.labelsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">print_config</span><span class="p">[</span><span class="s">'ytick.labelsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">print_config</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span> <span class="c1"># For printing
</span>    <span class="n">fig_scale</span> <span class="o">=</span> <span class="mf">0.28</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">print_config</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="lecture-7-bayesian-learning">Lecture 7. Bayesian Learning</h1>

<p><strong>Learning in an uncertain world</strong></p>

<p>Joaquin Vanschoren</p>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/frequentists_vs_bayesians.png" alt="ml" style="width: 40%" />
XKCD, Randall Monroe</p>

<h2 id="bayes-rule">Bayes’ rule</h2>
<p>Rule for updating the probability of a hypothesis $c$ given data $x$</p>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/06_bayes_rule.png" alt="ml" style="width: 60%" /></p>

<p>$P(c|x)$ is the posterior probability of class $c$ given data $x$.<br />
$P(c)$ is the <em>prior</em> probability of class $c$: what you believed before you saw the data $x$<br />
$P(x|c)$ is the <em>likelihood</em> of data point $x$ given that the class is $c$ (computed from your data)<br />
$P(x)$ is the prior probability of the data (<em>marginal likelihood</em>): the likelihood of the data $x$ under any circumstance (no matter what the class is)</p>

<h3 id="example-exploding-sun">Example: exploding sun</h3>
<ul>
  <li>Let’s compute the probability that the sun has exploded</li>
  <li>Prior $P(exploded)$: the sun has an estimated lifespan of 10 billion years, 
$P(exploded) = \frac{1}{4.38 x 10^{13}}$</li>
  <li>Likelihood that detector lies: $P(lie)= \frac{1}{36}$</li>
</ul>

<p>\(\begin{aligned}
P(exploded|yes) &amp;= \frac{P(yes|exploded)P(exploded)}{P(yes)} \\
 &amp;= \frac{(1-P(lie)) P(exploded)}{P(exploded)(1-P(lie))+P(lie)(1-P(exploded))} \\
 &amp;= \frac{1}{1.25226 x 10^{12}}
 \end{aligned}\)</p>
<ul>
  <li>The one positive observation of the detector increases the probability</li>
</ul>

<h3 id="example-covid-test">Example: COVID test</h3>
<ul>
  <li>What is the probability of having COVID-19 if a 96% accurate test returns positive? Assume a false positive rate of 4%</li>
  <li>Prior $P(C): 0.015$ (117M cases, 7.9B people)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(TP)=P(pos</td>
          <td>C)=0.96$, and $P(FP)=(pos</td>
          <td>notC)=0.04$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>If test is positive, prior becomes $P(C)=0.268$. 2nd positive test: $P(C</td>
          <td>pos)=0.9$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

\[\begin{aligned}
P(C|pos) &amp;= \frac{P(pos|C)P(C)}{P(pos)} \\
 &amp;= \frac{P(pos|C) P(C)}{P(pos|C)P(C)+P(pos|notC)(1-P(C))} \\
 &amp;= \frac{0.96*0.015}{0.96*0.015+0.04*0.985} \\
 &amp;= 0.268
 \end{aligned}\]

<table>
  <tbody>
    <tr>
      <td>2023 update: With 760M cases, $P(C</td>
      <td>pos)=0.718$</td>
    </tr>
  </tbody>
</table>

<h2 id="bayesian-models">Bayesian models</h2>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Learn the joint distribution $P(x,y)=P(x</td>
          <td>y)P(y)$.</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>Assumes that the data is Gaussian distributed (!)</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>With every input $x$ you get $P(y</td>
              <td>x)$, hence a mean and standard deviation for $y$ (blue)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>For every desired output $y$ you get $P(x</td>
              <td>y)$, hence you can sample new points $x$ (red)</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Easily updatable with new data using Bayes’ rule (‘turning the crank’)
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>Previous posterior $P(y</td>
              <td>x)$ becomes new prior $P(y)$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">axes3d</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="n">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interact_manual</span>

<span class="k">def</span> <span class="nf">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'x'</span><span class="p">,</span><span class="s">'y'</span><span class="p">],</span> <span class="n">plot_intersect</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="n">xr</span><span class="p">,</span> <span class="n">yr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">delta</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xr</span><span class="p">,</span><span class="n">yr</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">flatten</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">Y</span><span class="p">.</span><span class="n">flatten</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">]))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">covariance_matrix</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ax</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">figaspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">fig_scale</span><span class="o">*</span><span class="mf">1.1</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">contour</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="o">*</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'jet'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'jet'</span><span class="p">)</span>

    <span class="n">cset</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Reds'</span><span class="p">)</span>
    <span class="n">cset</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">offset</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Blues'</span><span class="p">)</span>
    <span class="n">Zys</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="n">Zys</span><span class="p">[</span><span class="mi">60</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[</span><span class="nb">int</span><span class="p">((</span><span class="n">y</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="n">delta</span><span class="p">)]</span>
    <span class="n">cset</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Zys</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Reds'</span><span class="p">)</span>
    <span class="n">Zys</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="n">Zys</span><span class="p">[:,</span><span class="mi">60</span><span class="p">]</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[</span><span class="nb">int</span><span class="p">((</span><span class="n">x</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="n">delta</span><span class="p">)]</span>
    <span class="n">cset</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Zys</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">offset</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Blues'</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">plot_intersect</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xr</span><span class="p">),</span> <span class="n">yr</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'-'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xr</span><span class="p">,</span> <span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">yr</span><span class="p">),</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'-'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labelpad</span><span class="o">=-</span><span class="mi">2</span><span class="o">/</span><span class="n">fig_scale</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labelpad</span><span class="o">=-</span><span class="mi">2</span><span class="o">/</span><span class="n">fig_scale</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">'Probability density'</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=-</span><span class="mi">3</span><span class="o">/</span><span class="n">fig_scale</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="c1">#ax.tick_params(axis='both')
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">set_zticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="n">pad</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
    
<span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">interact_joint_distribution</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span><span class="n">y</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># Shape of the joint distribution
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.8</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]])</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">contour</span><span class="p">,</span> <span class="n">plot_intersect</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=0.0, description='x', max=3.0, min=-3.0, step=0.5), FloatSlider(value=…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.8</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
    
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">figaspect</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span><span class="o">*</span><span class="n">fig_scale</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">plot_intersect</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="n">x</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">plot_intersect</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_8_0.png" alt="png" /></p>

<h3 id="generative-models">Generative models</h3>
<ul>
  <li>The joint distribution represents the training data for a particular output (e.g. a class)</li>
  <li>You can sample a <em>new</em> point $\textbf{x}$ with high predicted likelihood $P(x,c)$: that new point will be very similar to the training points</li>
  <li>Generate new (likely) points according to the same distribution: <em>generative model</em>
    <ul>
      <li>Generate examples that are <em>fake</em> but corresponding to a desired output</li>
      <li>Generative neural networks (e.g. GANs) can do this very accurately for text, images, …</li>
    </ul>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/06_discriminative_generative.png" alt="ml" style="width: 40%" /></p>

<h3 id="naive-bayes">Naive Bayes</h3>

<ul>
  <li>Predict the probability that a point belongs to a certain class, using Bayes’ Theorem</li>
</ul>

\[P(c|\textbf{x}) = \frac{P(\textbf{x}|c)P(c)}{P(\textbf{x})}\]

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Problem: since $\textbf{x}$ is a vector, computing $P(\textbf{x}</td>
          <td>c)$ can be very complex</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Naively  assume that all features are conditionally independent from each other, in which case:<br />
$P(\mathbf{x}|c) = P(x_1|c) \times P(x_2|c) \times … \times P(x_n|c)$</li>
  <li>Very fast: only needs to extract statistics from each feature.</li>
</ul>

<h4 id="on-categorical-data">On categorical data</h4>

<p>What’s the probability that your friend will play golf if the weather is sunny?</p>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/06_bayes_example.png" alt="ml" style="width: 1200px;" /></p>

<h4 id="on-numeric-data">On numeric data</h4>
<ul>
  <li>We need to fit a distribution (e.g. Gaussian) over the data points</li>
  <li>
    <p>GaussianNB: Computes mean $\mu_c$ and standard deviation $\sigma_c$ of the feature values per class: $p(x=v \mid c)=\frac{1}{\sqrt{2\pi\sigma^2_c}}\,e^{ -\frac{(v-\mu_c)^2}{2\sigma^2_c} }$</p>
  </li>
  <li>We can now make predictions using Bayes’ theorem: $p(c \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid c) \ p(c)}{p(\mathbf{x})}$</li>
</ul>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/06_nb.png" alt="ml" style="width: 40%;" /></p>

<ul>
  <li>What do the predictions of Gaussian Naive Bayes look like?</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Naive Bayes"</span><span class="p">]</span>
<span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span><span class="n">GaussianNB</span><span class="p">()]</span>

<span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_classifiers</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">classifiers</span><span class="p">,</span> <span class="n">figuresize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_14_0.png" alt="png" /></p>

<p>Other Naive Bayes classifiers:</p>

<ul>
  <li>BernoulliNB
    <ul>
      <li>Assumes binary data</li>
      <li>Feature statistics: Number of non-zero entries per class</li>
    </ul>
  </li>
  <li>MultinomialNB
    <ul>
      <li>Assumes count data</li>
      <li>Feature statistics: Average value per class</li>
      <li>Mostly used for text classification (bag-of-words data)</li>
    </ul>
  </li>
</ul>

<h3 id="bayesian-networks">Bayesian Networks</h3>
<ul>
  <li>What if we know that some variables are not independent?</li>
  <li>A <em>Bayesian Network</em> is a directed acyclic graph representing variables as nodes and conditional dependencies as edges.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>If an edge $(A, B)$ connects random variables A and B, then $P(B</td>
          <td>A)$ is a factor in the joint probability distribution. We must know $P(B</td>
          <td>A)$ for all values of $B$ and $A$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>The graph structure can be designed manually or learned (hard!)</li>
</ul>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/06_bayesian_network.jpeg" alt="ml" style="width: 40%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### GP implementation (based on example by Neil Lawrence http://inverseprobability.com/mlai2015/)
</span>
<span class="c1"># Compute covariances
</span><span class="k">def</span> <span class="nf">compute_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X2</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">K</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X2</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">K</span>

<span class="c1"># Exponentiated quadratic kernel (RBF)
</span><span class="k">def</span> <span class="nf">exponentiated_quadratic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">):</span>
    <span class="n">squared_distance</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">x_prime</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">variance</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">squared_distance</span><span class="p">)</span><span class="o">/</span><span class="n">lengthscale</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GP</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">compute_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span> <span class="o">=</span> <span class="n">sigma2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_args</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">update_inverse</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">update_inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Precompute the inverse covariance and some quantities of interest
</span>        <span class="c1">## NOTE: Not the correct *numerical* way to compute this! For ease of use.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">Kinv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="c1"># the log determinant of the covariance matrix.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">logdetK</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">det</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="c1"># The matrix inner product of the inverse covariance
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">Kinvy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Kinv</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>  
        <span class="bp">self</span><span class="p">.</span><span class="n">yKinvy</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">Kinvy</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># use the pre-computes to return the likelihood
</span>        <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">logdetK</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">yKinvy</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># use the pre-computes to return the objective function 
</span>        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">()</span>  
    
    <span class="k">def</span> <span class="nf">posterior_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">K_star</span> <span class="o">=</span> <span class="n">compute_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_args</span><span class="p">)</span>
        <span class="n">K_starstar</span> <span class="o">=</span> <span class="n">compute_kernel</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_args</span><span class="p">)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Kinv</span><span class="p">,</span> <span class="n">K_star</span><span class="p">)</span>
        <span class="n">mu_f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">C_f</span> <span class="o">=</span> <span class="n">K_starstar</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">K_star</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu_f</span><span class="p">,</span> <span class="n">C_f</span>

<span class="c1"># set covariance function parameters
</span><span class="n">variance</span> <span class="o">=</span> <span class="mf">16.0</span>
<span class="n">lengthscale</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c1"># noise variance
</span>
<span class="c1"># Plotting how GPs sample
</span><span class="k">def</span> <span class="nf">plot_gp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">nr_points</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">add_mean</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">nr_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">show_covariance</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_stdev</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># Compute GP
</span>    <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">nr_points</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:</span><span class="n">nr_points</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GP</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">exponentiated_quadratic</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="n">variance</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">)</span>
    <span class="n">mu_f</span><span class="p">,</span> <span class="n">C_f</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">posterior_f</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">ys</span><span class="p">)</span>

    <span class="c1"># Plot GP with or without covariance matrix
</span>    <span class="k">if</span> <span class="n">show_covariance</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
        <span class="n">gp_axs</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
        <span class="n">gp_axs</span> <span class="o">=</span> <span class="n">axs</span>

    <span class="c1"># Plot GP
</span>    <span class="k">if</span> <span class="n">add_mean</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_f</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">C_f</span><span class="p">,</span> <span class="n">nr_samples</span><span class="p">).</span><span class="n">T</span><span class="p">;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">C_f</span><span class="p">,</span> <span class="n">nr_samples</span><span class="p">).</span><span class="n">T</span><span class="p">;</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">show_stdev</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">gp_axs</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">samples</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'-'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">);</span>
    <span class="n">gp_axs</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">mu_f</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'-'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">);</span> 
        
    <span class="n">gp_axs</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="s">'ro'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">)</span>
    <span class="n">gp_axs</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Gaussian Process"</span><span class="p">)</span>
    <span class="c1">#gp_axs.set_xlabel("x")
</span>    <span class="c1">#gp_axs.set_ylabel("y")
</span>    <span class="n">gp_axs</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
    <span class="n">gp_axs</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
    
    <span class="c1"># Plot Covariance matrix
</span>    <span class="k">if</span> <span class="n">show_covariance</span><span class="p">:</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">C_f</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Covariance matrix"</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">);</span>
    
    <span class="c1"># Stdev
</span>    <span class="k">if</span> <span class="n">show_stdev</span><span class="p">:</span>
        <span class="n">var_f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">C_f</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">std_f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_f</span><span class="p">)</span>
        <span class="n">gp_axs</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
             <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">mu_f</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">std_f</span><span class="p">,</span>
                            <span class="p">(</span><span class="n">mu_f</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">std_f</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
             <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'95% confidence interval'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="gaussian-processes">Gaussian processes</h3>
<ul>
  <li>Model the data as a Gaussian distribution, conditioned on the training points</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_sin</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
    <span class="n">X_sin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">Y_sin</span> <span class="o">=</span> <span class="n">X_sin</span><span class="o">/</span><span class="mi">100</span><span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X_sin</span><span class="o">/</span><span class="mi">5</span><span class="p">)</span><span class="o">*</span><span class="mi">3</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.5</span>
    <span class="n">X_sin_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> 
    <span class="n">plot_gp</span><span class="p">(</span><span class="n">X_sin</span><span class="p">,</span><span class="n">Y_sin</span><span class="p">,</span><span class="n">X_sin_test</span><span class="p">,</span><span class="n">lengthscale</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">nr_points</span><span class="o">=</span><span class="n">nr_points</span><span class="p">,</span> <span class="n">nr_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">show_covariance</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
    
<span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_sin</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(IntSlider(value=10, description='nr_points', max=20), Output()), _dom_classes=('widget-i…
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_19_1.png" alt="png" /></p>

<h2 id="probabilistic-interpretation-of-regression">Probabilistic interpretation of regression</h2>
<p>Linear regression (recap):</p>

\[y = f(\mathbf{x}_i) = \mathbf{x}_i\mathbf{w} + b\]

<p>For one input feature: \(y = w_1 \cdot x_1 + b \cdot 1\)</p>

<p>We can solve this via linear algebra (closed form solution): $w^{*} = (X^{T}X)^{-1} X^T Y$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<p>$\mathbf{X}$ is our data matrix with a $x_0=1$ column to represent the bias $b$:</p>

\[\mathbf{X} = \begin{bmatrix} 
\mathbf{x}_1^\top \\\ 
\mathbf{x}_2^\top \\\ 
\vdots \\\
\mathbf{x}_N^\top
\end{bmatrix} = \begin{bmatrix}
1 &amp; x_1 \\\
1 &amp; x_2 \\\
\vdots &amp; \vdots \\\
1 &amp; x_N 
\end{bmatrix}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pods</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pods</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">olympic_marathon_men</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'Y'</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">))</span> <span class="c1"># [ones(size(x)) x]
</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="c1">#print("X: ",X[:5])
#print("xTx: ",np.dot(X.T, X))
#print("xTy: ",np.dot(X.T, y))
</span>
</code></pre></div></div>

<h4 id="example-olympic-marathon-data">Example: Olympic marathon data</h4>
<p>We learned: $ y= w_1 x + w_0 = -0.013 x + 28.895$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1890</span><span class="p">,</span> <span class="mi">2020</span><span class="p">,</span> <span class="mi">130</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>

<span class="n">f_test</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x_test</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">f_test</span><span class="p">,</span> <span class="s">'b-'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'ro'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Linear regression (w: {})"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_23_0.png" alt="png" /></p>

<h3 id="polynomial-regression-recap">Polynomial regression (recap)</h3>
<p>We can fit a 2nd degree polynomial by using a basis expansion (adding more <em>basis functions</em>):</p>

\[\mathbf{\Phi} = \left[ \mathbf{1} \quad \mathbf{x} \quad \mathbf{x}^2\right]\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Phi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Phi</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="n">f_test</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">x_test</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x_test</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">f_test</span><span class="p">,</span> <span class="s">'b-'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'ro'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Polynomial regression (w: {})"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_25_0.png" alt="png" /></p>

<h3 id="kernelized-regression-recap">Kernelized regression (recap)</h3>
<p>We can also kernelize the model and learn a dual coefficient per data point</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>

<span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_kernel_ridge</span><span class="p">(</span><span class="n">poly_degree</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">poly_gamma</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">rbf_gamma</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">rbf_alpha</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pods</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">olympic_marathon_men</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'Y'</span><span class="p">]</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1890</span><span class="p">,</span> <span class="mi">2020</span><span class="p">,</span> <span class="mi">130</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">]</span>

    <span class="n">reg</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'poly'</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">poly_degree</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">poly_gamma</span><span class="p">)).</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"Kernel Ridge (Polynomial)"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>

    <span class="n">reg2</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">rbf_alpha</span><span class="p">),</span> <span class="n">gamma</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">rbf_gamma</span><span class="p">)).</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">reg2</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"Kernel Ridge (RBF)"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'g'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'ro'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Kernel Ridge"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"best"</span><span class="p">);</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_kernel_ridge</span><span class="p">(</span><span class="n">poly_degree</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">poly_gamma</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">rbf_gamma</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">rbf_alpha</span><span class="o">=-</span><span class="mi">8</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(IntSlider(value=4, description='poly_degree', max=8, min=1), IntSlider(value=-6, descrip…
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_27_1.png" alt="png" /></p>

<h3 id="probabilistic-interpretation">Probabilistic interpretation</h3>
<ul>
  <li>These models do not give us any indication of the (un)certainty of the predictions</li>
  <li>Assume that the data is inherently uncertain. This can be modeled explictly by introducing a <a href="http://en.wikipedia.org/wiki/Slack_variable">slack variable</a>, $\epsilon_i$, known as noise.</li>
</ul>

\[y_i = w_1 x_i + w_0 + \epsilon_i.\]

<ul>
  <li>Assume that the noise is distributed according to a Gaussian distribution with zero mean and variance $\sigma^2$.</li>
</ul>

\[\epsilon_i \sim \mathcal{N}(0, \sigma^2)\]

<ul>
  <li>That means that $y(x)$ is now a Gaussian distribution with mean $\mathbf{wx}$ and variance $\sigma^2$</li>
</ul>

\[y = \mathcal{N}(\mathbf{wx}, \sigma^2)\]

<p>We have an uncertainty prediction, but it is the same for all predictions</p>
<ul>
  <li>You would expect to be more certain nearby your training points</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">BayesianRidge</span>

<span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_regression</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.001</span><span class="p">)):</span> 
    <span class="n">data</span> <span class="o">=</span> <span class="n">pods</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">olympic_marathon_men</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'Y'</span><span class="p">]</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1890</span><span class="p">,</span> <span class="mi">2020</span><span class="p">,</span> <span class="mi">130</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
    
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">(</span><span class="n">alpha_1</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">sigma</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">)</span>
    <span class="c1"># We have the fitted sigma but we're ignoring it for now.
</span>    <span class="n">y_pred</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">ridge</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
         <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_pred</span> <span class="o">-</span> <span class="mf">1.9600</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1.9600</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
         <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'95% confidence interval'</span><span class="p">)</span>
        
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s">'b-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Bayesian Ridge'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=0.005, description='sigma', max=0.01, min=0.001, step=0.001), Output()…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_regression</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_31_0.png" alt="png" /></p>

<h2 id="how-to-learn-probabilities">How to learn probabilities?</h2>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Maximum Likelihood Estimation (MLE): Maximize $P(\textbf{X}</td>
          <td>\textbf{w})$</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>Corresponds to optimizing $\mathbf{w}$, using (log) likelihood as the loss function</li>
      <li>Every prediction has a mean defined by $\textbf{w}$ and Gaussian noise
   \(P(\textbf{X}|\textbf{w}) = \prod_{i=0}^{n} P(\mathbf{y}_i|\mathbf{x}_i;\mathbf{w}) = \prod_{i=0}^{n} \mathcal{N}(\mathbf{wx,\sigma^2 I})\)</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Maximum A Posteriori estimation (MAP): Maximize the posterior $P(\textbf{w}</td>
          <td>\textbf{X})$</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>This can be done using Bayes’ rule after we choose a (Gaussian) prior $P(\textbf{w})$:
  \(P(\textbf{w}|\textbf{X}) = \frac{P(\textbf{X}|\textbf{w})P(\textbf{w})}{P(\textbf{X})}\)</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Bayesian approach: model the prediction $P(y</td>
          <td>x_{test},X)$ directly</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>Marginalize $w$ out: consider all possible models (some are more likely)</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>If prior $P(\textbf{w})$ is Gaussian, then $P(y</td>
              <td>x_{test},\textbf{X})$ is also Gaussian!</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li>A multivariate Gaussian with mean $\mu$ and covariance matrix $\Sigma$
  \(P(y|x_{test},\textbf{X}) = \int_w P(y|x_{test},\textbf{w}) P(\textbf{w}|\textbf{X}) dw = \mathcal{N}(\mathbf{\mu,\Sigma})\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="gaussian-prior-pw">Gaussian prior $P(w)$</h3>
<p>In the Bayesian approach, we assume a <em>prior (Gaussian) distribution</em> for the parameters, $\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \alpha \mathbf{I})$:</p>
<ul>
  <li>With zero mean ($\mu$=0) and covariance matrix $\alpha \mathbf{I}$. For 2D: $ \alpha \mathbf{I} = \begin{bmatrix}
\alpha &amp; 0 <br />
0 &amp; \alpha
\end{bmatrix}$</li>
</ul>

<p>I.e, $w_i$ is drawn from a Gaussian density with variance $\alpha$
\(w_i \sim \mathcal{N}(0,\alpha)\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">interact_prior</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># Shape of the joint distribution
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">alpha</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="p">]])</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">contour</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'w1'</span><span class="p">,</span><span class="s">'w2'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=0.5, description='alpha', max=1.0, min=0.1), Checkbox(value=False, des…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">interact_prior</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_36_0.png" alt="png" /></p>

<h3 id="sampling-from-the-prior-weight-space">Sampling from the prior (weight space)</h3>

<p>We can sample from the prior distribution to see what form we are imposing on the functions <em>a priori</em> (before seeing any data).</p>

<ul>
  <li>Draw $w$ (left) independently from a Gaussian density $\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \alpha\mathbf{I})$
    <ul>
      <li>Use any normally distributed sampling technique, e.g. Box-Mueller transform</li>
    </ul>
  </li>
  <li>Every sample yields a polynomial function $f(\mathbf{x})$ (right): $f(\mathbf{x}) = \mathbf{w} \boldsymbol{\phi}(\mathbf{x}).$
    <ul>
      <li>For example, with $\boldsymbol{\phi}(\mathbf{x})$ being a polynomial:</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alpha</span> <span class="o">=</span> <span class="mf">4.</span> <span class="c1"># set prior variance on w
</span><span class="n">degree</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># set the order of the polynomial basis set
</span><span class="n">sigma2</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c1"># set the noise variance
</span><span class="n">num_pred_data</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># how many points to use for plotting predictions
</span><span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1880</span><span class="p">,</span> <span class="mi">2030</span><span class="p">,</span> <span class="n">num_pred_data</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="c1"># input locations for predictions
</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'jet'</span><span class="p">)</span>
    
<span class="c1"># Build the basis matrices (on Olympics data)
</span><span class="k">def</span> <span class="nf">polynomial</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">degrees</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">degree</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">loc</span><span class="p">)</span><span class="o">/</span><span class="n">scale</span><span class="p">)</span><span class="o">**</span><span class="n">degrees</span>

<span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_function_space</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span><span class="n">degree</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">random</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pods</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">olympic_marathon_men</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'Y'</span><span class="p">]</span>
    
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">scale</span>
    <span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1880</span><span class="p">,</span> <span class="mi">2030</span><span class="p">,</span> <span class="n">num_pred_data</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="c1"># input locations for predictions
</span>
    <span class="n">Phi_pred</span> <span class="o">=</span> <span class="n">polynomial</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="n">polynomial</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">degree</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
        <span class="n">z_vec</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">w_sample</span> <span class="o">=</span> <span class="n">z_vec</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">f_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi_pred</span><span class="p">,</span><span class="n">w_sample</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">scatter</span><span class="p">(</span><span class="n">w_sample</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w_sample</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">"w_1"</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">"w_2"</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">f_sample</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'-'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>

        
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X1</span><span class="p">.</span><span class="n">flatten</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">X2</span><span class="p">.</span><span class="n">flatten</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">]))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="p">[[</span><span class="n">alpha</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="p">]])</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X2</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=2.1, description='alpha', max=5.0, min=0.1, step=0.5), IntSlider(value…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_function_space</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_39_0.png" alt="png" /></p>

<h3 id="learning-gaussian-distributions">Learning Gaussian distributions</h3>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>We assume that our data is Gaussian distributed: $P(y</td>
          <td>x_{test},\textbf{X}) = \mathcal{N}(\mathbf{\mu,\Sigma})$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Example with learned mean $[m,m]$ and covariance
$\begin{bmatrix}
\alpha &amp; \beta <br />
\beta &amp; \alpha
\end{bmatrix}$
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>The blue curve is the predicted $P(y</td>
              <td>x_{test},\textbf{X})$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">interact_prior</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span><span class="n">m</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">beta</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># Shape of the joint distribution
</span>    <span class="n">mean_matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">],[</span><span class="n">beta</span><span class="p">,</span><span class="n">alpha</span><span class="p">]])</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean_matrix</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="n">x_test</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">contour</span><span class="p">,</span><span class="n">plot_intersect</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'x_test'</span><span class="p">,</span><span class="s">'y'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=0.0, description='x_test', max=3.0, min=-3.0, step=0.5), FloatSlider(v…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">figaspect</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span><span class="o">*</span><span class="n">fig_scale</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span>
    <span class="n">mean_matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">],[</span><span class="n">beta</span><span class="p">,</span><span class="n">alpha</span><span class="p">]])</span>
    <span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"m=0, alpha=1, beta=0"</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean_matrix</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'x_test'</span><span class="p">,</span><span class="s">'y'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
    
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.3</span>
    <span class="n">mean_matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">],[</span><span class="n">beta</span><span class="p">,</span><span class="n">alpha</span><span class="p">]])</span>
    <span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"m=1, alpha=0.5, beta=0.3"</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean_matrix</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'x_test'</span><span class="p">,</span><span class="s">'y'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_42_0.png" alt="png" /></p>

<h3 id="understanding-covariances">Understanding covariances</h3>
<ul>
  <li>If two variables $x_i$ covariate strongly, knowing about $x_1$ tells us a lot about $x_2$</li>
  <li>If covariance is 0, knowing $x_1$ tells us nothing about $x_2$ (the conditional and marginal distributions are the same)</li>
  <li>For covariance matrix
$\begin{bmatrix}
1 &amp; \beta <br />
\beta &amp; 1
\end{bmatrix}$:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">interact_covariance</span><span class="p">(</span><span class="n">x1</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span><span class="n">x2</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span><span class="n">beta</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)):</span>
    <span class="c1"># Shape of the joint distribution
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="n">beta</span><span class="p">],[</span><span class="n">beta</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">plot_intersect</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'x1'</span><span class="p">,</span><span class="s">'x2'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=0.0, description='x1', max=3.0, min=-3.0, step=0.5), FloatSlider(value…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">figaspect</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span><span class="o">*</span><span class="n">fig_scale</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"beta=0"</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">plot_intersect</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'x1'</span><span class="p">,</span><span class="s">'x2'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
 
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.9</span><span class="p">],[</span><span class="mf">0.9</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"beta=0.9"</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plot_joint_distribution</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">covariance_matrix</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">contour</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">plot_intersect</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'x1'</span><span class="p">,</span><span class="s">'x2'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_45_0.png" alt="png" /></p>

<h3 id="stochastic-processes--sampling-from-higher-dimensional-distributions">Stochastic processes &amp; sampling from higher-dimensional distributions</h3>

<ul>
  <li>Instead of sampling $\mathbf{w}$ and then multiplying by $\boldsymbol{\Phi}$, we can also generate examples of $f(x)$ directly.</li>
  <li>$\mathbf{f}$ with $n$ values can be sampled from an $n$-dimensional Gaussian distribution with zero mean and covariance matrix $\mathbf{K} = \alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top$:
    <ul>
      <li>$\mathbf{f}$ is a <em>stochastic process</em>: series of normally distributed variables (interpolated in the plot)</li>
    </ul>
  </li>
</ul>

\[\mathbf{f} \sim \mathcal{N}(\mathbf{0},\mathbf{K})\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">MaxNLocator</span>

<span class="c1"># Exponentiated quadratic is another name for RBF
</span><span class="k">def</span> <span class="nf">exponentiated_quadratic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">):</span>
    <span class="n">squared_distance</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">x_prime</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">variance</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">squared_distance</span><span class="p">)</span><span class="o">/</span><span class="n">lengthscale</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Compute covariances directly
</span><span class="k">def</span> <span class="nf">compute_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X2</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">K</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X2</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">K</span>

<span class="k">def</span> <span class="nf">plot_process</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="s">"Polynomial"</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">10.</span><span class="p">):</span>
    <span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="c1"># input locations for predictions
</span>    <span class="k">if</span> <span class="n">phi</span> <span class="o">==</span> <span class="s">"Identity"</span><span class="p">:</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">phi</span> <span class="o">==</span> <span class="s">"RBF"</span><span class="p">:</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">compute_kernel</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">exponentiated_quadratic</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="n">length_scale</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">phi</span> <span class="o">==</span> <span class="s">"Polynomial"</span><span class="p">:</span>
            <span class="n">degree</span> <span class="o">=</span> <span class="mi">5</span>  
        <span class="k">elif</span> <span class="n">phi</span> <span class="o">==</span> <span class="s">"Linear"</span><span class="p">:</span>
            <span class="n">degree</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">scale</span>
        <span class="n">Phi_pred</span> <span class="o">=</span> <span class="n">polynomial</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi_pred</span><span class="p">,</span> <span class="n">Phi_pred</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">noise</span><span class="p">:</span>
        <span class="n">K</span> <span class="o">+=</span> <span class="n">sigma2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">x_pred</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">dimensions</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
        <span class="n">ax_d</span><span class="p">,</span> <span class="n">ax_s</span><span class="p">,</span> <span class="n">ax_c</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
        <span class="n">ax_s</span><span class="p">,</span> <span class="n">ax_c</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">im</span> <span class="o">=</span> <span class="n">ax_c</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
    <span class="n">ax_c</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Covariance matrix"</span><span class="p">)</span>
    <span class="n">ax_c</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
    <span class="n">ax_c</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="n">ax_c</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="n">ax_c</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">);</span>
    
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">4</span><span class="p">).</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dimensions</span><span class="p">),</span> <span class="n">K</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">).</span><span class="n">T</span><span class="p">;</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">ax_s</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">samples</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s">'-'</span><span class="p">);</span>
    <span class="n">ax_s</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="n">ax_s</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Interpolated samples </span><span class="se">\n</span><span class="s"> from Gaussian"</span><span class="p">)</span>
    <span class="n">ax_s</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"X"</span><span class="p">)</span>
    <span class="n">ax_s</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
    <span class="n">ax_s</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">ax_s</span><span class="p">.</span><span class="n">get_data_ratio</span><span class="p">())</span>
    <span class="n">ax_s</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    
    <span class="k">if</span> <span class="n">dimensions</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>    
        <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
        <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X1</span><span class="p">.</span><span class="n">flatten</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">X2</span><span class="p">.</span><span class="n">flatten</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">]))</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X2</span><span class="p">))</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">ax_d</span><span class="p">.</span><span class="n">get_data_ratio</span><span class="p">())</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"x0"</span><span class="p">)</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"x1"</span><span class="p">)</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Samples from 2D Gaussian"</span><span class="p">)</span>
        <span class="n">ax_d</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    
<span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_process_noiseless</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">dimensions</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">phi</span><span class="o">=</span><span class="p">[</span><span class="s">"Identity"</span><span class="p">,</span><span class="s">"Linear"</span><span class="p">,</span><span class="s">"Polynomial"</span><span class="p">]):</span>
    <span class="n">plot_process</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="n">phi</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=1.0, description='alpha', max=2.0, min=0.1), IntSlider(value=51, descr…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_process</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="s">"Polynomial"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_48_0.png" alt="png" /></p>

<p>Repeat for 40 dimensions, with $\boldsymbol{\Phi}$ the polynomial transform:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_process</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="s">"Polynomial"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_50_0.png" alt="png" /></p>

<p><a href="https://pymc3-testing.readthedocs.io/en/rtd-docs/notebooks/GP-covariances.html">More examples of covariances</a></p>

<h4 id="noisy-functions">Noisy functions</h4>

<p>We normally add Gaussian noise to obtain our observations: 
\(\mathbf{y} = \mathbf{f} + \boldsymbol{\epsilon}\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_covm_noise</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)):</span>
    <span class="n">plot_process</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="s">"Polynomial"</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=0.05, description='sigma', max=0.1, min=0.01, step=0.01), Output()), _…
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_process</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="s">"Polynomial"</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_54_0.png" alt="png" /></p>

<h2 id="gaussian-process">Gaussian Process</h2>

<ul>
  <li>Usually, we want our functions to be <em>smooth</em>: if two points are similar/nearby, the predictions should be similar.
    <ul>
      <li>Hence, we need a similarity measure (a kernel)</li>
    </ul>
  </li>
  <li>In a Gaussian process we can do this by specifying the <em>covariance function</em> directly (not as $\mathbf{K} = \alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top$)
    <ul>
      <li>The covariance matrix is simply the kernel matrix: $\mathbf{f} \sim \mathcal{N}(\mathbf{0},\mathbf{K})$</li>
    </ul>
  </li>
  <li>The RBF (Gaussian) covariance function (or <em>kernel</em>) is specified by</li>
</ul>

\[k(\mathbf{x}, \mathbf{x}^\prime) = \alpha \exp\left( -\frac{\left\Vert \mathbf{x}-\mathbf{x}^\prime\right\Vert^2}{2\ell^2}\right).\]

<p>where $\left\Vert\mathbf{x} - \mathbf{x}^\prime\right\Vert^2$ is the squared distance between the two input vectors</p>

\[\left\Vert\mathbf{x} - \mathbf{x}^\prime\right\Vert^2 = (\mathbf{x} - \mathbf{x}^\prime)^\top (\mathbf{x} - \mathbf{x}^\prime)\]

<p>and the length parameter $l$ controls the smoothness of the function and $\alpha$ the vertical variation.</p>

<p>Now the influence of a point decreases smoothly but exponentially</p>
<ul>
  <li>These are our priors  $P(y) = \mathcal{N}(\mathbf{0,\mathbf{K}})$, with mean 0</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>We now want to condition it on our training data: $P(y</td>
          <td>x_{test},\textbf{X}) = \mathcal{N}(\mathbf{\mu,\Sigma})$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_gprocess</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">lengthscale</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">dimensions</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">nr_samples</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">5</span><span class="p">)):</span>
    <span class="n">plot_process</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="s">"RBF"</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">nr_samples</span><span class="p">)</span>
    
<span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_gprocess</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nr_samples</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(FloatSlider(value=1.0, description='alpha', max=2.0, min=0.1), IntSlider(value=10, descr…
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_57_1.png" alt="png" /></p>

<h3 id="computing-the-posterior-pmathbfymathbfx">Computing the posterior $P(\mathbf{y}|\mathbf{X})$</h3>

<ul>
  <li>
    <p>Assuming that $P(X)$ is a Gaussian density with a covariance given by kernel matrix $\mathbf{K}$, the model likelihood becomes:
\(P(\mathbf{y}|\mathbf{X}) = \frac{P(y) \ P(\mathbf{X} \mid y)}{P(\mathbf{X})} = \frac{1}{(2\pi)^{\frac{n}{2}}|\mathbf{K}|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}\mathbf{y}^\top \left(\mathbf{K}+\sigma^2 \mathbf{I}\right)^{-1}\mathbf{y}\right)\)</p>
  </li>
  <li>
    <p>Hence, the negative log likelihood (the objective function) is given by:
\(E(\boldsymbol{\theta}) = \frac{1}{2} \log |\mathbf{K}| + \frac{1}{2} \mathbf{y}^\top \left(\mathbf{K} + \sigma^2\mathbf{I}\right)^{-1}\mathbf{y}\)</p>
  </li>
  <li>
    <p>The model parameters (e.g. noise variance $\sigma^2$) and the kernel parameters (e.g. lengthscale, variance) can be embedded in the covariance function and learned from data.</p>
  </li>
  <li>Good news: This loss function can be optimized using linear algebra (Cholesky Decomposition)</li>
  <li>Bad news: This is cubic in the number of data points AND the number of features: $\mathcal{O}(n^3 d^3)$</li>
  <li><a href="http://inverseprobability.com/talks/notes/gaussian-processes.html">Read more on the derivation</a></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GP</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">compute_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span> <span class="o">=</span> <span class="n">sigma2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_args</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">update_inverse</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">update_inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Precompute the inverse covariance and some quantities of interest
</span>        <span class="c1">## NOTE: Not the correct *numerical* way to compute this! For ease of use.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">Kinv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="c1"># the log determinant of the covariance matrix.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">logdetK</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">det</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="c1"># The matrix inner product of the inverse covariance
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">Kinvy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Kinv</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>  
        <span class="bp">self</span><span class="p">.</span><span class="n">yKinvy</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">Kinvy</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>

        
    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># use the pre-computes to return the likelihood
</span>        <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">logdetK</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">yKinvy</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># use the pre-computes to return the objective function 
</span>        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">()</span>  
</code></pre></div></div>

<h3 id="making-predictions">Making predictions</h3>
<p>The model makes predictions for $\mathbf{f}$ that are unaffected by future values of $\mathbf{f}^<em>$.<br />
If we think of $\mathbf{f}^</em>$ as test points, we can still write down a joint probability density over the training observations, $\mathbf{f}$ and the test observations, $\mathbf{f}^*$.</p>

<p>This joint probability density will be Gaussian, with a covariance matrix given by our kernel function, $k(\mathbf{x}_i, \mathbf{x}_j)$. 
\(\begin{bmatrix}\mathbf{f} \\ \mathbf{f}^*\end{bmatrix} \sim \mathcal{N}\left(\mathbf{0}, \begin{bmatrix} \mathbf{K} &amp; \mathbf{K}_\ast \\ \mathbf{K}_\ast^\top &amp; \mathbf{K}_{\ast,\ast}\end{bmatrix}\right)\)</p>

<p>where $\mathbf{K}$ is the kernel matrix computed between all the training points,<br />
$\mathbf{K}<em>\ast$ is the kernel matrix computed between the training points and the test points,<br />
$\mathbf{K}</em>{\ast,\ast}$ is the kernel matrix computed between all the tests points and themselves.</p>

<p>Analogy</p>

<p>The joint probability of two variable $x_1$ and $x_2$ following a multivariate Gaussian is: 
\(\begin{bmatrix}x_1 \\ x_2 \end{bmatrix} \sim \mathcal{N}\left(\mathbf{0}, \begin{bmatrix} \mathbf{K} &amp; \mathbf{K}_\ast \\ \mathbf{K}_\ast^\top &amp; \mathbf{K}_{\ast,\ast}\end{bmatrix}\right)\)</p>

<h3 id="conditional-density-pmathbfyx_test--mathbfx">Conditional Density $P(\mathbf{y}|x_{test} , \mathbf{X})$</h3>

<p>Finally, we need to define <em>conditional</em> distributions to answer particular questions of interest</p>

<p>We will need the <em>conditional density</em> for making predictions.
\(\mathbf{f}^* | \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}_f,\mathbf{C}_f)\)
with a mean given by
$
\boldsymbol{\mu}<em>f = \mathbf{K}</em>*^\top \left[\mathbf{K} + \sigma^2 \mathbf{I}\right]^{-1} \mathbf{y}
$</p>

<p>and a covariance given by 
$
\mathbf{C}<em>f = \mathbf{K}</em>{<em>,</em>} - \mathbf{K}<em>*^\top \left[\mathbf{K} + \sigma^2 \mathbf{I}\right]^{-1} \mathbf{K}</em>\ast.
$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">GP</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">exponentiated_quadratic</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="n">variance</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">)</span>
<span class="n">mu_f</span><span class="p">,</span> <span class="n">C_f</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">posterior_f</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="gaussian-process-example">Gaussian process example</h3>
<p>We can now get the mean and covariance learned on our dataset, and sample from this distribution!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shuffled_olympics</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pods</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">olympic_marathon_men</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'Y'</span><span class="p">]</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">x_shuffle</span><span class="p">,</span> <span class="n">y_shuffle</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">perm</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1890</span><span class="p">,</span> <span class="mi">2020</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x_shuffle</span><span class="p">,</span> <span class="n">y_shuffle</span><span class="p">,</span> <span class="n">X_test</span>

<span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_gp_olympics</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xt</span> <span class="o">=</span> <span class="n">shuffled_olympics</span><span class="p">()</span>
    <span class="n">plot_gp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">nr_points</span><span class="o">=</span><span class="n">nr_points</span><span class="p">,</span> <span class="n">nr_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">show_covariance</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    
<span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_gp_olympics</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(IntSlider(value=13, description='nr_points', max=27), Output()), _dom_classes=('widget-i…
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_65_1.png" alt="png" /></p>

<p>Remember that our prediction is the sum of the mean and the variance: $P(\mathbf{y}|x_{test} , \mathbf{X}) = \mathcal{N}(\mathbf{\mu,\Sigma})$</p>
<ul>
  <li>The mean is the same as the one computed with kernel ridge (if given the same kernel and hyperparameters)</li>
  <li>The Gaussian process learned the covariance and the hyperparameters</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_gp_olympics_mean</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">add_mean</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xt</span> <span class="o">=</span> <span class="n">shuffled_olympics</span><span class="p">()</span>
    <span class="n">plot_gp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">nr_points</span><span class="o">=</span><span class="n">nr_points</span><span class="p">,</span> <span class="n">add_mean</span><span class="o">=</span><span class="n">add_mean</span><span class="p">,</span> <span class="n">nr_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">show_covariance</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    
<span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_gp_olympics_mean</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(IntSlider(value=13, description='nr_points', max=27), Checkbox(value=False, description=…
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_67_1.png" alt="png" /></p>

<p>The values on the diagonal of the covariance matrix give us the variance, so we can simply plot the mean and 95% confidence interval</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_gp_olympics_stdev</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lengthscale</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xt</span> <span class="o">=</span> <span class="n">shuffled_olympics</span><span class="p">()</span>
    <span class="n">plot_gp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span> <span class="n">nr_points</span><span class="o">=</span><span class="n">nr_points</span><span class="p">,</span> <span class="n">nr_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">show_covariance</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_stdev</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    
<span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_gp_olympics_stdev</span><span class="p">(</span><span class="n">nr_points</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>  <span class="n">lengthscale</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(IntSlider(value=13, description='nr_points', max=27), IntSlider(value=12, description='l…
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_69_1.png" alt="png" /></p>

<h3 id="gaussian-processes-in-practice-with-gpy">Gaussian Processes in practice (with GPy)</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">GPyRegression</code></li>
  <li>Generate a kernel first
    <ul>
      <li>State the dimensionality of your input data</li>
      <li>Variance and lengthscale are optional, default = 1
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kernel</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Other kernels:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">BasisFuncKernel</span><span class="err">?</span>
</code></pre></div>    </div>
  </li>
  <li>Build model:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">kernel</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">GPy</span>
<span class="n">custom_kernel</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">RBF</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">RatQuad</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">figure</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">7</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">kerns</span> <span class="o">=</span> <span class="p">[</span><span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">RBF</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Matern32</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
         <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Matern52</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Brownian</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">RatQuad</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
         <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">PeriodicExponential</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">White</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
         <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Poly</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">MLP</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">custom_kernel</span><span class="p">]</span>
 
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kerns</span><span class="p">,</span> <span class="n">axes</span><span class="p">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">k</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">a</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">name</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'_'</span><span class="p">,</span> <span class="s">' '</span><span class="p">))</span>
    <span class="n">a</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">)</span>
    <span class="n">a</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">""</span><span class="p">)</span>
    <span class="n">a</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">""</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_71_0.png" alt="png" /></p>

<p><code class="language-plaintext highlighter-rouge">Matern</code> is a generalized RBF kernel that can scale between RBF and Exponential. <code class="language-plaintext highlighter-rouge">Sum</code> is RBF + Linear * RatQuad</p>

<h4 id="effect-of-different-kernels">Effect of different kernels</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!pip install "numpy&lt;=1.23.5"
# Needed because of an unfixed deprecation bug in GPy
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">copy</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">Xr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Yr</span> <span class="o">=</span> <span class="n">Xr</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">Xr</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.05</span>

<span class="c1"># I need to make a deep copy since I will be training the kernels
</span><span class="n">kerneldict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">.</span><span class="n">name</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'_'</span><span class="p">,</span> <span class="s">' '</span><span class="p">)</span> <span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">kerns</span><span class="p">)}</span>
<span class="n">height_ratio</span><span class="o">=</span><span class="mi">1</span>

<span class="o">@</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_kernels</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kerneldict</span><span class="p">.</span><span class="n">keys</span><span class="p">()):</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">"lines.markersize"</span><span class="p">:</span> <span class="mi">4</span><span class="p">})</span>
    <span class="n">figure</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="o">*</span><span class="n">height_ratio</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'width_ratios'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]})</span>
    <span class="n">kerneldict</span><span class="p">[</span><span class="n">kernel</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">Xr</span><span class="p">,</span><span class="n">Yr</span><span class="p">,</span><span class="n">kerneldict</span><span class="p">[</span><span class="n">kernel</span><span class="p">])</span>
    <span class="n">m</span><span class="p">.</span><span class="n">optimize_restarts</span><span class="p">(</span><span class="n">num_restarts</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_xaxis</span><span class="p">().</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_yaxis</span><span class="p">().</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
    
<span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">height_ratio</span><span class="o">=</span><span class="mf">0.6</span>
    <span class="n">plot_kernels</span><span class="p">(</span><span class="s">"rbf"</span><span class="p">)</span>
    <span class="n">plot_kernels</span><span class="p">(</span><span class="s">"sum"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interactive(children=(Dropdown(description='kernel', options=('rbf', 'Exponential', 'Mat32', 'Mat52', 'Brownia…
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_75_1.png" alt="png" /></p>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_75_2.png" alt="png" /></p>

<p>Build the untrained GP. The shaded region corresponds to ~95% confidence intervals (i.e. +/- 2 standard deviation)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generate noisy sine data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.05</span>

<span class="c1"># Build untrained model
</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> 
<span class="n">m</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">kernel</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_77_0.png" alt="png" /></p>

<p>Train the model (optimize the kernel parameters): maximize the likelihood of the data.<br />
Best to optimize with a few restarts: the optimizer may converges to the high-noise solution. The optimizer is then restarted with a few random initializations of the parameter values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span><span class="p">.</span><span class="n">optimize_restarts</span><span class="p">(</span><span class="n">num_restarts</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_79_0.png" alt="png" /></p>

<p>You can also show results in 2D</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># sample inputs and outputs
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,(</span><span class="mi">50</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.05</span> 
<span class="n">ker</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Matern52</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">ARD</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">White</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># create simple GP model
</span><span class="n">m</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">ker</span><span class="p">)</span>

<span class="c1"># optimize and plot
</span><span class="n">m</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">max_f_eval</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_81_0.png" alt="png" /></p>

<p>We can plot 2D slices using the <code class="language-plaintext highlighter-rouge">fixed_inputs</code> argument to the plot function.<br />
<code class="language-plaintext highlighter-rouge">fixed_inputs</code> is a list of tuples containing which of the inputs to fix, and to which value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">slices</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="n">figure</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">plotting</span><span class="p">.</span><span class="n">plotting_library</span><span class="p">().</span><span class="n">figure</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">figure</span><span class="p">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">slices</span><span class="p">):</span>
    <span class="c1"># Use fixed_inputs=[(0,y)] for vertical slices
</span>    <span class="n">canvas</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figure</span><span class="o">=</span><span class="n">figure</span><span class="p">,</span> <span class="n">fixed_inputs</span><span class="o">=</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="n">y</span><span class="p">)],</span> <span class="n">row</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">plot_data</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_83_0.png" alt="png" /></p>

<h4 id="gaussian-processes-with-scikit-learn">Gaussian Processes with scikit-learn</h4>
<ul>
  <li><code class="language-plaintext highlighter-rouge">GaussianProcessRegressor</code></li>
  <li>Hyperparameters:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">kernel</code>: kernel specifying the covariance function of the GP
        <ul>
          <li>Default: “1.0 * RBF(1.0)”</li>
          <li>Typically leave at default. Will be optimized during fitting</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">alpha</code>: regularization parameter
        <ul>
          <li>Tikhonov regularization of covariance between the training points.</li>
          <li>Adds a (small) value to diagonal of the kernel matrix during fitting.</li>
          <li>Larger values:
            <ul>
              <li>correspond to increased noise level in the observations</li>
              <li>also reduce potential numerical issues during fitting</li>
            </ul>
          </li>
          <li>Default: 1e-10</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">n_restarts_optimizer</code>: number of restarts of the optimizer
        <ul>
          <li>Default: 0. Best to do at least a few iterations.</li>
          <li>Optimizer finds kernel parameters maximizing log-marginal likelihood</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Retrieve predictions and confidence interval after fitting:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>Example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">RBF</span><span class="p">,</span> <span class="n">ConstantKernel</span> <span class="k">as</span> <span class="n">C</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="s">"""The function to predict."""</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">atleast_2d</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]).</span><span class="n">T</span>

<span class="c1"># Observations
</span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Mesh the input space for evaluations of the real function, the prediction and
# its MSE
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)).</span><span class="n">T</span>

<span class="c1"># Instanciate a Gaussian Process model
</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">C</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">))</span> <span class="o">*</span> <span class="n">RBF</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">))</span>
<span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>

<span class="c1"># Fit to data using Maximum Likelihood Estimation of the parameters
</span><span class="n">gp</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Make the prediction on the meshed x-axis (ask for MSE as well)
</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Plot the function, the prediction and the 95% confidence interval based on
# the MSE
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'r:'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">u</span><span class="s">'$f(x) = x\,\sin(x)$'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'r.'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">u</span><span class="s">'Observations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s">'b-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">u</span><span class="s">'Prediction'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
         <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_pred</span> <span class="o">-</span> <span class="mf">1.9600</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1.9600</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
         <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'95% confidence interval'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'$x$'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'$f(x)$'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">);</span> 
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_86_0.png" alt="png" /></p>

<p>Example with noisy data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">9.9</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="n">T</span>

<span class="c1"># Observations and noise
</span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">dy</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
<span class="n">y</span> <span class="o">+=</span> <span class="n">noise</span>

<span class="c1"># Instanciate a Gaussian Process model
</span><span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="n">dy</span> <span class="o">/</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span>
                              <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Fit to data using Maximum Likelihood Estimation of the parameters
</span><span class="n">gp</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Make the prediction on the meshed x-axis (ask for MSE as well)
</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Plot the function, the prediction and the 95% confidence interval based on
# the MSE
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'r:'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">u</span><span class="s">'$f(x) = x\,\sin(x)$'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'r.'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">u</span><span class="s">'Observations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s">'b-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">u</span><span class="s">'Prediction'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
         <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_pred</span> <span class="o">-</span> <span class="mf">1.9600</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1.9600</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
         <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'95% confidence interval'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'$x$'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'$f(x)$'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span> 
</code></pre></div></div>

<p><img src="/assets/2023-06-01-Bayesian-Learning_files/2023-06-01-Bayesian-Learning_88_0.png" alt="png" /></p>

<h3 id="gaussian-processes-conclusions">Gaussian processes: Conclusions</h3>

<p>Advantages:</p>
<ul>
  <li>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals.</li>
  <li>The prediction interpolates the observations (at least for regular kernels).</li>
  <li>Versatile: different kernels can be specified.</li>
</ul>

<p>Disadvantages:</p>
<ul>
  <li>They are typically not sparse, i.e., they use the whole sample/feature information to perform the prediction.
    <ul>
      <li>Sparse GPs also exist: they remember only the most important points</li>
    </ul>
  </li>
  <li>They lose efficiency in high dimensional spaces – namely when the number of features exceeds a few dozens.</li>
</ul>

<h3 id="gaussian-processes-and-neural-networks">Gaussian processes and neural networks</h3>
<ul>
  <li>You can prove that a Gaussian process is equivalent to a neural network with one layer and an infinite number of nodes</li>
  <li>You can build <em>deep Gaussian Processes</em> by constructing layers of GPs</li>
</ul>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/06_deep_gps.png" alt="ml" style="width: 70%;" /></p>

<h2 id="bayesian-optimization">Bayesian optimization</h2>

<ul>
  <li>The incremental updates you can do with Bayesian models allow a more effective way to optimize functions
    <ul>
      <li>E.g. to optimize the hyperparameter settings of a machine learning algorithm/pipeline</li>
    </ul>
  </li>
  <li>After a number of random search iterations we know more about the performance of hyperparameter settings on the given dataset</li>
  <li>We can use this data to train a model, and predict which other hyperparameter values might be useful
    <ul>
      <li>More generally, this is called model-based optimization</li>
      <li>This model is called a <em>surrogate model</em></li>
    </ul>
  </li>
  <li>This is often a probabilistic (e.g. Bayesian) model that predicts confidence intervals for all hyperparameter settings</li>
  <li>We use the predictions of this model to choose the next point to evaluate</li>
  <li>With every new evaluation, we update the surrogate model and repeat</li>
</ul>

<h3 id="example-see-figure">Example (see figure):</h3>

<ul>
  <li>Consider only 1 continuous hyperparameter (X-axis)
    <ul>
      <li>You can also do this for many more hyperparameters</li>
    </ul>
  </li>
  <li>Y-axis shows cross-validation performance</li>
  <li>Evaluate a number of random hyperparameter settings (black dots)
    <ul>
      <li>Sometimes an initialization design is used</li>
    </ul>
  </li>
  <li>Train a model, and predict the expected performance of other (unseen) hyperparameter values
    <ul>
      <li>Mean value (black line) and distribution (blue band)</li>
    </ul>
  </li>
  <li>An <em>acquisition function</em> (green line) trades off maximal expected performace and maximal uncertainty
    <ul>
      <li>Exploitation vs exploration</li>
    </ul>
  </li>
  <li>Optimal value of the asquisition function is the next hyperparameter setting to be evaluated</li>
  <li>Repeat a fixed number of times, or until time budget runs out</li>
</ul>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/03_Bayesian_optimization.png" alt="ml" style="width: 800px;" /></p>

<p>Shahriari et al. Taking the Human Out of the Loop: A Review of Bayesian Optimization</p>

<p>In 2 dimensions:</p>

<p><img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/05_2dbo.png" alt="ml" style="width: 80%" /></p>

<h3 id="surrogate-models">Surrogate models</h3>

<ul>
  <li>Surrogate model can be anything as long as it can do regression and is probabilistic</li>
  <li>Gaussian Processes are commonly used
    <ul>
      <li>Smooth, good extrapolation, but don’t scale well to many hyperparameters (cubic)</li>
      <li>Sparse GPs: select ‘inducing points’ that minimize info loss, more scalable</li>
      <li>Multi-task GPs: transfer surrogate models from other tasks</li>
    </ul>
  </li>
  <li>Random Forests
    <ul>
      <li>A lot more scalable, but don’t extrapolate well</li>
      <li>Often an interpolation between predictions is used instead of the raw (step-wise) predictions</li>
    </ul>
  </li>
  <li>Bayesian Neural Networks:
    <ul>
      <li>Expensive, sensitive to hyperparameters</li>
    </ul>
  </li>
</ul>

<h3 id="acquisition-functions">Acquisition Functions</h3>

<ul>
  <li>When we have trained the surrogate model, we ask it to predict a number of samples
    <ul>
      <li>Can be simply random sampling</li>
      <li>Better: <em>Thompson sampling</em>
        <ul>
          <li>fit a Gaussian distribution (a mixture of Gaussians) over the sampled points</li>
          <li>sample new points close to the means of the fitted Gaussians</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Typical acquisition function: <em>Expected Improvement</em>
    <ul>
      <li>Models the predicted performance as a Gaussian distribution with the predicted mean and standard deviation</li>
      <li>Computes the <em>expected</em> performance improvement over the previous best configuration $\mathbf{X^+}$:
\(EI(X) := \mathbb{E}\left[ \max\{0, f(\mathbf{X^+}) - f_{t+1}(\mathbf{X}) \} \right]\)</li>
      <li>Computing the expected performance requires an integration over the posterior distribution, but has a <a href="http://ash-aldujaili.github.io/blog/2018/02/01/ei/">closed form solution</a>.</li>
    </ul>
  </li>
</ul>

<h3 id="bayesian-optimization-conclusions">Bayesian Optimization: conclusions</h3>

<ul>
  <li>More efficient way to optimize hyperparameters</li>
  <li>More similar to what humans would do</li>
  <li>Harder to parallellize</li>
  <li>Choice of surrogate model depends on your search space
    <ul>
      <li>Very active research area</li>
      <li>For very high-dimensional search spaces, random forests are popular</li>
    </ul>
  </li>
</ul>

</div>

<span class="post-tags">
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/AI"><nobr>AI</nobr></code>&nbsp;</a>    
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/Bayesian"><nobr>Bayesian</nobr></code>&nbsp;</a>    
    
</span>

<div class="recent">
  <h2>Recent Posts</h2>
  <ul class="recent-posts">
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo2.html">
            A Diffusion Model from Scratch in Pytorch
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo1.html">
            Diffusion Models Tutorial
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/02/Inspect-BERT-Vocabulary.html">
            Inspect BERT Vocabulary
          </a>
          <small>[02 Dec 2023]</small>
        </h4>
      </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
