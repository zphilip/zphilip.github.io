<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Gaussian Mixture Model Clearly Explained &middot; Zhu Philip's AI Journey
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- merge something else -->
  
  <!-- merge something else 
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" /> -->
  
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>  
  
  <script defer src="/assets/js/lbox.js"></script>
   

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script> 

</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <link rel="stylesheet" href="/assets/style-3.css">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <div align="center">
          <img src="/assets/profile-pixel.png" class="profilepic pt-3 pb-2">
        </div>
        <!-- <a href="/"> -->
          Zhu Philip's AI Journey
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/working">Working</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <!-- <a class="sidebar-nav-item" href="/projects">Projects</a> -->
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">publications</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/working/">Working</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/zphilip/zphilip.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:zphilip48@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/tianda-zhu-37a5b031" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/zphilip" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/zphilip" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Gaussian Mixture Model Clearly Explained</h1>
  <span class="post-date">26 Jan 2023</span>
  <h1 id="gaussian-mixture-model-clearly-explained">Gaussian Mixture Model Clearly Explained</h1>
<h3 id="the-only-guide-you-need-to-learn-everything-about-gmm"><em>The only guide you need to learn everything about GMM</em></h3>

<p>When we talk about Gaussian Mixture Model (later, this will be denoted as GMM in this article), it’s essential to know how the KMeans algorithm works. Because GMM is quite similar to the KMeans, more likely it’s a probabilistic version of KMeans. This probabilistic feature allows GMM to be applied to many complex problems that KMeans can’t fit into.</p>

<p>In summary, KMeans have below limitations,</p>

<ol>
  <li>It assumed that the clusters were spherical and equally sized, which is not valid in most real-world scenarios.</li>
  <li>It’s a hard clustering method. Meaning each data point is assigned to a single cluster.</li>
</ol>

<p>Due to these limitations, we should know alternatives for KMeans when working on our machine learning projects. In this article, we will explore one of the best alternatives for KMeans clustering, called the Gaussian Mixture Model.</p>

<p>Throughout this article, we will be covering the below points.</p>

<ol>
  <li>How Gaussian Mixture Model (GMM) algorithm works — in plain English.</li>
  <li>Mathematics behind GMM.</li>
  <li>Implement GMM using Python from scratch.</li>
</ol>

<h2 id="how-gaussian-mixture-model-gmm-algorithm-works--in-plain-english">How Gaussian Mixture Model (GMM) algorithm works — in plain English</h2>

<p>How Gaussian Mixture Model (GMM) algorithm works — in plain English
As I have mentioned earlier, we can call GMM probabilistic KMeans because the starting point and training process of the KMeans and GMM are the same. However, KMeans uses a distance-based approach, and GMM uses a probabilistic approach. There is one primary assumption in GMM: the dataset consists of multiple Gaussians, in other words, a mixture of the gaussian.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#import requred libraries
</span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">warnings</span><span class="p">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generate some data with multiple modes
</span><span class="n">data1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Plot the data using seaborn's distplot function
</span><span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">hist</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'alpha'</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
<span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">hist</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'alpha'</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>

<span class="c1"># Add a legend
</span><span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Data 1'</span><span class="p">,</span> <span class="s">'Data 2'</span><span class="p">])</span>

<span class="c1"># Show the plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/2023-08-01-GMM-from-scratch_6_0.png" alt="png" /></p>

<p>The above kind of distribution is often called multi-model distribution. Each peak represents the different gaussian distribution or the cluster in our dataset. But the question is,</p>

<h4 id="how-do-we-estimate-these-distributions"><em>how do we estimate these distributions?</em></h4>

<p>Before answering this question, let’s create some gaussian distribution first. Please note here I am generating multivariate normal distribution; it’s a higher dimensional extension of the univariate normal distribution.</p>

<p>Let’s define the mean and covariance of our data points. Using mean and covariance, we can generate the distribution as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Set the mean and covariance
</span><span class="n">mean1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">mean2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cov1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">7</span><span class="p">],</span> <span class="p">[.</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">cov2</span> <span class="o">=</span> <span class="p">[[.</span><span class="mi">5</span><span class="p">,</span> <span class="p">.</span><span class="mi">4</span><span class="p">],</span> <span class="p">[.</span><span class="mi">4</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">]]</span>

<span class="c1"># Generate data from the mean and covariance
</span><span class="n">data1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">cov1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean2</span><span class="p">,</span> <span class="n">cov2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data1</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">data1</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">data2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

<span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">data1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">data2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/2023-08-01-GMM-from-scratch_9_0.png" alt="png" /></p>

<p>As you can see here, we generated random gaussian distribution using mean and covariance matrices. What about reversing this process? That’s what exactly GMM is doing. But how?</p>

<p><em>Because, in the beginning, we didn’t have any insights about clusters nor their associated mean and covariance matrices</em></p>

<p>Well, It happens according to the below steps,</p>

<ol>
  <li>Decide the number of clusters (to decide this, we can use domain knowledge or other methods such as BIC/AIC) for the given dataset. Assume that we have 1000 data points, and we set the number of groups as 2.</li>
  <li>Initiate mean, covariance, and weight parameter per cluster. (we will explore more about this in a later section)</li>
  <li>Use the Expectation Maximization algorithm to do the following,
    <ul>
      <li>Expectation Step (E step): Calculate the probability of each data point belonging to each data point, then evaluate the likelihood function using the current estimate for the parameters</li>
      <li>Maximization step (M step): Update the previous mean, covariance, and weight parameters to maximize the expected likelihood found in the E step</li>
      <li>Repeat these steps until the model converges.</li>
    </ul>
  </li>
</ol>

<p>With this information, I am concluding the no-math explanation of the GMM algorithm.</p>

<h2 id="mathematics-behind-gmm">Mathematics behind GMM</h2>

<p>The core of GMM lies within Expectation Maximization(EM) algorithm described in the previous section.</p>

<p>Let’s demonstrate the EM algorithm in the sense of GMM.</p>

<p><strong>Step 01: Initialize mean, covariance and weight parameters</strong></p>

<ol>
  <li>mean (μ): initialize randomly.</li>
  <li>covariance (Σ): initialize randomly</li>
  <li>weight (mixing coefficients) (π): fraction per class refers to the likelihood that a particular data point belongs to each class. In the beginning, this will be equal for all clusters. Assume that we fit a GMM with three components. In this case weight parameter might be set to 1/3 for each component, resulting in a probability distribution of (1/3, 1/3, 1/3).</li>
</ol>

<p><strong>Step 02: Expectation Step (E step)</strong></p>

<p>For each data point 𝑥𝑖:
Calculate the probability that the data point belongs to cluster (𝑐) using the below equation. k is the number of distributions we are supposed to find.</p>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/2121608b-a16d-4642-b6bc-428e396dc127.png" alt="eq1.png" /></p>

<table>
  <tbody>
    <tr>
      <td>Where 𝜋_𝑐 is the mixing coefficient (sometimes called weight) for the Gaussian distribution c, which was initialized in the previous stage, and 𝑁(𝒙</td>
      <td>𝝁,𝚺) describes the probability density function (PDF) of a Gaussian distribution with mean 𝜇 and covariance Σ with respect to data point x; We can denote it as below.</td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/49882d87-90fc-4d52-a140-5dd179d1e103.png" alt="eq2.png" /></p>

<p>The E-step computes these probabilities using the current estimates of the model’s parameters. These probabilities are typically referred to as the “responsibilities” of the Gaussian distributions. They are represented by the variables r_ic, where i is the index of the data point, and c is the index of the Gaussian distribution. The responsibility measures how much the c-th Gaussian distribution is responsible for generating the i-th data point. Conditional probability is used here, more specifically, Bayes theorem.</p>

<p>Let’s take a simple example. Assume we have 100 data points and need to cluster them into two groups. We can write r_ic(i=20,c=1) as follows. Where i represents the data point’s index, and c represents the index of the cluster we are considering.</p>

<p>Please note at the beginning, 𝜋_𝑐 initialized to equal for each cluster c = 1,2,3,..,k. In our case, 𝜋_1 = 𝜋_2 = 1/2.</p>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/4b033967-6cc4-46ba-8588-13f3c72524d2.png" alt="image.png" /></p>

<p>The result of the E-step is a set of responsibilities for each data point and each Gaussian distribution in the mixture model. These responsibilities are used in the M-step to update the estimates of the model’s parameters.</p>

<p><strong>Step 03: Maximization Step (M step)</strong></p>

<p>In this step, the algorithm uses the responsibilities of the Gaussian distributions (computed in the E-step) to update the estimates of the model’s parameters.</p>

<p>The M-step updates the estimates of the parameters as follows:</p>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/b602fa40-cd3a-490e-a9fd-ea4897aca947.png" alt="equatios3.png" /></p>

<ol>
  <li>Update the πc (mixing coefficients) using equation 4 above.</li>
  <li>
    <p>Update the μc using equation number 5 above.</p>
  </li>
  <li>Then update the Σc using the 6th equation.</li>
</ol>

<p>Additional Fact:</p>

<p><em>πc can be considered equivalent to the fraction of points allocated to 𝑐 because numerator Σ_𝑖 *𝑟_𝑖𝑐 represents the likelihood of the data point belonging to the gaussian c. If we assume we have 3 clusters and 𝑖-th data point belongs to cluster 1, we can write the related vector as [0.97,0.02,0.01]. If we sum these vectors for each data point, the result vector is approximately equal to the number of data points per cluster.</em></p>

<p>This updated estimate is used in the next E-step to compute new responsibilities for the data points.</p>

<p>So on and so forth, this process will repeat until algorithm convergence, typically achieved when the model parameters do not change significantly from one iteration to the next.</p>

<p>Lots of ugly and complex equations, right? :)</p>

<h3 id="lets-summarize-the-above-facts-into-one-simple-diagram"><em>Let’s summarize the above facts into one simple diagram,</em></h3>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/055c41a6-51f5-4d74-891a-578eca27f4b6.png" alt="summary.png" /></p>

<p>Don’t worry; when it comes to coding, it will be one line per each equation. Let’s start to implement GMM from scratch using Python.</p>

<h2 id="implement-gmm-using-python-from-scratch">Implement GMM using Python from scratch.</h2>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/d256a37b-40da-44e0-86e6-4fa116c3d8a6.gif" alt="animated_GMM new.gif" /></p>

<p>First thing first, let’s create a fake dataset. In this section, I will implement GMM for the 1-D dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.2</span> 
<span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">1.8</span> 
<span class="n">mu3</span><span class="p">,</span> <span class="n">sigma3</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.6</span> 

<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma1</span><span class="p">),</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">),</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">mu3</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma3</span><span class="p">),</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_pdf</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">'k--'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">):</span>
    <span class="s">"""
    Plot 1-D data and its PDF curve.

    Parameters
    ----------
    X : array-like, shape (n_samples,)
        The input data.
    """</span>
    <span class="c1"># Compute the mean and standard deviation of the data
</span>
    <span class="c1"># Plot the data
</span>    
    <span class="n">X</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="n">density</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

    <span class="c1"># Plot the PDF
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">X</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">linestyle</span><span class="p">)</span>

</code></pre></div></div>

<p>And plot the generated data as follows. Please note that instead of plotting the data itself, I have plotted the probability density of each sample.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_pdf</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span><span class="n">sigma1</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">"$\mu={} \ ; \ \sigma={}$"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span><span class="n">sigma1</span><span class="p">),</span><span class="n">color</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">plot_pdf</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span><span class="n">sigma2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">"$\mu={} \ ; \ \sigma={}$"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span><span class="n">sigma2</span><span class="p">),</span><span class="n">color</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">plot_pdf</span><span class="p">(</span><span class="n">mu3</span><span class="p">,</span><span class="n">sigma3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">"$\mu={} \ ; \ \sigma={}$"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu3</span><span class="p">,</span><span class="n">sigma3</span><span class="p">),</span><span class="n">color</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Original Distribution"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/2023-08-01-GMM-from-scratch_39_0.png" alt="png" /></p>

<p>Let’s build each step described in the previous section,</p>

<p><strong>Step 01: Initialize mean, covariance, and weights</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_init</span><span class="p">(</span><span class="n">n_compenents</span><span class="p">):</span>
    
    <span class="s">"""Initialize means, weights and variance randomly"""</span>
    
    <span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_compenents</span><span class="p">))</span> <span class="o">/</span> <span class="n">n_compenents</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_compenents</span><span class="p">)</span>
    <span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random_sample</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_compenents</span><span class="p">)</span>
    <span class="n">plot_pdf</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">variances</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s">'Random Init 01'</span><span class="p">,)</span>
    <span class="n">plot_pdf</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">variances</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s">'Random Init 02'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">)</span>
    <span class="n">plot_pdf</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">variances</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="s">'Random Init 03'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">'orange'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Random Initialization"</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">,</span><span class="n">pi</span>
</code></pre></div></div>

<p><strong>Step 02: Expectation Step (E step)</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">step_expectation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">n_components</span><span class="p">,</span><span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">):</span>
    <span class="s">"""E Step
    
    Parameters
    ----------
    X : array-like, shape (n_samples,)
        The data.
    n_components : int
        The number of clusters
    means : array-like, shape (n_components,)
        The means of each mixture component.
    variances : array-like, shape (n_components,)
        The variances of each mixture component.
        
    Returns
    -------
    weights : array-like, shape (n_components,n_samples)
    """</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_components</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">means</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variances</span><span class="p">[</span><span class="n">j</span><span class="p">])).</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span>
</code></pre></div></div>

<p>After this function, we covered the first two equations we discussed in E Step. Here we have generated the gaussian distribution for the current model parameter means and variances. We accomplished that by using the scipy’s stat module. After, we used the pdf method to calculate the likelihood of belonging to each data point for each cluster.</p>

<p><strong>Step 03: Maximization Step (M step)</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">step_maximization</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">,</span><span class="n">n_compenents</span><span class="p">,</span><span class="n">pi</span><span class="p">):</span>
    <span class="s">"""M Step
    
    Parameters
    ----------
    X : array-like, shape (n_samples,)
        The data.
    weights : array-like, shape (n_components,n_samples)
        initilized weights array
    means : array-like, shape (n_components,)
        The means of each mixture component.
    variances : array-like, shape (n_components,)
        The variances of each mixture component.
    n_components : int
        The number of clusters
    pi: array-like (n_components,)
        mixture component weights
        
    Returns
    -------
    means : array-like, shape (n_components,)
        The means of each mixture component.
    variances : array-like, shape (n_components,)
        The variances of each mixture component.
    """</span>
    <span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_compenents</span><span class="p">):</span>  
        <span class="n">r</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">pi</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">pi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_compenents</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

        <span class="n">means</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
        <span class="n">variances</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">means</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
    
        <span class="n">pi</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">variances</span><span class="p">,</span><span class="n">means</span><span class="p">,</span><span class="n">pi</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_intermediate_steps</span><span class="p">(</span><span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">save</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">file_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    
    <span class="n">plot_pdf</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span><span class="n">sigma1</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">'r--'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">'Original Distibutions'</span><span class="p">)</span>
    <span class="n">plot_pdf</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span><span class="n">sigma2</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">'r--'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">'Original Distibutions'</span><span class="p">)</span>
    <span class="n">plot_pdf</span><span class="p">(</span><span class="n">mu3</span><span class="p">,</span><span class="n">sigma3</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s">'r--'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">'Original Distibutions'</span><span class="p">)</span>
    
    <span class="n">color_gen</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'green'</span><span class="p">,</span><span class="s">'blue'</span><span class="p">,</span><span class="s">'orange'</span><span class="p">])</span>
    
    <span class="k">for</span> <span class="n">mu</span><span class="p">,</span><span class="n">sigma</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">):</span>
        <span class="n">plot_pdf</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">'d'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">color_gen</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">save</span> <span class="ow">or</span> <span class="n">file_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">file_name</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">"_"</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s">"steps/</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s">.png"</span><span class="p">,</span><span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Let’s implement the training loop.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">n_compenents</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">plot_intermediate_steps_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">""" Training step of the GMM model
    
    Parameters
    ----------
    data : array-like, shape (n_samples,)
        The data.
    n_components : int
        The number of clusters
    n_steps: int
        number of iterations to run
    """</span>
    
    
    <span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">,</span><span class="n">pi</span> <span class="o">=</span> <span class="n">random_init</span><span class="p">(</span><span class="n">n_compenents</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">step_expectation</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">n_compenents</span><span class="p">,</span><span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">)</span>
        <span class="n">variances</span><span class="p">,</span><span class="n">means</span><span class="p">,</span><span class="n">pi</span> <span class="o">=</span> <span class="n">step_maximization</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">n_compenents</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">plot_intermediate_steps_flag</span><span class="p">:</span><span class="n">plot_intermediate_steps</span><span class="p">(</span><span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">,)</span><span class="c1">#file_name=f'step_{step+1}')
</span>    <span class="n">plot_intermediate_steps</span><span class="p">(</span><span class="n">means</span><span class="p">,</span><span class="n">variances</span><span class="p">)</span>
</code></pre></div></div>

<p>When we start the model training, we will do E and M steps according to the n_steps parameter we set.</p>

<p>But in the actual use cases, you will use the scikit-learn version of the GMM more often. There you can find additional parameters, such as</p>

<p>tol: defining the model’s stop criteria. EM iterations will stop when the lower bound average gain is below the tol parameter.</p>

<p>init_params: The method used to initialize the weights</p>

<p>You may refer to the documentation <a href="https://scikitlearn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html.">here</a></p>

<p>Alright, let’s see how our handcrafted GMM performs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train_gmm(X,n_steps=30,plot_intermediate_steps_flag=True)
</span></code></pre></div></div>

<p><img src="/assets/2023-08-01-GMM-from-scratch_files/c9dd115b-7dca-4cc0-8afa-78f94f5e613c.png" alt="model traininng.png" /></p>

<p>In the above diagrams, red dashed lines represent the original distribution, while other graphs represent the learned distributions. After the 30th iteration, we can see that our model performed well on this toy dataset.</p>

</div>

<span class="post-tags">
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/AI"><nobr>AI</nobr></code>&nbsp;</a>    
    
</span>

<div class="recent">
  <h2>Recent Posts</h2>
  <ul class="recent-posts">
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo2.html">
            A Diffusion Model from Scratch in Pytorch
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/02/Inspect-BERT-Vocabulary.html">
            Inspect BERT Vocabulary
          </a>
          <small>[02 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/working/2023/12/01/My-Tasks-and-Notes.html">
            working todo
          </a>
          <small>[01 Dec 2023]</small>
        </h4>
      </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
