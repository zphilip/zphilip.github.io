<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Bayesian Optimization with GPyOpt &middot; Zhu Philip's AI Journey
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- merge something else -->
  
  <!-- merge something else 
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" /> -->
  
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>  
  
  <script defer src="/assets/js/lbox.js"></script>
   

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script> 

</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <link rel="stylesheet" href="/assets/style-3.css">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <div align="center">
          <img src="/assets/profile-pixel.png" class="profilepic pt-3 pb-2">
        </div>
        <!-- <a href="/"> -->
          Zhu Philip's AI Journey
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/working">Working</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <!-- <a class="sidebar-nav-item" href="/projects">Projects</a> -->
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">publications</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/working/">Working</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/zphilip/zphilip.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:zphilip48@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/tianda-zhu-37a5b031" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/zphilip" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/zphilip" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Bayesian Optimization with GPyOpt</h1>
  <span class="post-date">26 Jan 2021</span>
  <p><strong>Bayesian Optimization with GPyOpt</strong><br />
<strong>This example only show how the GPyOpt API work</strong></p>

<p>Try to find optimal hyperparameters to XGBoost model using Bayesian optimization with GP, with the diabetes dataset (from sklearn) as input. Letâ€™s first load the dataset with the following python code snippet:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_diabetes</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>
</code></pre></div></div>
<p>We will use cross-validation score to estimate accuracy and our goal will be to tune: max_depth, learning_rate, n_estimators parameters. First, we have to define optimization function and domains, as shown in the code below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Optimizer will try to find minimum, so let's add a "-" sign.
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="o">-</span><span class="n">cross_val_score</span><span class="p">(</span>
        <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                     <span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                     <span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>
                     <span class="n">gamma</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                     <span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span> 
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_root_mean_squared_error'</span>
    <span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>
     
<span class="c1"># Bounds (define continuous variables first, then discrete!)
</span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'learning_rate'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)},</span>
    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'gamma'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)},</span>
    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'max_depth'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)},</span>
    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'n_estimators'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)},</span>
    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'min_child_weight'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}</span>
<span class="p">]</span>
</code></pre></div></div>

<p>Letâ€™s find the baseline RMSE with default XGBoost parameters is . Letâ€™s see if we can do better.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">baseline</span> <span class="o">=</span> <span class="o">-</span><span class="n">cross_val_score</span><span class="p">(</span>
    <span class="n">XGBRegressor</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_root_mean_squared_error'</span>
<span class="p">).</span><span class="n">mean</span><span class="p">()</span>
<span class="n">baseline</span>
<span class="c1"># 64.90693011829266
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>64.90693033120199
</code></pre></div></div>

<p>Now, run the Bayesian optimization with GPyOpt and plot convergence, as in the next code snippet:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">GPyOpt</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">GPyOpt</span><span class="p">.</span><span class="n">methods</span><span class="p">.</span><span class="n">BayesianOptimization</span><span class="p">(</span>
    <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">acquisition_type</span> <span class="o">=</span><span class="s">'MPI'</span><span class="p">,</span> <span class="c1">## method to optimize the acq. function
</span>    <span class="n">acquisition_par</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">exact_eval</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">max_iter</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">max_time</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">max_time</span><span class="p">)</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">plot_convergence</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/2023-12-01-Bayesian-Optimization-1_files/2023-12-01-Bayesian-Optimization-1_7_0.png" alt="png" /></p>

<p>Extract the best values of the parameters and compute the RMSE / gain obtained with Bayesian Optimization, using the following code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="p">.</span><span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">Y</span><span class="p">)]</span>
<span class="c1"># array([2.01515532e-01, 1.35401092e+00, 1.00000000e+00, 
# 3.00000000e+02, 1.00000000e+00])
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([5.00047461e-02, 3.10049677e+00, 1.00000000e+00, 3.00000000e+02,
       1.00000000e+01])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'RMSE:'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">Y</span><span class="p">),</span>
      <span class="s">'Gain:'</span><span class="p">,</span> <span class="n">baseline</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="c1"># RMSE: 57.6844355488563 Gain: 112.52069904249859
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RMSE: 56.1566484514227 Gain: 125.44111632088877
</code></pre></div></div>

<p>Paramerter Tuning for SVR</p>

<p>-Now, letâ€™s tune a Support Vector Regressor model with Bayesian Optimization and find the optimal values for three parameters: C, epsilon and gamma.</p>

<p>-Letâ€™s use range (1e-5, 1000) for C, (1e-5, 10) for epsilon and gamma.</p>

<p>-Letâ€™s use MPI as an acquisition function with weight 0.1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span><span class="c1"># Bounds (define continuous variables first, then discrete!)
</span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'C'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)},</span>    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'epsilon'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)},</span>    <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'gamma'</span><span class="p">,</span>
     <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span>
     <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}</span>
<span class="p">]</span>
 
<span class="c1"># Score. Optimizer will try to find minimum, so we will add a "-" sign.
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="o">-</span><span class="n">cross_val_score</span><span class="p">(</span>
        <span class="n">SVR</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> 
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_root_mean_squared_error'</span>
    <span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="n">scoreoptimizer</span> <span class="o">=</span> <span class="n">GPyOpt</span><span class="p">.</span><span class="n">methods</span><span class="p">.</span><span class="n">BayesianOptimization</span><span class="p">(</span>
            <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
            <span class="n">acquisition_type</span> <span class="o">=</span><span class="s">'MPI'</span><span class="p">,</span>
            <span class="n">acquisition_par</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="n">exact_eval</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">scoreoptimizer</span>

<span class="n">max_iter</span> <span class="o">=</span> <span class="mi">50</span><span class="o">*</span><span class="mi">4</span>
<span class="n">max_time</span> <span class="o">=</span> <span class="mi">60</span><span class="o">*</span><span class="mi">4</span>

<span class="n">optimizer</span><span class="p">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">max_time</span><span class="p">)</span>
<span class="n">baseline</span> <span class="o">=</span> <span class="o">-</span><span class="n">cross_val_score</span><span class="p">(</span> <span class="n">SVR</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_root_mean_squared_error'</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">baseline</span><span class="p">)</span>

<span class="c1"># 70.44352670586173print(optimizer.X[np.argmin(optimizer.Y)])
# [126.64337652   8.49323372   8.59189135]
</span><span class="k">print</span><span class="p">(</span><span class="s">'RMSE:'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">Y</span><span class="p">),</span>
      <span class="s">'Gain:'</span><span class="p">,</span> <span class="n">baseline</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="c1"># RMSE: 54.02576574389976 Gain: 130.38876124364006     best_epsilon = optimizer.X[np.argmin(optimizer.Y)][1] 
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">plot_convergence</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>70.44352670586173
RMSE: 56.1566484514227 Gain: 125.44111632088877
</code></pre></div></div>

<p><img src="/assets/2023-12-01-Bayesian-Optimization-1_files/2023-12-01-Bayesian-Optimization-1_12_1.png" alt="png" /></p>

<p>Surrogate Function
The surrogate function is a technique used to best approximate the mapping of input examples to an output score.</p>

<h2 id="how-to-implement-bayesian-optimization-from-scratch-in-python">How to Implement Bayesian Optimization from Scratch in Python</h2>

<p>https://machinelearningmastery.com/what-is-bayesian-optimization/</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># example of a gaussian process surrogate function
</span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sin</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">pi</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">arange</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">asarray</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">normal</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">catch_warnings</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">simplefilter</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>

<span class="c1"># objective function
</span><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
	<span class="n">noise</span> <span class="o">=</span> <span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mf">6.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1"># surrogate or approximation for the objective function
</span><span class="k">def</span> <span class="nf">surrogate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
	<span class="c1"># catch any warning generated when making a prediction
</span>	<span class="k">with</span> <span class="n">catch_warnings</span><span class="p">():</span>
		<span class="c1"># ignore generated warnings
</span>		<span class="n">simplefilter</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot real observations vs surrogate function
</span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
	<span class="c1"># scatter plot of inputs and real objective function
</span>	<span class="n">pyplot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
	<span class="c1"># line plot of surrogate function across domain
</span>	<span class="n">Xsamples</span> <span class="o">=</span> <span class="n">asarray</span><span class="p">(</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">))</span>
	<span class="n">Xsamples</span> <span class="o">=</span> <span class="n">Xsamples</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xsamples</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
	<span class="n">ysamples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">surrogate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xsamples</span><span class="p">)</span>
	<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xsamples</span><span class="p">,</span> <span class="n">ysamples</span><span class="p">)</span>
	<span class="c1"># show the plot
</span>	<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># sample the domain sparsely with noise
</span><span class="n">X</span> <span class="o">=</span> <span class="n">random</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">asarray</span><span class="p">([</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
<span class="c1"># reshape into rows and cols
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># define the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">()</span>
<span class="c1"># fit the model
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># plot the surrogate function
</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/2023-12-01-Bayesian-Optimization-1_files/2023-12-01-Bayesian-Optimization-1_14_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># example of bayesian optimization for a 1d function from scratch
# this is pure gaussian bayesian optimization to find the next x to sampling...
</span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sin</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">pi</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">arange</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">vstack</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">argmax</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">asarray</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">normal</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">catch_warnings</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">simplefilter</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>

<span class="c1"># the true objective loss function??
</span><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
	<span class="n">noise</span> <span class="o">=</span> <span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mf">6.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1"># surrogate or approximation for the objective function
</span><span class="k">def</span> <span class="nf">surrogate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
	<span class="c1"># catch any warning generated when making a prediction
</span>	<span class="k">with</span> <span class="n">catch_warnings</span><span class="p">():</span>
		<span class="c1"># ignore generated warnings
</span>		<span class="n">simplefilter</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># probability of improvement acquisition function
</span><span class="k">def</span> <span class="nf">acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Xsamples</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
	<span class="c1"># calculate the best surrogate score found so far
</span>	<span class="n">yhat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">surrogate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
	<span class="n">best</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
	<span class="c1"># calculate mean and stdev via surrogate function
</span>	<span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">surrogate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xsamples</span><span class="p">)</span>
	<span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
	<span class="c1"># calculate the probability of improvement
</span>	<span class="n">probs</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">mu</span> <span class="o">-</span> <span class="n">best</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span><span class="o">+</span><span class="mf">1E-9</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">probs</span>

<span class="c1"># optimize the acquisition function
</span><span class="k">def</span> <span class="nf">opt_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
	<span class="c1"># random search, generate random samples
</span>	<span class="n">Xsamples</span> <span class="o">=</span> <span class="n">random</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
	<span class="n">Xsamples</span> <span class="o">=</span> <span class="n">Xsamples</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xsamples</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
	<span class="c1"># calculate the acquisition function for each sample
</span>	<span class="n">scores</span> <span class="o">=</span> <span class="n">acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Xsamples</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
	<span class="c1"># locate the index of the largest scores
</span>	<span class="n">ix</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">Xsamples</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># plot real observations vs surrogate function
</span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
	<span class="c1"># scatter plot of inputs and real objective function
</span>	<span class="n">pyplot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
	<span class="c1"># line plot of surrogate function across domain
</span>	<span class="n">Xsamples</span> <span class="o">=</span> <span class="n">asarray</span><span class="p">(</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">))</span>
	<span class="n">Xsamples</span> <span class="o">=</span> <span class="n">Xsamples</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xsamples</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
	<span class="n">ysamples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">surrogate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xsamples</span><span class="p">)</span>
	<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xsamples</span><span class="p">,</span> <span class="n">ysamples</span><span class="p">)</span>
	<span class="c1"># show the plot
</span>	<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># sample the domain sparsely with noise
</span><span class="n">X</span> <span class="o">=</span> <span class="n">random</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">asarray</span><span class="p">([</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
<span class="c1"># reshape into rows and cols
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># define the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">()</span>
<span class="c1"># fit the model
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># plot before hand
</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="c1"># perform the optimization process
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
	<span class="c1"># select the next point to sample
</span>	<span class="n">x</span> <span class="o">=</span> <span class="n">opt_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
	<span class="c1"># if hyperparamters , here need to apply the paramters into the object function
</span>    <span class="c1"># sample the point
</span>	<span class="n">actual</span> <span class="o">=</span> <span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="c1"># summarize the finding
</span>	<span class="n">est</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">surrogate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="n">x</span><span class="p">]])</span>
	<span class="k">print</span><span class="p">(</span><span class="s">'&gt;x=%.3f, f()=%3f, actual=%.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">est</span><span class="p">,</span> <span class="n">actual</span><span class="p">))</span>
	<span class="c1"># add the data to the dataset
</span>	<span class="n">X</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="n">x</span><span class="p">]]))</span>
	<span class="n">y</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="p">[[</span><span class="n">actual</span><span class="p">]]))</span>
	<span class="c1"># update the model
</span>	<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># plot all samples and the final surrogate function
</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="c1"># best result
</span><span class="n">ix</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Best Result: x=%.3f, y=%.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">ix</span><span class="p">]))</span>
</code></pre></div></div>

<p><img src="/assets/2023-12-01-Bayesian-Optimization-1_files/2023-12-01-Bayesian-Optimization-1_15_0.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;x=0.753, f()=0.130899, actual=0.202
&gt;x=0.914, f()=0.441535, actual=0.830
&gt;x=0.914, f()=0.468772, actual=0.614
&gt;x=0.448, f()=0.072738, actual=0.213
&gt;x=0.255, f()=-0.005875, actual=-0.006
&gt;x=0.546, f()=0.125458, actual=0.071
&gt;x=0.644, f()=0.099045, actual=-0.099
&gt;x=0.546, f()=0.115388, actual=0.125
&gt;x=0.913, f()=0.480679, actual=0.718
&gt;x=0.724, f()=0.094843, actual=0.413
&gt;x=0.108, f()=0.029385, actual=-0.000
&gt;x=0.366, f()=0.019852, actual=0.169
&gt;x=0.935, f()=0.466089, actual=0.308
&gt;x=0.912, f()=0.484408, actual=0.802
&gt;x=0.913, f()=0.501859, actual=0.812
&gt;x=0.846, f()=0.379995, actual=0.132
&gt;x=0.661, f()=0.088295, actual=0.177
&gt;x=0.383, f()=0.037196, actual=-0.013
&gt;x=0.113, f()=0.026929, actual=-0.118
&gt;x=0.981, f()=0.147262, actual=-0.012
&gt;x=0.868, f()=0.436012, actual=0.285
&gt;x=0.911, f()=0.503125, actual=0.739
&gt;x=0.646, f()=0.095198, actual=-0.099
&gt;x=0.355, f()=0.019695, actual=0.077
&gt;x=0.535, f()=0.120008, actual=0.179
&gt;x=0.729, f()=0.099606, actual=0.180
&gt;x=0.913, f()=0.515075, actual=0.530
&gt;x=0.309, f()=0.003572, actual=0.045
&gt;x=0.950, f()=0.416246, actual=0.119
&gt;x=0.912, f()=0.506245, actual=0.845
&gt;x=0.911, f()=0.520569, actual=0.806
&gt;x=0.911, f()=0.532109, actual=0.848
&gt;x=0.135, f()=0.018783, actual=-0.117
&gt;x=0.911, f()=0.544241, actual=0.610
&gt;x=0.148, f()=0.011521, actual=-0.119
&gt;x=0.002, f()=0.093856, actual=0.073
&gt;x=0.221, f()=-0.003341, actual=-0.009
&gt;x=0.072, f()=0.008024, actual=-0.175
&gt;x=0.293, f()=0.001839, actual=0.134
&gt;x=0.804, f()=0.259678, actual=-0.110
&gt;x=0.111, f()=-0.004541, actual=0.012
&gt;x=0.679, f()=0.071661, actual=0.215
&gt;x=0.426, f()=0.079402, actual=-0.100
&gt;x=0.967, f()=0.280364, actual=-0.133
&gt;x=0.814, f()=0.283361, actual=-0.118
&gt;x=0.529, f()=0.126789, actual=0.145
&gt;x=0.664, f()=0.077135, actual=0.231
&gt;x=0.914, f()=0.528958, actual=0.622
&gt;x=0.124, f()=0.001293, actual=0.042
&gt;x=0.911, f()=0.533073, actual=0.884
&gt;x=0.357, f()=0.020477, actual=-0.036
&gt;x=0.393, f()=0.043710, actual=-0.044
&gt;x=0.638, f()=0.094322, actual=0.031
&gt;x=0.469, f()=0.101346, actual=0.175
&gt;x=0.584, f()=0.122175, actual=0.081
&gt;x=0.911, f()=0.545548, actual=0.825
&gt;x=0.701, f()=0.072042, actual=0.487
&gt;x=0.911, f()=0.552639, actual=0.811
&gt;x=0.641, f()=0.098095, actual=-0.014
&gt;x=0.305, f()=-0.004428, actual=0.003
&gt;x=0.295, f()=-0.005267, actual=0.150
&gt;x=0.767, f()=0.153709, actual=-0.174
&gt;x=0.658, f()=0.081160, actual=0.100
&gt;x=0.171, f()=0.014084, actual=-0.156
&gt;x=0.657, f()=0.081738, actual=0.422
&gt;x=0.640, f()=0.100931, actual=-0.157
&gt;x=0.911, f()=0.559911, actual=0.786
&gt;x=0.194, f()=0.007003, actual=0.065
&gt;x=0.884, f()=0.528691, actual=0.647
&gt;x=0.927, f()=0.550419, actual=0.328
&gt;x=0.682, f()=0.075016, actual=0.308
&gt;x=0.531, f()=0.135375, actual=0.071
&gt;x=0.079, f()=-0.014623, actual=0.022
&gt;x=0.918, f()=0.559093, actual=0.659
&gt;x=0.826, f()=0.329826, actual=0.135
&gt;x=0.380, f()=0.031751, actual=-0.054
&gt;x=0.255, f()=-0.002043, actual=-0.072
&gt;x=0.755, f()=0.124829, actual=0.248
&gt;x=0.431, f()=0.070760, actual=-0.057
&gt;x=0.437, f()=0.071749, actual=-0.102
&gt;x=0.115, f()=-0.000257, actual=0.042
&gt;x=0.549, f()=0.129516, actual=0.278
&gt;x=0.053, f()=-0.009525, actual=-0.098
&gt;x=0.348, f()=-0.000176, actual=0.010
&gt;x=0.133, f()=0.005985, actual=0.021
&gt;x=0.912, f()=0.562865, actual=0.720
&gt;x=0.911, f()=0.567181, actual=0.835
&gt;x=0.716, f()=0.087171, actual=0.275
&gt;x=0.052, f()=-0.015777, actual=-0.162
&gt;x=0.743, f()=0.114964, actual=0.195
&gt;x=0.335, f()=-0.005154, actual=0.003
&gt;x=0.910, f()=0.573601, actual=0.816
&gt;x=0.278, f()=-0.007704, actual=0.097
&gt;x=0.692, f()=0.087360, actual=0.370
&gt;x=0.560, f()=0.138122, actual=0.218
&gt;x=0.583, f()=0.137465, actual=-0.197
&gt;x=0.530, f()=0.126498, actual=0.055
&gt;x=0.963, f()=0.324050, actual=0.177
&gt;x=0.241, f()=0.007597, actual=-0.173
&gt;x=0.910, f()=0.577252, actual=0.785
</code></pre></div></div>

<p><img src="/assets/2023-12-01-Bayesian-Optimization-1_files/2023-12-01-Bayesian-Optimization-1_15_2.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best Result: x=0.911, y=0.884
</code></pre></div></div>

<p>In this case, the model via mean 5-fold cross-validation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># example of bayesian optimization with scikit-optimize
</span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">mean</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">skopt.space</span> <span class="kn">import</span> <span class="n">Integer</span>
<span class="kn">from</span> <span class="nn">skopt.utils</span> <span class="kn">import</span> <span class="n">use_named_args</span>
<span class="kn">from</span> <span class="nn">skopt</span> <span class="kn">import</span> <span class="n">gp_minimize</span>

<span class="c1"># generate 2d classification dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># define the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="c1"># define the space of hyperparameters to search
</span><span class="n">search_space</span> <span class="o">=</span> <span class="p">[</span><span class="n">Integer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'n_neighbors'</span><span class="p">),</span> <span class="n">Integer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'p'</span><span class="p">)]</span>

<span class="c1"># define the function used to evaluate a given configuration
</span><span class="o">@</span><span class="n">use_named_args</span><span class="p">(</span><span class="n">search_space</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">):</span>
	<span class="c1"># something
</span>	<span class="n">model</span><span class="p">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
	<span class="c1"># calculate 5-fold cross validation
</span>	<span class="n">result</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>
	<span class="c1"># calculate the mean of the scores
</span>	<span class="n">estimate</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
	<span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">estimate</span>

<span class="c1"># perform optimization
</span><span class="n">result</span> <span class="o">=</span> <span class="n">gp_minimize</span><span class="p">(</span><span class="n">evaluate_model</span><span class="p">,</span> <span class="n">search_space</span><span class="p">)</span>
<span class="c1"># summarizing finding:
</span><span class="k">print</span><span class="p">(</span><span class="s">'Best Accuracy: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">result</span><span class="p">.</span><span class="n">fun</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Best Parameters: n_neighbors=%d, p=%d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">result</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best Accuracy: 1.000
Best Parameters: n_neighbors=3, p=2
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span><span class="p">.</span><span class="n">fun</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.0
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

</div>

<span class="post-tags">
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/AI"><nobr>AI</nobr></code>&nbsp;</a>    
    
</span>

<div class="recent">
  <h2>Recent Posts</h2>
  <ul class="recent-posts">
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo2.html">
            A Diffusion Model from Scratch in Pytorch
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/02/Inspect-BERT-Vocabulary.html">
            Inspect BERT Vocabulary
          </a>
          <small>[02 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/working/2023/12/01/My-Tasks-and-Notes.html">
            working todo
          </a>
          <small>[01 Dec 2023]</small>
        </h4>
      </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
