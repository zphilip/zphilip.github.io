<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      XGBoost from scratch &middot; Zhu Philip's AI Journey
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- merge something else -->
  
  <!-- merge something else 
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" /> -->
  
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>  
  
  <script defer src="/assets/js/lbox.js"></script>
   

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script> 

</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <link rel="stylesheet" href="/assets/style-3.css">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <div align="center">
          <img src="/assets/profile-pixel.png" class="profilepic pt-3 pb-2">
        </div>
        <!-- <a href="/"> -->
          Zhu Philip's AI Journey
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/working">Working</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <!-- <a class="sidebar-nav-item" href="/projects">Projects</a> -->
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">publications</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/working/">Working</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/zphilip/zphilip.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:zphilip48@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/tianda-zhu-37a5b031" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/zphilip" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/zphilip" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">XGBoost from scratch</h1>
  <span class="post-date">26 Jan 2023</span>
  <h1 id="xgboost-from-scratch">XGBoost from scratch</h1>

<p><img src="/assets/2023-11-02-xgboost-from-scratch_files/adb4c4af-7b2f-4a8e-8b0a-32846e450ebb.png" alt="image.png" /></p>

<h2 id="the--xgboost-model-class">The  XGBoost Model Class</h2>

<p>We begin with the user-facing API for our model, a class called <code class="language-plaintext highlighter-rouge">XGBoostModel</code> which will implement gradient boosting and prediction.  To be more consistent with the XGBoost library, we’ll pass hyperparameters to our model in a parameter dictionary, so our init method is going to pull relevant parameters out of the dictionary and set them as object attributes. Note the use of python’s <code class="language-plaintext highlighter-rouge">defaultdict</code> so we don’t have to worry about handling key errors if we try to access a parameter that the user didn’t set in the dictionary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">XGBoostModel</span><span class="p">():</span>
    <span class="s">'''XGBoost from Scratch
    '''</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">subsample</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'subsample'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'subsample'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'learning_rate'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'learning_rate'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">0.3</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">base_prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'base_score'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'base_score'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">0.5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'max_depth'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'max_depth'</span><span class="p">]</span> <span class="k">else</span> <span class="mi">5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
</code></pre></div></div>

<p>The fit method, based on our classic GBM,  takes a feature dataframe, a target vector, the objective function, and the number of boosting rounds as arguments. The user-supplied objective function should be an object with loss, gradient, and hessian methods, each of which takes a target vector and a prediction vector as input; the loss method should return a scalar loss score, the gradient method should return a vector of gradients, and the  hessian method should return a vector of hessians.</p>

<ul>
  <li>step1:  In contrast to boosting in the classic GBM, instead of computing residuals between the current predictions and the target, we compute gradients and hessians of the loss function with respect to the current predictions<br />
      记一阶导为gradients: $ g_i=l^{\prime}\left(y_i, \widehat{y}_i^{(t-1)}\right) $
      二阶导为hessians: $h_i=l^{\prime \prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)$.</li>
  <li>
    <p>step2: and instead of predicting residuals with a decision tree, we fit a special XGBoost tree booster (which we’ll implement in a moment) using the gradients and hessians.</p>
  </li>
  <li>step3: The rest of the fit method is the same as the classic GBM, and the predict method is identical too.</li>
</ul>

<p>I’ve also added row subsampling by drawing a random subset of instance indices and passing them to the tree booster during each boosting round.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">current_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">base_prediction</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">boosters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_boost_round</span><span class="p">):</span>
        <span class="c1">## compute gradients and hessians
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">objective</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">current_predictions</span><span class="p">)</span>
        <span class="n">hessians</span> <span class="o">=</span> <span class="n">objective</span><span class="p">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">current_predictions</span><span class="p">)</span>
        <span class="c1">## subsampling by drawing a random subset of instance indices and passing them to the tree booster during each boosting round
</span>        <span class="n">sample_idxs</span> <span class="o">=</span> <span class="bp">None</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">subsample</span> <span class="o">==</span> <span class="mf">1.0</span> \
            <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">rng</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> 
                                 <span class="n">size</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">subsample</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> 
                                 <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1">#set up the boost tree
</span>        <span class="n">booster</span> <span class="o">=</span> <span class="n">TreeBooster</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">hessians</span><span class="p">,</span> 
                              <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">sample_idxs</span><span class="p">)</span>
        <span class="n">current_predictions</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">booster</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">boosters</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">booster</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> 
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">] train loss = </span><span class="si">{</span><span class="n">objective</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">current_predictions</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
            
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">base_prediction</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> 
            <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">booster</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">booster</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">boosters</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">XGBoostModel</span><span class="p">.</span><span class="n">fit</span> <span class="o">=</span> <span class="n">fit</span>
<span class="n">XGBoostModel</span><span class="p">.</span><span class="n">predict</span> <span class="o">=</span> <span class="n">predict</span>            
</code></pre></div></div>

<p>All we have to do now is implement the tree booster.</p>

<h2 id="the-xgboost-tree-booster">The XGBoost Tree Booster</h2>

<p>The XGBoost tree booster is a modified version of the decision tree that we built in the decision tree from scratch post. Like the decision tree, we recursively build a binary tree structure by finding the best split rule for each node in the tree.</p>

<ul>
  <li>The main difference is the criterion for evaluating splits and the way that we define a leaf’s predicted value. Instead of being functions of the target values of the instances in each node, the criterion and predicted values are functions of the instance gradients and hessians. Thus we need only make a couple of modifications to our previous decision tree implementation to create the XGBoost tree booster.</li>
</ul>

<h3 id="initialization-and-inserting-child-nodes">Initialization and Inserting Child Nodes</h3>
<p>Most of the init method is just parsing the parameter dictionary to assign parameters as object attributes.</p>

<ul>
  <li>The one notable difference from our decision tree is in the way we define the node’s predicted value. We define <code class="language-plaintext highlighter-rouge">self.value</code> according to equation 5 of the XGBoost paper, a simple function of the gradient and hessian values of the instances in the current node. Of course the init also goes on to build the tree via the maybe insert child nodes method. This method is nearly identical to the one we implemented for our decision tree. So far so good.</li>
</ul>

<p>Note: XGBoost目标函数的各个叶子结点的目标式子是相互独立的。即每个叶子结点的式子都达到最值点，整个目标函数也达到最值点。则每个叶子结点的权重 $w_j^*$ 及此时达到最优的 $O b j$ 目标值 :
\(w_j^*=-\frac{G_j}{H_j+\lambda} \quad O b j=-\frac{1}{2} \sum_{j=1}^T \frac{G_j{ }^2}{H_j+\lambda}+\gamma T\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TreeBooster</span><span class="p">():</span>
    <span class="c1">#init get the input include gradient and hessian value of all data
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">idxs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">params</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'max_depth must be nonnegative'</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'min_child_weight'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'min_child_weight'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'reg_lambda'</span><span class="p">]</span> <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'reg_lambda'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">colsample_bynode</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'colsample_bynode'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'colsample_bynode'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="c1">#if dataframe then covnert to data array
</span>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">):</span> <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">values</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">):</span> <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">values</span>
        <span class="k">if</span> <span class="n">idxs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">idxs</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">idxs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">c</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs</span><span class="p">),</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1">#set the top node w value /predict value
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="n">g</span><span class="p">[</span><span class="n">idxs</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">idxs</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">)</span> <span class="c1"># Eq (5)
</span>        
        <span class="c1">#init stat , if later have no change(no split so far) , it means it is leaf node
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_maybe_insert_child_nodes</span><span class="p">()</span>

    <span class="c1">#try to build one node for the tree
</span>    <span class="k">def</span> <span class="nf">_maybe_insert_child_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">c</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">_find_better_split</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span> <span class="k">return</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">split_feature_idx</span><span class="p">]</span>
        <span class="n">left_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">right_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">TreeBooster</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> 
                                <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">[</span><span class="n">left_idx</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">TreeBooster</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> 
                                 <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">[</span><span class="n">right_idx</span><span class="p">])</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">is_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span> <span class="o">==</span> <span class="mf">0.</span>

    <span class="k">def</span> <span class="nf">_find_better_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_idx</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<h3 id="split-finding">Split Finding</h3>

<p>Split finding follows the exact same pattern that we used in the decision tree, except we keep track of gradient and hessian stats instead of target value stats, and of course we use the XGBoost gain criterion (equation 7 from the paper) for evaluating splits.</p>

\[O b j=-\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda}+\gamma T\]

\[{ Gain }=\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{\left(G_L+G_R\right)^2}{H_L+H_R+\lambda}\right]-\gamma\]

<p><img src="/assets/2023-11-02-xgboost-from-scratch_files/eb939d3d-7647-48c5-83cb-587133e35ed0.png" alt="image.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_find_better_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_idx</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">,</span> <span class="n">feature_idx</span><span class="p">]</span>
    <span class="n">g</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">]</span>
    <span class="c1">#sort the data with x feature
</span>    <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sort_g</span><span class="p">,</span> <span class="n">sort_h</span><span class="p">,</span> <span class="n">sort_x</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span> <span class="n">h</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">]</span>
    <span class="c1">#init the GL, GR, HL, HR
</span>    <span class="n">sum_g</span><span class="p">,</span> <span class="n">sum_h</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="n">h</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">sum_g_right</span><span class="p">,</span> <span class="n">sum_h_right</span> <span class="o">=</span> <span class="n">sum_g</span><span class="p">,</span> <span class="n">sum_h</span>
    <span class="n">sum_g_left</span><span class="p">,</span> <span class="n">sum_h_left</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#cacluate the  GL, GR, HL, HR under the split point is i
</span>        <span class="n">g_i</span><span class="p">,</span> <span class="n">h_i</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">x_i_next</span> <span class="o">=</span> <span class="n">sort_g</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sort_h</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sort_x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sort_x</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">sum_g_left</span> <span class="o">+=</span> <span class="n">g_i</span><span class="p">;</span> <span class="n">sum_g_right</span> <span class="o">-=</span> <span class="n">g_i</span>
        <span class="n">sum_h_left</span> <span class="o">+=</span> <span class="n">h_i</span><span class="p">;</span> <span class="n">sum_h_right</span> <span class="o">-=</span> <span class="n">h_i</span>
        
        <span class="c1">#set up the break condition from iteration
</span>        <span class="c1">#what the considtion that x_i == x_i_next??
</span>        <span class="k">if</span> <span class="n">sum_h_left</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_child_weight</span> <span class="ow">or</span> <span class="n">x_i</span> <span class="o">==</span> <span class="n">x_i_next</span><span class="p">:</span><span class="k">continue</span>
        <span class="k">if</span> <span class="n">sum_h_right</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_child_weight</span><span class="p">:</span> <span class="k">break</span>
        
        <span class="c1"># calcuate the gain 
</span>        <span class="n">gain</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">sum_g_left</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">sum_h_left</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">))</span>
                        <span class="o">+</span> <span class="p">(</span><span class="n">sum_g_right</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">sum_h_right</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">))</span>
                        <span class="o">-</span> <span class="p">(</span><span class="n">sum_g</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">sum_h</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">))</span>
                        <span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># Eq(7) in the xgboost paper
</span>        
        <span class="c1">#record this split if the gain is better
</span>        <span class="k">if</span> <span class="n">gain</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span><span class="p">:</span> 
            <span class="bp">self</span><span class="p">.</span><span class="n">split_feature_idx</span> <span class="o">=</span> <span class="n">feature_idx</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span> <span class="o">=</span> <span class="n">gain</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">+</span> <span class="n">x_i_next</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            
<span class="n">TreeBooster</span><span class="p">.</span><span class="n">_find_better_split</span> <span class="o">=</span> <span class="n">_find_better_split</span>
</code></pre></div></div>

<h3 id="prediction">Prediction</h3>

<p>Prediction works exactly the same as in our decision tree, and the methods are nearly identical.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">_predict_row</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">X</span><span class="p">.</span><span class="n">iterrows</span><span class="p">()])</span>

<span class="k">def</span> <span class="nf">_predict_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span> 
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">value</span>
    <span class="n">child</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">left</span> <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">split_feature_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span> \
        <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">right</span>
    <span class="k">return</span> <span class="n">child</span><span class="p">.</span><span class="n">_predict_row</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>

<span class="n">TreeBooster</span><span class="p">.</span><span class="n">predict</span> <span class="o">=</span> <span class="n">predict</span> 
<span class="n">TreeBooster</span><span class="p">.</span><span class="n">_predict_row</span> <span class="o">=</span> <span class="n">_predict_row</span> 
</code></pre></div></div>

<h2 id="the-complete-xgboost-from-scratch-implementation">The Complete XGBoost From Scratch Implementation</h2>

<p>Here’s the entire implementation which produces a usable <code class="language-plaintext highlighter-rouge">XGBoostModel</code> class with fit and predict methods.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">XGBoostModel</span><span class="p">():</span>
    <span class="s">'''XGBoost from Scratch
    '''</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">subsample</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'subsample'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'subsample'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'learning_rate'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'learning_rate'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">0.3</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">base_prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'base_score'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'base_score'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">0.5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'max_depth'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'max_depth'</span><span class="p">]</span> <span class="k">else</span> <span class="mi">5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
                
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">current_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">base_prediction</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">boosters</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_boost_round</span><span class="p">):</span>
            <span class="n">gradients</span> <span class="o">=</span> <span class="n">objective</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">current_predictions</span><span class="p">)</span>
            <span class="n">hessians</span> <span class="o">=</span> <span class="n">objective</span><span class="p">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">current_predictions</span><span class="p">)</span>
            <span class="n">sample_idxs</span> <span class="o">=</span> <span class="bp">None</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">subsample</span> <span class="o">==</span> <span class="mf">1.0</span> \
                <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">rng</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> 
                                     <span class="n">size</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">subsample</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> 
                                     <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">booster</span> <span class="o">=</span> <span class="n">TreeBooster</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">hessians</span><span class="p">,</span> 
                                  <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">sample_idxs</span><span class="p">)</span>
            <span class="n">current_predictions</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">booster</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">boosters</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">booster</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> 
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">] train loss = </span><span class="si">{</span><span class="n">objective</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">current_predictions</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">base_prediction</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> 
                <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">booster</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">booster</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">boosters</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TreeBooster</span><span class="p">():</span>
 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">idxs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">params</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'max_depth must be nonnegative'</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'min_child_weight'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'min_child_weight'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'reg_lambda'</span><span class="p">]</span> <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'reg_lambda'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">colsample_bynode</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">'colsample_bynode'</span><span class="p">]</span> \
            <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s">'colsample_bynode'</span><span class="p">]</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">):</span> <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">values</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">):</span> <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">values</span>
        <span class="k">if</span> <span class="n">idxs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">idxs</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">idxs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">c</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs</span><span class="p">),</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="n">g</span><span class="p">[</span><span class="n">idxs</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">idxs</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">)</span> <span class="c1"># Eq (5)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_maybe_insert_child_nodes</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_maybe_insert_child_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">c</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">_find_better_split</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span> <span class="k">return</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">split_feature_idx</span><span class="p">]</span>
        <span class="n">left_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">right_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">TreeBooster</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> 
                                <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">[</span><span class="n">left_idx</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">TreeBooster</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> 
                                 <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">[</span><span class="n">right_idx</span><span class="p">])</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">is_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span> <span class="o">==</span> <span class="mf">0.</span>
    
    <span class="k">def</span> <span class="nf">_find_better_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_idx</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">,</span> <span class="n">feature_idx</span><span class="p">]</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">idxs</span><span class="p">]</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">sort_g</span><span class="p">,</span> <span class="n">sort_h</span><span class="p">,</span> <span class="n">sort_x</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span> <span class="n">h</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">]</span>
        <span class="n">sum_g</span><span class="p">,</span> <span class="n">sum_h</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="n">h</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">sum_g_right</span><span class="p">,</span> <span class="n">sum_h_right</span> <span class="o">=</span> <span class="n">sum_g</span><span class="p">,</span> <span class="n">sum_h</span>
        <span class="n">sum_g_left</span><span class="p">,</span> <span class="n">sum_h_left</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">g_i</span><span class="p">,</span> <span class="n">h_i</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">x_i_next</span> <span class="o">=</span> <span class="n">sort_g</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sort_h</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sort_x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sort_x</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">sum_g_left</span> <span class="o">+=</span> <span class="n">g_i</span><span class="p">;</span> <span class="n">sum_g_right</span> <span class="o">-=</span> <span class="n">g_i</span>
            <span class="n">sum_h_left</span> <span class="o">+=</span> <span class="n">h_i</span><span class="p">;</span> <span class="n">sum_h_right</span> <span class="o">-=</span> <span class="n">h_i</span>
            <span class="k">if</span> <span class="n">sum_h_left</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_child_weight</span> <span class="ow">or</span> <span class="n">x_i</span> <span class="o">==</span> <span class="n">x_i_next</span><span class="p">:</span><span class="k">continue</span>
            <span class="k">if</span> <span class="n">sum_h_right</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_child_weight</span><span class="p">:</span> <span class="k">break</span>

            <span class="n">gain</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">sum_g_left</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">sum_h_left</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">))</span>
                            <span class="o">+</span> <span class="p">(</span><span class="n">sum_g_right</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">sum_h_right</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">))</span>
                            <span class="o">-</span> <span class="p">(</span><span class="n">sum_g</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">sum_h</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg_lambda</span><span class="p">))</span>
                            <span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># Eq(7) in the xgboost paper
</span>            <span class="k">if</span> <span class="n">gain</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span><span class="p">:</span> 
                <span class="bp">self</span><span class="p">.</span><span class="n">split_feature_idx</span> <span class="o">=</span> <span class="n">feature_idx</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">best_score_so_far</span> <span class="o">=</span> <span class="n">gain</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">+</span> <span class="n">x_i_next</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
                
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">_predict_row</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">X</span><span class="p">.</span><span class="n">iterrows</span><span class="p">()])</span>

    <span class="k">def</span> <span class="nf">_predict_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span> 
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">value</span>
        <span class="n">child</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">left</span> <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">split_feature_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span> \
            <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">right</span>
        <span class="k">return</span> <span class="n">child</span><span class="p">.</span><span class="n">_predict_row</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="testing">Testing</h2>

<p>Let’s take this baby for a spin and benchmark its performance against the actual XGBoost library. We use the scikit learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html">California housing dataset</a> for benchmarking.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
    
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">43</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s start with a nice friendly squared error objective function for training. We should probably have a future post all about how to define custom objective functions in XGBoost, but for now, here’s how I define squared error.<br />
   gradients: $g_i=l^{\prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)$ ，hessians: $h_i=l^{\prime \prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SquaredErrorObjective</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span> <span class="k">return</span> <span class="n">pred</span> <span class="o">-</span> <span class="n">y</span>
    <span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<p>Here I use a more or less arbitrary set of hyperparameters for training.  Feel free to play around with tuning and trying other parameter combinations yourself.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="n">xgb</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'learning_rate'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s">'subsample'</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="s">'reg_lambda'</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span>
    <span class="s">'gamma'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s">'min_child_weight'</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span>
    <span class="s">'base_score'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s">'tree_method'</span><span class="p">:</span> <span class="s">'exact'</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">num_boost_round</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># train the from-scratch XGBoost model
</span><span class="n">model_scratch</span> <span class="o">=</span> <span class="n">XGBoostModel</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_scratch</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">SquaredErrorObjective</span><span class="p">(),</span> <span class="n">num_boost_round</span><span class="p">)</span>

<span class="c1"># train the library XGBoost model
</span><span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">dtest</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>
<span class="n">model_xgb</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<p>Let’s check the models’ performance on the held out test data to benchmark our implementation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred_scratch_train</span> <span class="o">=</span> <span class="n">model_scratch</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">pred_scratch</span> <span class="o">=</span> <span class="n">model_scratch</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">pred_xgb</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'scratch score: </span><span class="si">{</span><span class="n">SquaredErrorObjective</span><span class="p">().</span><span class="n">loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_scratch</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'xgboost score: </span><span class="si">{</span><span class="n">SquaredErrorObjective</span><span class="p">().</span><span class="n">loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_xgb</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">RSE</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span> 
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">g</span><span class="p">))</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Training error: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">RSE</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">pred_scratch_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Validation error: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">RSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_scratch</span><span class="p">)))</span>
</code></pre></div></div>

<p>Well, look at that! Our scratch-built SGBoost is looking pretty consistent with the library. Go us!</p>

<h2 id="wrapping-up">Wrapping Up</h2>
<p>I’d say this is a pretty good milestone for us here at Random Realizations. We’ve been hammering away at the various concepts around gradient boosting, leaving a trail of equations and scratch-built algos in our wake. Today we put all of that together to create a legit scratch build of XGBoost, something that would have been out of reach for me before we embarked on this journey together over a year ago. To anyone with the patience to read through this stuff, cheers to you! I hope you’re learning and enjoying this as much as I am.</p>

<h2 id="reader-exercises">Reader Exercises</h2>
<p>If you want to take this a step further and deepen your understanding and coding abilities, let me recommend some exercises for you.</p>

<ol>
  <li>Implement column subsampling. XGBoost itself provides column subsampling by tree, by level, and by node. Try implementing by tree first, then try adding by level or by node as well. These should be pretty straightforward to do.</li>
  <li>Implement sparsity aware split finding for missing feature values (Algorithm 2 in the <a href="https://arxiv.org/abs/1603.02754">XGBoost paper</a>). This will be a little more involved, since you’ll need to refactor and modify several parts of the tree booster class.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

</div>

<span class="post-tags">
    
      <i class="fa fa-tag fa-xs" aria-hidden="true"></i>
      
      <a class="no-underline" href="/tag/AI"><nobr>AI</nobr></code>&nbsp;</a>    
    
</span>

<div class="recent">
  <h2>Recent Posts</h2>
  <ul class="recent-posts">
    
      <li>
        <h4>
          <a href="/coding/2023/12/11/diffusion-model-demo2.html">
            A Diffusion Model from Scratch in Pytorch
          </a>
          <small>[11 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/coding/2023/12/02/Inspect-BERT-Vocabulary.html">
            Inspect BERT Vocabulary
          </a>
          <small>[02 Dec 2023]</small>
        </h4>
      </li>
    
      <li>
        <h4>
          <a href="/working/2023/12/01/My-Tasks-and-Notes.html">
            working todo
          </a>
          <small>[01 Dec 2023]</small>
        </h4>
      </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
